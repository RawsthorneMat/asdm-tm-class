{
 "metadata": {
  "name": "",
  "signature": "sha256:ccbbbba89091b141b9c8002d6d9f70da9d462efa8cd8fd9be524027c3359ba0d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab --no-import-all inline\n",
      "%precision 3\n",
      "import codecs\n",
      "import nltk\n",
      "import random\n",
      "import re\n",
      "from collections import defaultdict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Twitter Sentiment Analysis with a MaxEnt Classifier"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Reading the corpus"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "A pure Python approach"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### A note on encoding\n",
      "\n",
      "The \"default\" `open()` in Python 2 (and most other programming languages) is \"poison\" for international text using non-ASCII encodings, as you need to **remember to *always* decode** any strings you read with Python 2.x's `open()` to the `unicode` type. With Python 3 on the other had, you really have a hard time breaking character encoding and that alone is [for me] more than enough reason to use it. But, Py3 libraries were hard to come by and you needed to do a LOT of your own plumbing. However, things are getting better recently!\n",
      "\n",
      "Similarly, the `csv` module of Python 2 **doesn\u2019t support Unicode** input. Also, there are currently some issues regarding ASCII NUL characters. Accordingly, all input should be printable ASCII to be safe, but there is a way to make `csv` in Python 2.7 \"Unicode-ready\", which is what we are doing here. This is simply code copy-pasted from the very bottom of the official `csv` [documentation](https://docs.python.org/2.7/library/csv.html)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv, cStringIO\n",
      "\n",
      "class UTF8Recoder:\n",
      "    \"\"\"\n",
      "    Iterator that reads an encoded stream and reencodes the input to UTF-8\n",
      "    \"\"\"\n",
      "    def __init__(self, f, encoding):\n",
      "        self.reader = codecs.getreader(encoding)(f)\n",
      "\n",
      "    def __iter__(self):\n",
      "        return self\n",
      "\n",
      "    def next(self):\n",
      "        return self.reader.next().encode(\"utf-8\")\n",
      "\n",
      "class UnicodeCSVReader:\n",
      "    \"\"\"\n",
      "    A CSV reader which will iterate over lines in the CSV file \"f\",\n",
      "    which is encoded in the given encoding.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, f, dialect=csv.excel, encoding=\"utf-8\", **kwds):\n",
      "        f = UTF8Recoder(f, encoding)\n",
      "        self.reader = csv.reader(f, dialect=dialect, **kwds)\n",
      "\n",
      "    def next(self):\n",
      "        row = self.reader.next()\n",
      "        return [unicode(s, \"utf-8\") for s in row]\n",
      "\n",
      "    def __iter__(self):\n",
      "        return self\n",
      "\n",
      "class UnicodeCSVWriter:\n",
      "    \"\"\"\n",
      "    A CSV writer which will write rows to CSV file \"f\",\n",
      "    which is encoded in the given encoding.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, f, dialect=csv.excel, encoding=\"utf-8\", **kwds):\n",
      "        # Redirect output to a queue\n",
      "        self.queue = cStringIO.StringIO()\n",
      "        self.writer = csv.writer(self.queue, dialect=dialect, **kwds)\n",
      "        self.stream = f\n",
      "        self.encoder = codecs.getincrementalencoder(encoding)()\n",
      "\n",
      "    def writerow(self, row):\n",
      "        self.writer.writerow([s.encode(\"utf-8\") for s in row])\n",
      "        # Fetch UTF-8 output from the queue ...\n",
      "        data = self.queue.getvalue()\n",
      "        data = data.decode(\"utf-8\")\n",
      "        # ... and reencode it into the target encoding\n",
      "        data = self.encoder.encode(data)\n",
      "        # write to the target stream\n",
      "        self.stream.write(data)\n",
      "        # empty queue\n",
      "        self.queue.truncate(0)\n",
      "\n",
      "    def writerows(self, rows):\n",
      "        for row in rows:\n",
      "            self.writerow(row)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reader = UnicodeCSVReader(open('twitter_raw/tweet_product_sentiment.csv', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Reading off the first line:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reader.next()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[u'tweet_text',\n",
        " u'emotion_in_tweet_is_directed_at',\n",
        " u'is_there_an_emotion_directed_at_a_brand_or_product']"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Field indices:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TWEET = 0\n",
      "EMOTION = 1\n",
      "PRODUCT = 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus = [tweet for tweet in reader]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We really have properly decode the Unicode text, as we did not get an Unicode de- or encoding error!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(corpus[0][0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "unicode"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Despite the number of lines in the file is more, the CSV reader took care of that issue for us!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "9093"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A nifty code pattern you might want to remeber for splitting columns in each row into separate variables:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tweets, products, emotions = zip(*corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "set(emotions)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "{u\"I can't tell\",\n",
        " u'Negative emotion',\n",
        " u'No emotion toward brand or product',\n",
        " u'Positive emotion'}"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "set(products)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "{u'',\n",
        " u'Android',\n",
        " u'Android App',\n",
        " u'Apple',\n",
        " u'Google',\n",
        " u'Other Apple product or service',\n",
        " u'Other Google product or service',\n",
        " u'iPad',\n",
        " u'iPad or iPhone App',\n",
        " u'iPhone'}"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice we have an \"empty\" (*undefined*) product category!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[tweets[i] for i in range(len(tweets)) if products[i] == \"\"][:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "[u'@teachntech00 New iPad Apps For #SpeechTherapy And Communication Are Showcased At The #SXSW Conference http://ht.ly/49n4M #iear #edchat #asd',\n",
        " u'',\n",
        " u'Holler Gram for iPad on the iTunes App Store -  http://t.co/kfN3f5Q (via @marc_is_ken) #sxsw',\n",
        " u'Attn: All  #SXSW frineds, @mention Register for #GDGTLive  and see Cobra iRadar for Android. {link}',\n",
        " u'Anyone at  #sxsw want to sell their old iPad?',\n",
        " u'Anyone at  #SXSW who bought the new iPad want to sell their older iPad to me?',\n",
        " u'At #sxsw.  Oooh. RT @mention Google to Launch Major New Social Network Called Circles, Possibly Today {link}',\n",
        " u'SPIN Play - a new concept in music discovery for your iPad from @mention &amp; spin.com {link} #iTunes #sxsw @mention',\n",
        " u'VatorNews - Google And Apple Force Print Media to Evolve? {link} #sxsw',\n",
        " u'HootSuite - HootSuite Mobile for #SXSW ~ Updates for iPhone, BlackBerry &amp; Android: Whether you\\u2019re getting friend... {link}']"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Cleaning/pruning: Replace newlines and tabs with spaces"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tweets = [t.replace('\\n', ' ').replace('\\t', ' ') for t in tweets]\n",
      "corpus = [(row[0].replace('\\n', ' ').replace('\\t', ' '), row[1], row[2]) for row in corpus]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets write the pruned corpus back out, but using tab-separation, now that we have ensured that each tweet is on a single line and the only tabs will be our field/cell separators."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reader = UnicodeCSVWriter(open('twitter_raw/fixed.tsv', 'wb'), dialect=csv.excel_tab)\n",
      "reader.writerows(corpus)\n",
      "reader.stream.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "A NLTK corpus apporach"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After converting the \"fixed\" corpus into plain-text files with one tweet per line, they are placed into two directories (for products and sentiment), thereby separating the tweets by each product or sentiment label into their own file.\n",
      "\n",
      "Now we will use NLTK to load this corpus."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import CategorizedPlaintextCorpusReader as CPCReader\n",
      "# NLTK3: nltk.corpus.reader"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "CPCReader??"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define a pattern to split the corpus into two categories and create the reader:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file_id_regex = r'((sentiment|product)/.*)\\.txt' # notice the r before the regex!\n",
      "documents = CPCReader(\"twitter_corpus\", file_id_regex, cat_pattern=file_id_regex)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Reading one tweet per line\n",
      "\n",
      "**Meta-programming** pattern ahead (\"monkey patching\") - handle with care!\n",
      "\n",
      "In a nutshell, we're overriding the (private) sentence segmentation implementation (`_read_sent_block`) of the `CategorizedPlaintextCorpusReader` by *dynamically adding* a modified *instance method* that segments on line-breaks, replacing an equally named method defined on the class, so we can read the corpus per tweet with the (public) `documents.sents()` method:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import types\n",
      "\n",
      "def read_sent_block(self, stream):\n",
      "    sents = []\n",
      "    \n",
      "    for para in self._para_block_reader(stream):\n",
      "        sents.extend(para.decode('utf-8').splitlines())\n",
      "\n",
      "    return sents\n",
      "\n",
      "documents._read_sent_block = types.MethodType(read_sent_block, documents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "NLTK provides a few methods to \"explore\" the corpus:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "documents.categories()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "['product/android',\n",
        " 'product/android_app',\n",
        " 'product/apple',\n",
        " 'product/apple_other',\n",
        " 'product/google',\n",
        " 'product/google_other',\n",
        " 'product/ios_app',\n",
        " 'product/ipad',\n",
        " 'product/iphone',\n",
        " 'product/none',\n",
        " 'sentiment/negative',\n",
        " 'sentiment/none',\n",
        " 'sentiment/positive',\n",
        " 'sentiment/unknown']"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "documents.fileids()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "['product/android.txt',\n",
        " 'product/android_app.txt',\n",
        " 'product/apple.txt',\n",
        " 'product/apple_other.txt',\n",
        " 'product/google.txt',\n",
        " 'product/google_other.txt',\n",
        " 'product/ios_app.txt',\n",
        " 'product/ipad.txt',\n",
        " 'product/iphone.txt',\n",
        " 'product/none.txt',\n",
        " 'sentiment/negative.txt',\n",
        " 'sentiment/none.txt',\n",
        " 'sentiment/positive.txt',\n",
        " 'sentiment/unknown.txt']"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SENTIMENTS = [\n",
      " 'sentiment/negative',\n",
      " 'sentiment/none',\n",
      " 'sentiment/positive',\n",
      " 'sentiment/unknown'\n",
      "]\n",
      "PRODUCTS = [\n",
      " 'product/android',\n",
      " 'product/android_app',\n",
      " 'product/apple',\n",
      " 'product/apple_other',\n",
      " 'product/google',\n",
      " 'product/google_other',\n",
      " 'product/ios_app',\n",
      " 'product/ipad',\n",
      " 'product/iphone',\n",
      " 'product/none',\n",
      "]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "documents.categories('product/android.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "['product/android']"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "no_product = documents.sents('product/none.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "no_product[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "[u'@teachntech00 New iPad Apps For #SpeechTherapy And Communication Are Showcased At The #SXSW Conference http://ht.ly/49n4M #iear #edchat #asd',\n",
        " u'Holler Gram for iPad on the iTunes App Store -  http://t.co/kfN3f5Q (via @marc_is_ken) #sxsw',\n",
        " u'Attn: All  #SXSW frineds, @mention Register for #GDGTLive  and see Cobra iRadar for Android. {link}',\n",
        " u'Anyone at  #sxsw want to sell their old iPad?',\n",
        " u'Anyone at  #SXSW who bought the new iPad want to sell their older iPad to me?',\n",
        " u'At #sxsw.  Oooh. RT @mention Google to Launch Major New Social Network Called Circles, Possibly Today {link}',\n",
        " u'SPIN Play - a new concept in music discovery for your iPad from @mention &amp; spin.com {link} #iTunes #sxsw @mention',\n",
        " u'VatorNews - Google And Apple Force Print Media to Evolve? {link} #sxsw',\n",
        " u'HootSuite - HootSuite Mobile for #SXSW ~ Updates for iPhone, BlackBerry &amp; Android: Whether you\\u2019re getting friend... {link}',\n",
        " u'Hey #SXSW - How long do you think it takes us to make an iPhone case? answer @mention using #zazzlesxsw and we\\u2019ll make you one!']"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Because of our \"programming-jungle black magic\" (monkey patching), we now are abusing the corpus-reader's sentence segmentation functionality to simply split at each newline (i.e., at each tweet)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(documents.sents(categories=SENTIMENTS))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "9092"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Or are we? If you check the actual corpus, it has 9093 tweets. What happened? Well, the NLTK corpus readers ignore empty lines, and there was one \"empty tweet\" in the dataset. Either you can consider this a bad thing and work it out, or just be happy and think \"NLTK did some corpus pruning for me for free\"..."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Determining a tokenization strategy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.tokenize?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "example = no_product[96]\n",
      "print example"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u263a We love 2 entertain you\u2026Please don\u2019t be grateful! \u263c {link} \u2126 #edchat #musedchat #sxsw #sxswi #classical #newTwitter\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`nltk.tokenize.sent_tokenize` (as expected) splits (only) into sentences"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "for s in nltk.tokenize.sent_tokenize(example): print s, \"|\","
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u263a We love 2 entertain you\u2026Please don\u2019t be grateful! | \u263c {link} \u2126 #edchat #musedchat #sxsw #sxswi #classical #newTwitter |\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`nltk.tokenize.word_tokenize` splits words, but maintains final dot: could be an abbreviation marker!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "for t in nltk.tokenize.word_tokenize(example): print t, \"|\","
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u263a | We | love | 2 | entertain | you\u2026Please | don\u2019t | be | grateful | ! | \u263c | { | link | } | \u2126 | # | edchat | # | musedchat | # | sxsw | # | sxswi | # | classical | # | newTwitter |\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`nltk.tokenize.wordpunct_tokenize` splits between alphanums and symbols, but look at the first token: we do not want that in on piece, do we?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "for t in nltk.tokenize.wordpunct_tokenize(example): print t, \"|\","
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u263a | We | love | 2 | entertain | you | \u2026 | Please | don | \u2019 | t | be | grateful | ! | \u263c | { | link | } | \u2126 | # | edchat | # | musedchat | # | sxsw | # | sxswi | # | classical | # | newTwitter |\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Solution: let's combine the sentence and word tokenizers (we could as well try to stick with `wordpunct_tokenize`!)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "for s in nltk.tokenize.sent_tokenize(example):\n",
      "    for t in nltk.tokenize.wordpunct_tokenize(s):\n",
      "        print t, \"|\","
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u263a | We | love | 2 | entertain | you | \u2026 | Please | don | \u2019 | t | be | grateful | ! | \u263c | { | link | } | \u2126 | # | edchat | # | musedchat | # | sxsw | # | sxswi | # | classical | # | newTwitter |\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Wrapping that up in a function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import sent_tokenize, wordpunct_tokenize\n",
      "\n",
      "def tokenize(tweet):\n",
      "    return [wordpunct_tokenize(sentence) for sentence in sent_tokenize(tweet)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "for s in tokenize(example):\n",
      "    for t in s:\n",
      "        print t,\n",
      "    print \"\\n\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u263a We love 2 entertain you \u2026 Please don \u2019 t be grateful ! \n",
        "\n",
        "\u263c { link } \u2126 # edchat # musedchat # sxsw # sxswi # classical # newTwitter \n",
        "\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "tokenize(\"hi :-)\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "[['hi', ':-)']]"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Normally, we'd have used the `nltk.word_tokenize` function, but due to the strange characters in tweets, the `wordpunct_tokenize` function turned out to be more effective. Note that this is at best an \"OK\" strategy. We are splitting \"don't\" and similar contractions into three tokens, which in particular might be not what we want: ideally, we would convert those `[` ...`n, ', t ]` triplets to bigrams of the form `[` ...`, not ]` and join the two final tokens generated from a possesive s to a single \"`'s`\" token. When inspecting more tweet, other tokenization issues might appear, too."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Observations about the corpus data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While working with the data - in reality, you would have been staring at tweets, desperately trying to figure out what else to extract other than n-grams - we discovered a few issues that we need to fix first:\n",
      "\n",
      "Some tweets contain HTML Entities and/or URLs - importing an HTML Entity escaping function and defining a URL normalization pattern "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from HTMLParser import HTMLParser\n",
      "unescape = HTMLParser().unescape\n",
      "URL = re.compile('http://[^ ]+')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's modify the injected method to immediately produce a tokenized version of the tweets:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def read_sent_block(self, stream):\n",
      "    tweets = []\n",
      "    \n",
      "    for para in self._para_block_reader(stream):\n",
      "        # NB that the corpus already contains some \"{link}\" entries!\n",
      "        para = unescape(URL.sub('{link}', para.decode('utf-8')).replace('{link}', 'LINK'))\n",
      "        tweets.extend([tokenize(tweet) for tweet in para.splitlines()])\n",
      "    \n",
      "    return tweets\n",
      "\n",
      "documents._read_sent_block = types.MethodType(read_sent_block, documents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Overall, the effect is:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "no_product = documents.sents('product/none.txt')\n",
      "\n",
      "for tweet in no_product[:3]:\n",
      "    for sent in tweet:\n",
      "        print u\" \".join(sent)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "@ teachntech00 New iPad Apps For # SpeechTherapy And Communication Are Showcased At The # SXSW Conference LINK # iear # edchat # asd\n",
        "Holler Gram for iPad on the iTunes App Store - LINK ( via @ marc_is_ken ) # sxsw\n",
        "Attn : All # SXSW frineds , @ mention Register for # GDGTLive and see Cobra iRadar for Android .\n",
        "LINK\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We also have seen that repeated letters are a common theme: \"huuungry!\" or \"noooo\". So we normalize letter sequences with repetitions of  more than two letters to **two** letters (i.e., to \"huungry\"), so that letter expansions are preserved, but regularized. This is not a feature, and should be part of our tokenization strategy already."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```regex\n",
      "s/(\\w)(\\1)(\\1+)/\\1\\2/g\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "letter_triplet = re.compile(r'(\\w)(\\1)(\\1+)')\n",
      "\n",
      "def normalize_repeated_chars(word):\n",
      "    return letter_triplet.sub(r'\\1\\2', word)\n",
      "\n",
      "normalize_repeated_chars(\"huuuuuungrrrry\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "'huungrry'"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll use this function later on."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Quick PoS tagging and token normalization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Uses the earlier function to normalize repeated characters. Note that PoS tagging is expensive (relative to everything else we are doing)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem.porter import PorterStemmer\n",
      "stemmer = PorterStemmer()\n",
      "\n",
      "def normalize(token):\n",
      "    norm = normalize_repeated_chars(stemmer.stem(token).lower())\n",
      "    return \"no\" if norm == \"noo\" else norm\n",
      "    \n",
      "def tag_and_stem(tokens):\n",
      "    return [(t[0], normalize(t[0]), t[1]) for t in nltk.pos_tag(tokens)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "print \"word\\tnorm\\ttag\\n\"\n",
      "\n",
      "for s in [tag_and_stem(sentence) for sentence in tokenize(example)]:\n",
      "    for t in s:\n",
      "        print \"%s\\t%s\\t%s\" % t"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "word\tnorm\ttag\n",
        "\n",
        "\u263a\t\u263a\tNN"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "We\twe\tPRP\n",
        "love\tlove\tVBP\n",
        "2\t2\tCD\n",
        "entertain\tentertain\tNN\n",
        "you\tyou\tPRP\n",
        "\u2026\t\u2026\tVBP\n",
        "Please\tpleas\tNNP\n",
        "don\tdon\tNN\n",
        "\u2019\t\u2019\t:\n",
        "t\tt\tNN\n",
        "be\tbe\tVB\n",
        "grateful\tgrate\tJJ\n",
        "!\t!\t.\n",
        "\u263c\t\u263c\tNN\n",
        "{\t{\t``\n",
        "link\tlink\tNN\n",
        "}\t}\t:\n",
        "\u2126\t\u03c9\t:\n",
        "#\t#\t#\n",
        "edchat\tedchat\tWDT\n",
        "#\t#\t#\n",
        "musedchat\tmusedchat\tRB\n",
        "#\t#\t#\n",
        "sxsw\tsxsw\tNN\n",
        "#\t#\t#\n",
        "sxswi\tsxswi\tNN\n",
        "#\t#\t#\n",
        "classical\tclassic\tJJ\n",
        "#\t#\t#\n",
        "newTwitter\tnewtwitt\tNNP\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exploring the MaxEnt classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.classify import maxent"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we learn about the classifier:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "maxent.MaxentClassifier?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There's a demo for maxent! Let's look at how to write a maxent classifier:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "maxent.demo??"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And here we learn how to run it and what the demo actually does:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.classify.util.names_demo??"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.classify.util.names_demo_features??"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In other words, NLTK's MaxentClassifier expects to be fed with this kind of data structure:\n",
      "\n",
      "```python\n",
      "MaxentClassifier(\n",
      "  [ # a list of per-document (feature-dictionary, label) tuples:\n",
      "    # from document 1:\n",
      "    ( { \"feature1_name\": value,\n",
      "        \"feature2_name\": value,\n",
      "        ... }, 'label' ),\n",
      "    # from document 2:\n",
      "    ( { \"feature1_name\": value,\n",
      "        ... }, 'label' ),\n",
      "    ...\n",
      "  ]\n",
      ")\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Compatibility issue**: NLTK dropped support for SciPy of its MaxEnt classifier (i.e., the C-based L-BFGS optimizer), so it now only comes with IIS (improved iterative scaling), an inferior (read: much slower) hillclimbing approach to the optimization problem. Only hope: they fix this, or, if you have huge amounts of training data, you better learn how to optimize Multinomial Logistic Regression in SciPy. For this course, however, its a bit too far out. The only alternative is to install [MegaM](http://www.umiacs.umd.edu/~hal/megam/), which is written in OCaml. At least on a Mac with Homebrew installed, setting up MegaM is not hard, although: `brew install megam`, while for Linux, the author provides binaries.\n",
      "\n",
      "**Bug issue**: Another side-effect is that you might have an old NLTK version installed that still tries to use SciPy, but a SciPy version that no longer provides the API NLTK expects. If that is the case, the `demo()` call below will fail with some error about SciPy. In that case, simply directly import the `maxent.py` package I provide here (Or, if you are a crack, move the original file in the NLTK backage away, and replace it with this file.) by uncommenting the import statement.\n",
      "\n",
      "**Halting the training**: \"interrupting the Kernel\" of the IPython Notebook (equivalent to `CTRL-C` in a [Linux/Mac] interpreter) will abort the training process."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import maxent"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "maxent.demo()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training classifier...\n",
        "  ==> Training (100 iterations)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "      Iteration    Log Likelihood    Accuracy\n",
        "      ---------------------------------------\n",
        "             1          -0.69315        0.374"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "             2          -0.61426        0.626"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "             3          -0.59970        0.626"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "             4          -0.58597        0.627"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "             5          -0.57305        0.633"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "      Training stopped: keyboard interrupt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Final          -0.56092        0.652"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Testing classifier...\n",
        "Accuracy: 0.6640"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Avg. log likelihood: -0.8424"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Unseen Names      P(Male)  P(Female)\n",
        "----------------------------------------\n",
        "  Octavius        *0.4776   0.5224\n",
        "  Thomasina        0.3155  *0.6845\n",
        "  Barnett         *0.4082   0.5918\n",
        "  Angelina         0.2415  *0.7585\n",
        "  Saunders        *0.4397   0.5603\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Preparing +/- subjectivity gazetteers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def subjclues_reader(path):\n",
      "    for line in codecs.open(path, encoding='utf-8'):\n",
      "        try:\n",
      "            yield dict([cell.split('=') for cell in line.split() if \"=\" in cell and not cell.endswith('=')])\n",
      "        except:\n",
      "            print line\n",
      "            raise"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "subjclues = list(subjclues_reader('subjclues.txt'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract(clues, positive=True, verbs=False):\n",
      "    polarity = ['negative', 'positive'][positive]\n",
      "    if verbs:\n",
      "        return frozenset([c['word1'] for c in clues if c['pos1'] == 'verb' and c['priorpolarity'] == polarity])\n",
      "    else:\n",
      "        return frozenset([c['word1'] for c in clues if c['pos1'] != 'verb' and c['priorpolarity'] == polarity])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pos_token_pg = extract(subjclues)\n",
      "pos_verb_pg = extract(subjclues, verbs=True)\n",
      "neg_token_pg = extract(subjclues, False)\n",
      "neg_verb_pg = extract(subjclues, False, verbs=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Side-Project: Finding Collocations with NLTK"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will be using bigrams; For a Na\u00efve Bayes classifier, here is how you could determine the N (here 200) most significant ones (note that NLTK also has a `TrigramCollocationFinder`):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bigram_finder = nltk.collocations.BigramCollocationFinder.from_documents([\n",
      "    t for s in documents.sents(categories=SENTIMENTS) for t in s\n",
      "])\n",
      "bigrams = frozenset(bigram_finder.nbest(nltk.metrics.BigramAssocMeasures.pmi, 200))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "for bg in list(bigrams)[:20]:\n",
      "    print u\" \".join(bg)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Grant Hill\n",
        "Sencha /#\n",
        "%? russian\n",
        "Salesperson Darryl\n",
        "Nik Daftary\n",
        "PRX equates\n",
        "BAD FORM\n",
        "PepsiCo Playground\n",
        "Evan Woods\n",
        "MORE WIRES\n",
        "Reveal Later\n",
        "Kaiju Hero\n",
        "Earthquake Victims\n",
        "Os Ipanemas\n",
        "Maudies Tacos\n",
        "Psycho woot\n",
        "Perimeter Mall\n",
        "Goal funnels\n",
        "Bandcamp Offering\n",
        "Floor64 Inc\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Defining features for the classifier"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How would we feed our own classifier? Like this:\n",
      "\n",
      "```python\n",
      "sentimentClassifier = MaxentClassifier([\n",
      "  (features(tweet), label) for tweet in corpus\n",
      "])\n",
      "```\n",
      "\n",
      "So, let's define some features we \"extract\" from our tokenized tweets:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def features(tweet):\n",
      "    f = dict()\n",
      "    f['*pos*count*'] = 0\n",
      "    f['*neg*count*'] = 0\n",
      "    f['*pos*verbs*'] = 0\n",
      "    f['*neg*verbs*'] = 0\n",
      "\n",
      "    for sentence in tweet:\n",
      "        words, stems, pos = zip(*tag_and_stem(sentence))\n",
      "        verbs = [s for s, p in zip(stems, pos) if p.startswith(\"VB\")]\n",
      "        f.update([(s, True) for s in stems])\n",
      "        f.update([(' '.join(b), True) for b in nltk.bigrams(stems)])\n",
      "        f.update([(' '.join(t), True) for t in nltk.trigrams(stems)])\n",
      "        f['*pos*count*'] += sum([w.lower() in pos_token_pg for w in words])\n",
      "        f['*neg*count*'] += sum([w.lower() in neg_token_pg for w in words])\n",
      "        f['*pos*verbs*'] += sum([v in pos_verb_pg for v in verbs])\n",
      "        f['*neg*verbs*'] += sum([v in neg_verb_pg for v in verbs])\n",
      "        \n",
      "        for pos in set(pos):\n",
      "            if pos not in f: f[pos] = 0\n",
      "            f[pos] += pos.count(pos)\n",
      "        \n",
      "    return f"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A light-weight (PoS-free) features alternative - turns out to be just as effective:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def features(tweet):\n",
      "    f = dict()\n",
      "    f['*pos*count*'] = 0\n",
      "    f['*neg*count*'] = 0\n",
      "    f['*pos*verbs*'] = 0\n",
      "    f['*neg*verbs*'] = 0\n",
      "\n",
      "    for sentence in tweet:\n",
      "        tokens = [w.lower() for w in sentence]\n",
      "        f.update([(w, True) for w in tokens])\n",
      "        f.update([(' '.join(b), True) for b in nltk.bigrams(tokens)])\n",
      "        f.update([(' '.join(t), True) for t in nltk.trigrams(tokens)])\n",
      "        f['*pos*count*'] += sum([w in pos_token_pg for w in tokens])\n",
      "        f['*neg*count*'] += sum([w in neg_token_pg for w in tokens])\n",
      "        f['*pos*verbs*'] += sum([v in pos_verb_pg for v in tokens])\n",
      "        f['*neg*verbs*'] += sum([v in neg_verb_pg for v in tokens])\n",
      "        \n",
      "    return f"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "print example"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u263a We love 2 entertain you\u2026Please don\u2019t be grateful! \u263c {link} \u2126 #edchat #musedchat #sxsw #sxswi #classical #newTwitter\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "print \" \".join([\n",
      "    u'\"%s\"%s' % (k, \"\" if v is True else \"=\" + str(v)) for\n",
      "    k, v in features(tokenize(unescape(example))).iteritems()\n",
      "])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\"\u263a we love\" \"please\" \"love 2 entertain\" \"sxswi # classical\" \"grateful\" \"musedchat #\" \"musedchat\" \"# sxswi\" \"# classical #\" \"musedchat # sxsw\" \"\u2019 t\" \"\u2026 please\" \"\u263c\" \"grateful !\" \"\u03c9\" \"*pos*count*\"=2 \"\u03c9 #\" \"be grateful !\" \"we love\" \"{ link }\" \"t\" \"sxswi\" \"edchat # musedchat\" \"you \u2026\" \"# classical\" \"don \u2019 t\" \"classical\" \"edchat #\" \"link\" \"\u263c { link\" \"#\" \"please don \u2019\" \"t be grateful\" \"\u263a we\" \"be\" \"we\" \"sxsw\" \"} \u03c9\" \"entertain\" \"# musedchat\" \"# sxsw\" \"} \u03c9 #\" \"newtwitter\" \"*neg*count*\"=0 \"we love 2\" \"{\" \"{ link\" \"love\" \"\u03c9 # edchat\" \"entertain you \u2026\" \"link }\" \"classical #\" \"please don\" \"don \u2019\" \"\u2026\" \"# newtwitter\" \"2\" \"2 entertain you\" \"\u2026 please don\" \"\u2019 t be\" \"# edchat\" \"love 2\" \"*neg*verbs*\"=0 \"sxsw # sxswi\" \"\u263a\" \"# edchat #\" \"# musedchat #\" \"# sxswi #\" \"classical # newtwitter\" \"\u2019\" \"# sxsw #\" \"be grateful\" \"entertain you\" \"!\" \"2 entertain\" \"you\" \"\u263c {\" \"t be\" \"you \u2026 please\" \"sxswi #\" \"edchat\" \"*pos*verbs*\"=3 \"don\" \"link } \u03c9\" \"sxsw #\" \"}\"\n"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that our fetaure generation leaves much left to be improved; Particularly, MaxEnt is a [logistic, therefore] linear classifier. As we can assume that words have a strong conditional dependency and behave non-linearly, it is important to make use of \"joint\" features, things like\n",
      "```python\n",
      "any(t in pos_token_pg for t in tokens) and brand_or_product_name in tokens\n",
      "```\n",
      "Our basic feature choices give us \"good\" results, but to get the expected 80% accuracy, we will need to do more than that."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Generation of all document features-label pairs for one category"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def generate(cats):\n",
      "    for fileid in documents.fileids(categories=cats):\n",
      "        label = documents.categories(fileid)[0]\n",
      "        for tweet in documents.sents(fileid):\n",
      "            yield ((features(tweet), label), tweet)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This next step will take quite a while; we need to generate all features for our collection. Particularly, the PoS tagging will take its time."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentimentFeatures = list(generate(SENTIMENTS))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "random.shuffle(sentimentFeatures)\n",
      "len(sentimentFeatures)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 54,
       "text": [
        "9083"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"; \".join(u'\"%s\"=%s' % (i[0], int(i[1])) for i in sentimentFeatures[0][0][0].iteritems())\n",
      "print sentimentFeatures[0][0][1]\n",
      "print \" \".join([t for s in sentimentFeatures[0][1] for t in s])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\"you rsvp\"=1; \"@ mention\"=1; \"an iphone ?\"=1; \"rt @\"=1; \"3 /\"=1; \"at #\"=1; \"at # sxsw\"=1; \"link perfect attendance\"=1; \"light bar !\"=1; \"perfect attendance\"=1; \"an\"=1; \"perfect attendance at\"=1; \"mention rsvp here\"=1; \"at\"=1; \"can you rsvp\"=1; \"perfect\"=1; \"o an\"=1; \"an iphone\"=1; \"light bar\"=1; \"bar !\"=1; \"iphone ?\"=1; \"<- please rt\"=1; \"/ 18\"=1; \"# sxsw !\"=1; \"rt\"=1; \"18 | light\"=1; \"#\"=1; \"attendance\"=1; \"rsvp here\"=1; \"-> link\"=1; \"attendance at\"=1; \"please\"=1; \"mention rsvp\"=1; \"/\"=1; \"/ 18 |\"=1; \"rt @ mention\"=1; \"3\"=1; \"<- please\"=1; \"iphone\"=1; \"| light\"=1; \"you\"=1; \"link perfect\"=1; \"?\"=1; \"rsvp\"=1; \"@\"=1; \"rsvp w\"=1; \"sxsw\"=1; \"can you\"=1; \"please rt\"=1; \"18 |\"=1; \"3 / 18\"=1; \"attendance at #\"=1; \"rsvp here ->\"=1; \"here\"=1; \"!\"=1; \"*pos*count*\"=2; \"mention\"=1; \"rsvp w /\"=1; \"link\"=1; \"*neg*verbs*\"=1; \"-> link perfect\"=1; \"bar\"=1; \"@ mention rsvp\"=1; \"<-\"=1; \"*pos*verbs*\"=2; \"w /\"=1; \"o an iphone\"=1; \"# sxsw\"=1; \"18\"=1; \"w / o\"=1; \"/ o\"=1; \"o\"=1; \"here -> link\"=1; \"/ o an\"=1; \"*neg*count*\"=0; \"can\"=1; \"w\"=1; \"light\"=1; \"sxsw !\"=1; \"| light bar\"=1; \"->\"=1; \"here ->\"=1; \"|\"=1; \"you rsvp w\"=1\n",
        "sentiment/none\n",
        "Can you rsvp w / o an iPhone ? RT @ mention RSVP HERE -> LINK Perfect Attendance at # SXSW ! 3 / 18 | LIGHT BAR ! <- PLEASE RT\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Training a MaxentClassifier"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Note* that I've installed [MegaM](http://www.umiacs.umd.edu/~hal/megam/) to use the faster L-BFGS optimizer rather than the \"built-in\" IIS approach as described earlier, otherwise we'd not see results in the course..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.classify import config_megam \n",
      "config_megam('/usr/local/Cellar/megam/0.9.2/bin/megam')\n",
      "algorithm = 'megam'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[Found /usr/local/Cellar/megam/0.9.2/bin/megam: /usr/local/Cellar/megam/0.9.2/bin/megam]\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To use \"regular\" the on-board (but slow) optimizer, evaluate this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "algorithm = 'IIS'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cutoff = len(sentimentFeatures)*90/100\n",
      "training_set, test_set = [f for f, t in sentimentFeatures[:cutoff]], [f for f, t in sentimentFeatures[cutoff:]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remember that you can interrupt the kernel to abort training and use the model you have so far."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentimentClassifier = maxent.MaxentClassifier.train(training_set, algorithm)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "nltk.classify.util.accuracy(sentimentClassifier, test_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 69,
       "text": [
        "0.722"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note how this function provides an easy way to implement label-specific precision, recall, and F-score."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def evaluate(classifier, collection):\n",
      "    refsets = defaultdict(set)\n",
      "    testsets = defaultdict(set)\n",
      "\n",
      "    for i, (feats, label) in enumerate(collection):\n",
      "        refsets[label].add(i)\n",
      "        observed = classifier.classify(feats)\n",
      "        testsets[observed].add(i)\n",
      " \n",
      "    for label in refsets:\n",
      "        for measure in [\n",
      "            nltk.metrics.precision,\n",
      "            nltk.metrics.recall,\n",
      "            nltk.metrics.f_measure\n",
      "        ]:\n",
      "            if testsets[label]:\n",
      "                print label, \"%9s\" % measure.__name__, \"%.3f\" % measure(refsets[label], testsets[label])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "evaluate(sentimentClassifier, test_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "sentiment/negative precision 0.643\n",
        "sentiment/negative    recall 0.397\n",
        "sentiment/negative f_measure 0.491\n",
        "sentiment/none precision 0.767\n",
        "sentiment/none    recall 0.827\n",
        "sentiment/none f_measure 0.796\n",
        "sentiment/positive precision 0.646\n",
        "sentiment/positive    recall 0.630\n",
        "sentiment/positive f_measure 0.638\n",
        "sentiment/unknown precision 0.000\n",
        "sentiment/unknown    recall 0.000\n",
        "sentiment/unknown f_measure 0.000\n"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we produce some of the results for inspections, first wrong cases, then a few correct cases."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "stop_after = 10\n",
      "\n",
      "for idx in range(cutoff, len(sentimentFeatures)):\n",
      "    if sentimentFeatures[idx][0][1] != sentimentClassifier.classify(sentimentFeatures[idx][0][0]):\n",
      "        print \"predicted\", sentimentClassifier.classify(sentimentFeatures[idx][0][0])\n",
      "        print \"true     \", sentimentFeatures[idx][0][1]\n",
      "        print \" \".join(t for s in sentimentFeatures[idx][1] for t in s)\n",
      "        print\n",
      "        stop_after -= 1\n",
      "        if not stop_after:\n",
      "            break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "predicted sentiment/none\n",
        "true      sentiment/positive\n",
        "Someone ' s buying herself a present ;-) RT @ mention Apple Is Opening A Pop - Up Store In Austin For # SXSW LINK\n",
        "\n",
        "predicted sentiment/none\n",
        "true      sentiment/positive\n",
        "RT @ mention No way ! ' Matt fricking Damon at Google party at Maggie mae ' s . # Sxsw @ mention LINK\n",
        "\n",
        "predicted sentiment/positive\n",
        "true      sentiment/none\n",
        "The prob . w / seeing @ mention at a # SXSW Interactive show is that these nerds are more concerned with their iphone videos than the actual show\n",
        "\n",
        "predicted sentiment/none\n",
        "true      sentiment/positive\n",
        "RT @ mention They are everywhere : It ' s just crazy to look around sxsw and realize last year no one had an iPad . # sxsw\n",
        "\n",
        "predicted sentiment/positive\n",
        "true      sentiment/negative\n",
        "Marissa Mayer : Google maps should have better customer service , quicker responses . # sxsw # FH\n",
        "\n",
        "predicted sentiment/positive\n",
        "true      sentiment/none\n",
        "RT @ mention At # sxsw ? We ' re giving away an iPad 2 to the creator of the most popular disc during interactive . Create a disc & share on Twitter !\n",
        "\n",
        "predicted sentiment/none\n",
        "true      sentiment/positive\n",
        "Prepping for and attending # SXSW without an iPhone is a dehumanizing experience .\n",
        "\n",
        "predicted sentiment/positive\n",
        "true      sentiment/none\n",
        "Get to the nav schemes I am done with the iPad shape lesson # sxsw\n",
        "\n",
        "predicted sentiment/positive\n",
        "true      sentiment/none\n",
        "RT @ mention 4G will do for connectivity what the iPhone did for smart phones - Joe Berry # SXSW # connectedcar\n",
        "\n",
        "predicted sentiment/positive\n",
        "true      sentiment/none\n",
        "Hey all you lucky ducks headed to # SXSW want to win an iPad ? @ mention ( client ) is giving one away . Details here : LINK # SXSWi\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "stop_after = 10\n",
      "\n",
      "for idx in range(cutoff, len(sentimentFeatures)):\n",
      "    if sentimentFeatures[idx][0][1] == sentimentClassifier.classify(sentimentFeatures[idx][0][0]):\n",
      "        print \"label\", sentimentFeatures[idx][0][1]\n",
      "        print \" \".join(t for s in sentimentFeatures[idx][1] for t in s)\n",
      "        print\n",
      "        stop_after -= 1\n",
      "        if not stop_after:\n",
      "            break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "label sentiment/none\n",
        "prepping up for # sxsw aka \" Nerds Gone Wild \" - here ' s a list of all the parties thanks @ mention LINK\n",
        "\n",
        "label sentiment/positive\n",
        "I got turkey ! RT @ mention Random Apple Fan just bought lunch for everyone in line at # SXSW Popup Apple Store # ipad2 cc / @ mention\n",
        "\n",
        "label sentiment/none\n",
        "Base Camp Apple . # SXSW LINK\n",
        "\n",
        "label sentiment/positive\n",
        "RT @ mention Woo hoo ! @ mention is finally back on the iPhone . LINK # sxsw # il\n",
        "\n",
        "label sentiment/none\n",
        "Afraid being at # SXSW would make you miss out on buying the new iPad ? No worries - Apple is opening a pop up shop there LINK\n",
        "\n",
        "label sentiment/positive\n",
        "RT @ mention Love that @ mention set up a pop - up store ( 6th & Congress ) to deal with mass geek influx during ipad2 release # sxsw\n",
        "\n",
        "label sentiment/none\n",
        "Just had my 1st 27 \" LCD cinema display FaceTime to IPad 2 FaceTime with @ mention ... # sxsw bar in my living room lol !\n",
        "\n",
        "label sentiment/none\n",
        "Today ' s sessions ( 1 of 2 ) at # sxsw -- Print design as future : LINK | Your Mom / iPad : LINK\n",
        "\n",
        "label sentiment/positive\n",
        "Congrats to @ mention on winning the last @ mention # iPad case for her boyfriend aw :) # SXSW # cbatsxsw\n",
        "\n",
        "label sentiment/positive\n",
        "Bummed to miss # SXSW this year , but we ' re all quarantined with the same kiddie cold , and I have GarageBand on my iPad to comfort me . # fb\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we can inspect which features have the most extreme weights (lambdas)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "sentimentClassifier.show_most_informative_features(25)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  -4.622 @ mention==True and label is 'sentiment/negative'\n",
        "   2.943 rt @==True and label is 'sentiment/negative'\n",
        "   2.298 mention==True and label is 'sentiment/negative'\n",
        "   2.277 label is 'sentiment/none'\n",
        "   1.917 rt==True and label is 'sentiment/none'\n",
        "  -1.885 *neg*count*==0 and label is 'sentiment/negative'\n",
        "   1.842 # sxsw==True and label is 'sentiment/negative'\n",
        "   1.609 cool==True and label is 'sentiment/positive'\n",
        "   1.523 rt @ mention==True and label is 'sentiment/none'\n",
        "   1.496 @==True and label is 'sentiment/negative'\n",
        "  -1.451 iphone==True and label is 'sentiment/none'\n",
        "   1.414 &==True and label is 'sentiment/none'\n",
        "  -1.407 ipad==True and label is 'sentiment/none'\n",
        "   1.341 ipad==True and label is 'sentiment/positive'\n",
        "  -1.287 *pos*count*==0 and label is 'sentiment/positive'\n",
        "  -1.253 cool==True and label is 'sentiment/none'\n",
        "   1.194 not==True and label is 'sentiment/negative'\n",
        "   1.185 rt @==True and label is 'sentiment/positive'\n",
        "   1.174 !==True and label is 'sentiment/positive'\n",
        "   1.136 sxsw==True and label is 'sentiment/none'\n",
        "   1.129 at #==True and label is 'sentiment/positive'\n",
        "   1.122 awesome==True and label is 'sentiment/positive'\n",
        "   1.120 great==True and label is 'sentiment/positive'\n",
        "  -1.102 not==True and label is 'sentiment/positive'\n",
        "   1.094 smart==True and label is 'sentiment/positive'\n"
       ]
      }
     ],
     "prompt_number": 74
    }
   ],
   "metadata": {}
  }
 ]
}