{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization, stemming, and sentence segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical course material for the ASDM Class 09 (Text Mining) by Florian Leitner.\n",
    "\n",
    "Â© 2016 Florian Leitner. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today's lab will cover some of the basic techniques to work with text: tokenization, stemming, and word embeddings.\n",
    "\n",
    "First we run the standard setup seen in the preparatory notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline --no-import-all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another (optional) preparatory step: Install the `segtok` tokenization and segmentation library created by your instructor:\n",
    "\n",
    "- Anaconda Python: `conda install segtok`\n",
    "- Stock Python: `pip3 install segtok`\n",
    "\n",
    "Once done, the following import should work; if not, you can always skip the `segtok`-based examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import segtok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the next command (`import nltk`...) does not succeed, remember to install NLTK first (day 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileSystemPathPointer('/Users/fleitner/nltk_data/corpora/gutenberg/carroll-alice.txt')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.data.find('corpora/gutenberg/carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alice = nltk.data.load('corpora/gutenberg/carroll-alice.txt', format='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into the\n",
      "book her sister was reading, but it had no pictures or conversations in\n",
      "it, 'and what is the use of a book,' thought Alice 'without pictures or\n",
      "conversation?'\n",
      "\n",
      "So she was considering in her own mind (as well as she could, for the\n",
      "hot day made her feel very sleepy and stupid), whether the pleasure\n",
      "of making a ...\n"
     ]
    }
   ],
   "source": [
    "print(alice[:543], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplistic tokenization: white-space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"[Alice's\", 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865]', 'CHAPTER', 'I.', 'Down', 'the', 'Rabbit-Hole', 'Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank,', 'and', 'of', 'having', 'nothing', 'to', 'do:', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading,', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it,', \"'and\", 'what', 'is', 'the', 'use', 'of', 'a', \"book,'\", 'thought', 'Alice', \"'without\", 'pictures', 'or', \"conversation?'\", 'So', 'she', 'was', 'considering', 'in', 'her', 'own', 'mind', '(as', 'well', 'as', 'she', 'could,', 'for', 'the', 'hot', 'day', 'made', 'her', 'feel', 'very', 'sleepy', 'and', 'stupid),', 'whether', 'the', 'pleasure', 'of', 'making', 'a']\n"
     ]
    }
   ],
   "source": [
    "print(alice.split()[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the above result. Can you tell if this is a good strategy or not? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard tokenization: letters and numbers only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = re.compile(r'\\w+', re.UNICODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that with the above pattern, we leave all dots attached to the tokens. That is because then the tokenizer output can be used as input for a sentence segmentation algorithm that should decide if the dot is used as an abbrevation marker (and stay attached) or as sentence terminal (and should be split it off as its own token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alice', 's', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865', 'CHAPTER', 'I', 'Down', 'the', 'Rabbit', 'Hole', 'Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', 'and', 'of', 'having', 'nothing', 'to', 'do', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', 'and', 'what', 'is', 'the', 'use', 'of', 'a', 'book', 'thought', 'Alice', 'without', 'pictures', 'or', 'conversation', 'So', 'she', 'was', 'considering', 'in', 'her', 'own', 'mind', 'as', 'well', 'as', 'she', 'could', 'for', 'the', 'hot', 'day', 'made', 'her', 'feel', 'very', 'sleepy', 'and', 'stupid', 'whether', 'the', 'pleasure', 'of']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.findall(alice)[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clearly looks much better; We have actual words and numbers and this indeed is useful. But some problems remain: \n",
    "\n",
    "- over-tokenization, e.g., a time string like \"19:30\", or, in the above example, the hyphen or the possessive s\n",
    "- loss of information (punctuation)\n",
    "\n",
    "The latter problem (loss of punctuation) can be overcome, to some extent, by allowing some of the cases (e.g. ASCII hyphens, dots and dashes, as above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = re.compile(r'(\\'?[\\w.-]+)', re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def punctuation_tokenizer(text):\n",
    "    tokens = tokenizer.split(text)\n",
    "    return [t for t in map(str.strip, tokens) if t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Alice', \"'s\", 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865', ']', 'CHAPTER', 'I.', 'Down', 'the', 'Rabbit-Hole', 'Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'having', 'nothing', 'to', 'do', ':', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', ',', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', ',', \"'and\", 'what', 'is', 'the', 'use', 'of', 'a', 'book', \",'\", 'thought', 'Alice', \"'without\", 'pictures', 'or', 'conversation', \"?'\", 'So', 'she', 'was', 'considering', 'in', 'her', 'own', 'mind', '(', 'as', 'well', 'as', 'she', 'could', ',', 'for', 'the', 'hot', 'day', 'made', 'her']\n"
     ]
    }
   ],
   "source": [
    "print(punctuation_tokenizer(alice)[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced tokenization: NLTK and `segtok`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize as word_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know how the NLTK tokenizer is implemented, here are the gory details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nltk.tokenize.treebank??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar tokenizer with modular functionality is provided in the `segtok` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from segtok.tokenizer import web_tokenizer\n",
    "from segtok.tokenizer import split_contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into the\n",
      "book her sister was reading, but it had no pictures or conversations in\n",
      "it, 'and what is the use of a book,' thought Alice 'without pictures or\n",
      "conversation?'\n",
      "\n",
      "So she was considering in her own mind (as well as she could, for the\n",
      "hot day\n"
     ]
    }
   ],
   "source": [
    "print(alice[:471])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Alice', \"'s\", 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865', ']', 'CHAPTER', 'I', '.', 'Down', 'the', 'Rabbit-Hole', 'Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'having', 'nothing', 'to', 'do', ':', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', ',', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', ',', \"'and\", 'what', 'is', 'the', 'use', 'of', 'a', 'book', ',', \"'\", 'thought', 'Alice', \"'without\", 'pictures', 'or', 'conversation', '?', \"'\", 'So', 'she', 'was', 'considering', 'in', 'her', 'own', 'mind', '(', 'as', 'well', 'as', 'she', 'could', ',', 'for', 'the', 'hot']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenizer(alice)[:100]) # nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Alice', \"'s\", 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865', ']', 'CHAPTER', 'I.', 'Down', 'the', 'Rabbit-Hole', 'Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'having', 'nothing', 'to', 'do', ':', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', ',', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', ',', \"'\", 'and', 'what', 'is', 'the', 'use', 'of', 'a', 'book', \",'\", 'thought', 'Alice', \"'\", 'without', 'pictures', 'or', 'conversation', \"?'\", 'So', 'she', 'was', 'considering', 'in', 'her', 'own', 'mind', '(', 'as', 'well', 'as', 'she', 'could', ',', 'for', 'the', 'hot', 'day']\n"
     ]
    }
   ],
   "source": [
    "print(split_contractions(web_tokenizer(alice))[:100]) # segtok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, results seem pretty similar, with the differences being in the fine-print.\n",
    "Here are some really hard examples to gauge the performance of a tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = \"\"\"\n",
    "Chemical formulas like \"[Alâ(SâOâ)â]Â²â»\" or, dates as in 31/12/01, times like in 19:30\n",
    "or IP addresses such as 192.168.0.1, units like 10 mÂ³ or money as in US$ 10,000.00\n",
    "and even names as in \"Mr. Lewis Carroll\" can all spell trouble. And, do not forget\n",
    "those pesky web and email adresses: http://www.company.com/index.htm and first.last@company.com!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chemical', 'formulas', 'like', '\"[', 'Alâ', '(', 'SâOâ', ')', 'â', ']', 'Â²', 'â»\"', 'or', ',', 'dates', 'as', 'in', '31', '/', '12', '/', '01', ',', 'times', 'like', 'in', '19', ':', '30', 'or', 'IP', 'addresses', 'such', 'as', '192.168.0.1', ',', 'units', 'like', '10', 'mÂ³', 'or', 'money', 'as', 'in', 'US', '$', '10', ',', '000.00', 'and', 'even', 'names', 'as', 'in', '\"', 'Mr.', 'Lewis', 'Carroll', '\"', 'can', 'all', 'spell', 'trouble.', 'And', ',', 'do', 'not', 'forget', 'those', 'pesky', 'web', 'and', 'email', 'adresses', ':', 'http', '://', 'www.company.com', '/', 'index.htm', 'and', 'first.last', '@', 'company.com', '!']\n"
     ]
    }
   ],
   "source": [
    "print(punctuation_tokenizer(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chemical', 'formulas', 'like', '``', '[', 'Alâ', '(', 'SâOâ', ')', 'â', ']', 'Â²â»', \"''\", 'or', ',', 'dates', 'as', 'in', '31/12/01', ',', 'times', 'like', 'in', '19:30', 'or', 'IP', 'addresses', 'such', 'as', '192.168.0.1', ',', 'units', 'like', '10', 'mÂ³', 'or', 'money', 'as', 'in', 'US', '$', '10,000.00', 'and', 'even', 'names', 'as', 'in', '``', 'Mr.', 'Lewis', 'Carroll', \"''\", 'can', 'all', 'spell', 'trouble', '.', 'And', ',', 'do', 'not', 'forget', 'those', 'pesky', 'web', 'and', 'email', 'adresses', ':', 'http', ':', '//www.company.com/index.htm', 'and', 'first.last', '@', 'company.com', '!']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.word_tokenize(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chemical', 'formulas', 'like', '\"[', 'Alâ', '(', 'SâOâ', ')â]Â²â»\"', 'or', ',', 'dates', 'as', 'in', '31', '/', '12', '/', '01', ',', 'times', 'like', 'in', '19:30', 'or', 'IP', 'addresses', 'such', 'as', '192.168.0.1', ',', 'units', 'like', '10', 'mÂ³', 'or', 'money', 'as', 'in', 'US', '$', '10,000.00', 'and', 'even', 'names', 'as', 'in', '\"', 'Mr.', 'Lewis', 'Carroll', '\"', 'can', 'all', 'spell', 'trouble.', 'And', ',', 'do', 'not', 'forget', 'those', 'pesky', 'web', 'and', 'email', 'adresses', ':', 'http://www.company.com/index.htm', 'and', 'first.last', '@', 'company.com', '!']\n"
     ]
    }
   ],
   "source": [
    "print(web_tokenizer(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, even our basic tokenizer isn't all that bad, but you get more tokens that you might want.\n",
    "If there is no other reason to decide, you can use the speed of the tokenizers as another data-point to make a decision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loops, best of 3: 63.9 Âµs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit punctuation_tokenizer(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 3: 788 Âµs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit nltk.word_tokenize(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 3: 500 Âµs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit web_tokenizer(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, NLTK and `segtok` are about the same as fast; But if all you need is speed and can live with over-tokenization, our basic regular expression tokenizer is an order of magnitude faster than either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK provides you with many different stemming algorithms; Here we will only explore one as an example. But look at their API to find more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.stem.SnowballStemmer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alice_stemmed = [stemmer.stem(t) for t in segtok.tokenizer.word_tokenizer(alice)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'alic', 'adventur', 'in', 'wonderland', 'by', 'lewi', 'carrol', '1865', ']', 'chapter', 'i.', 'down', 'the', 'rabbit-hol', 'alic', 'was', 'begin', 'to', 'get', 'veri', 'tire', 'of', 'sit', 'by', 'her', 'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'have', 'noth', 'to', 'do', ':', 'onc', 'or', 'twice', 'she', 'had', 'peep', 'into', 'the', 'book', 'her', 'sister', 'was', 'read', ',', 'but', 'it', 'had', 'no', 'pictur', 'or', 'convers', 'in', 'it', ',', \"'\", 'and', 'what', 'is', 'the', 'use', 'of', 'a', 'book', \",'\", 'thought', 'alic', \"'\", 'without', 'pictur', 'or', 'convers', \"?'\", 'so', 'she', 'was', 'consid', 'in', 'her', 'own', 'mind', '(', 'as', 'well', 'as', 'she', 'could', ',', 'for', 'the', 'hot', 'day', 'made']\n"
     ]
    }
   ],
   "source": [
    "print(alice_stemmed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "NLTK's current default sentence splitter (available as `nltk.sent_tokenize`) is an implmentation of the [Punkt Sentence Tokenizer](http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.punkt) with the properties discussed in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How funny it'll seem to come out among the people that walk with\n",
      "their heads downward!\n",
      "\n",
      "The Antipathies, I think--' (she was rather glad\n",
      "there WAS no one listening, this time, as it didn't sound at all the\n",
      "right word) '--but I shall have to ask them what the name of the country\n",
      "is, you know.\n",
      "\n",
      "Please, Ma'am, is this New Zealand or Australia?'\n",
      "\n",
      "(and\n",
      "she tried to curtsey as she spoke--fancy CURTSEYING as you're falling\n",
      "through the air!\n",
      "\n",
      "Do you think you could manage it?)\n",
      "\n",
      "'And what an\n",
      "ignorant little girl she'll think me for asking!\n",
      "\n",
      "No, it'll never do to\n",
      "ask: perhaps I shall see it written up somewhere.'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\".join(nltk.sent_tokenize(alice)[26:33]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This happens to be the sentences segmenter (model for English) that can be loaded explicitly like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, the `segtok` package provides a rule-based sentence splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from segtok.segmenter import split_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#segtok.segmenter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split_multi??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How funny it'll seem to come out among the people that walk with\n",
      "their heads downward!\n",
      "\n",
      "The Antipathies, I think--' (she was rather glad\n",
      "there WAS no one listening, this time, as it didn't sound at all the\n",
      "right word) '--but I shall have to ask them what the name of the country\n",
      "is, you know.\n",
      "\n",
      "Please, Ma'am, is this New Zealand or Australia?'\n",
      "\n",
      "(and\n",
      "she tried to curtsey as she spoke--fancy CURTSEYING as you're falling\n",
      "through the air! Do you think you could manage it?)\n",
      "\n",
      "'And what an\n",
      "ignorant little girl she'll think me for asking!\n",
      "\n",
      "No, it'll never do to\n",
      "ask: perhaps I shall see it written up somewhere.'\n",
      "\n",
      "Down, down, down.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\".join(list(split_multi(alice))[28:35]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tricky_stuff = \"\"\"\n",
    "Species mentions like S. lividans or H. sapiens and Mr. Name is easy.\n",
    "Periods in Mr. Smith and Johann S. Bach do not mark boundaries.\n",
    "i is a good variable name.\n",
    "Â¡Good sentence segmentation is hard!\n",
    "(1) First things go here.\n",
    "(2) And second goes here.\n",
    "(3) Last, but not least.\n",
    "1. This is one.\n",
    "2. And that is two.\n",
    "3. Finally, three, too.\n",
    "It is expected, on the basis of (Olmsted, M. C., C. F. Anderson, and\n",
    "M. T. Record, Jr. 1989. Proc. Natl. Acad. Sci. USA. 100:100), to decrease sharply.\n",
    "And there are many other things... Would you not think?\n",
    "(How does it deal with this parenthesis?)\n",
    "\"It should be part of the previous sentence.\"\n",
    "\"(And the same with this one.)\"\n",
    "('And this one!')\n",
    "\"('(And (this)) '?)\"\n",
    "[(and this. )]\n",
    "That's it, folks!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Species mentions like S. lividans or H. sapiens and Mr. Name is easy.\n",
      "Periods in Mr. Smith and Johann S. Bach do not mark boundaries.\n",
      "i is a good variable name.\n",
      "Â¡Good sentence segmentation is hard!\n",
      "(1) First things go here.\n",
      "(2) And second goes here.\n",
      "(3) Last, but not least.\n",
      "1.\n",
      "This is one.\n",
      "2.\n",
      "And that is two.\n",
      "3.\n",
      "Finally, three, too.\n",
      "It is expected, on the basis of (Olmsted, M. C., C. F. Anderson, and M. T. Record, Jr. 1989.\n",
      "Proc.\n",
      "Natl.\n",
      "Acad.\n",
      "Sci.\n",
      "USA.\n",
      "100:100), to decrease sharply.\n",
      "And there are many other things... Would you not think?\n",
      "(How does it deal with this parenthesis?)\n",
      "\"It should be part of the previous sentence.\"\n",
      "\"(And the same with this one.)\"\n",
      "('And this one!')\n",
      "\"('(And (this)) '?)\"\n",
      "[(and this. )]\n",
      "That's it, folks! \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(nltk.sent_tokenize(tricky_stuff.replace('\\n', ' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Species mentions like S. lividans or H. sapiens and Mr. Name is easy.\n",
      "Periods in Mr. Smith and Johann S.\n",
      "Bach do not mark boundaries.\n",
      "i is a good variable name.\n",
      "Â¡Good sentence segmentation is hard!\n",
      "(1) First things go here.\n",
      "(2) And second goes here.\n",
      "(3) Last, but not least.\n",
      "1. This is one.\n",
      "2. And that is two.\n",
      "3. Finally, three, too.\n",
      "It is expected, on the basis of (Olmsted, M. C., C. F. Anderson, and M. T. Record, Jr. 1989. Proc. Natl. Acad. Sci. USA. 100:100), to decrease sharply.\n",
      "And there are many other things...\n",
      "Would you not think?\n",
      "(How does it deal with this parenthesis?)\n",
      "\"It should be part of the previous sentence.\"\n",
      "\"(And the same with this one.)\" ('And this one!')\n",
      "\"('(And (this)) '?)\" [(and this. )] That's it, folks!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(split_multi(tricky_stuff.replace('\\n', ' '))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a general rule of thumb: If you have a orthographically correct text (scientific publications, books, etc.), it probably pays of to develop a rule-based segmenter. If not, or you lack the time to develop good rules, the Punkt algorithm gets you pretty close to a very good result with very little work (assuming you have a ready-made implemntation available, like here from NLTK). Plus, Punkt might detect up some special cases in your corpus that the rules did not catch (like \"Johann S. Bach\" above). The main downside of Punkt is that it usually comes at the cost of **under-splitting**. That *might* be an issue in combination with subsequent deep parsing, because the runtime can behave exponential in the length of the sentence (number of tokens) being analyzed and it typically will introduce errors on those sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
