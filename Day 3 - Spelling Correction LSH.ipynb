{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling Correction using Locality Sensitive Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical course material for the ASDM Class 09 (Text Mining) by Florian Leitner.\n",
    "\n",
    "Â© 2016 Florian Leitner. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook combines yesterday's spelling correction example from Peter Norivg with a LSH to achieve higher correction speeds (although at slightly worse accuracy). In an acctual application with tens of thousands of words, you'd probably use the LSH approach with more lenient matching likelihoods that will match more spelling variants in one bucket, and then use some approach to find an exact match in that bucket (if any). In the same fashion as we match words with characters, the approach commonly is used to match similar documents with tokens. For example, LSH has been used to develop very efficient document plagiarism detection systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%.3f'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%precision 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the LSH implementation again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class MinHashSignature:\n",
    "    \"\"\"Hash signatures for sets/tuples using minhash.\"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        \"\"\"\n",
    "        Define the dimension of the hash pool\n",
    "        (number of hash functions).\n",
    "        \"\"\"\n",
    "        self.dim = dim\n",
    "        self.hashes = self.hash_functions()\n",
    "\n",
    "    def hash_functions(self):\n",
    "        \"\"\"Return dim different hash functions.\"\"\"\n",
    "        def hash_factory(n):\n",
    "            return lambda x: hash(\"salt\" + str(n) + str(x) + \"salt\")\n",
    "        \n",
    "        return [ hash_factory(_) for _ in range(self.dim) ]\n",
    "\n",
    "    def sign(self, item):\n",
    "        \"\"\"Return the minhash signatures for the `item`.\"\"\"\n",
    "        sig = [ float(\"inf\") ] * self.dim\n",
    "        \n",
    "        for hash_ix, hash_fn in enumerate(self.hashes):\n",
    "            # minhashing; requires item is iterable:\n",
    "            sig[hash_ix] = min(hash_fn(i) for i in item)\n",
    "        \n",
    "        return sig\n",
    "\n",
    "\n",
    "class LSH:\n",
    "    \"\"\"\n",
    "    Locality sensitive hashing.\n",
    "\n",
    "    Uses a banding approach to hash\n",
    "    similar signatures to the same buckets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, threshold):\n",
    "        \"\"\"\n",
    "        LSH approximating a given similarity `threshold`\n",
    "        with a given hash signature `size`.\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.threshold = threshold\n",
    "        self.bandwidth = self.get_bandwidth(size, threshold)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_bandwidth(n, t):\n",
    "        \"\"\"\n",
    "        Approximate the bandwidth (number of rows in each band)\n",
    "        needed to get threshold.\n",
    "\n",
    "        Threshold t = (1/b) ** (1/r)\n",
    "        where\n",
    "        b = # of bands\n",
    "        r = # of rows per band\n",
    "        n = b * r = size of signature\n",
    "        \"\"\"\n",
    "        best = n # 1\n",
    "        minerr = float(\"inf\")\n",
    "\n",
    "        for r in range(1, n + 1):\n",
    "            try:\n",
    "                b = 1. / (t ** r)\n",
    "            except: # Divide by zero, your signature is huge\n",
    "                return best\n",
    "\n",
    "            err = abs(n - b * r)\n",
    "\n",
    "            if err < minerr:\n",
    "                best = r\n",
    "                minerr = err\n",
    "\n",
    "        return best\n",
    "\n",
    "    def hash(self, sig):\n",
    "        \"\"\"Generate hash values for this signature.\"\"\"\n",
    "        for band in zip(*(iter(sig),) * self.bandwidth):\n",
    "            yield hash(\"salt\" + str(band) + \"tlas\")\n",
    "\n",
    "    @property\n",
    "    def exact_threshold(self):\n",
    "        \"\"\"The exact threshold defined by the chosen bandwith.\"\"\"\n",
    "        r = self.bandwidth\n",
    "        b = self.size / r\n",
    "        return (1. / b) ** (1. / r)\n",
    "\n",
    "    def get_n_bands(self):\n",
    "        \"\"\"The number of bands.\"\"\"\n",
    "        return int(self.size / self.bandwidth)\n",
    "\n",
    "\n",
    "class UnionFind:\n",
    "    \"\"\"\n",
    "    Union-find data structure.\n",
    "\n",
    "    Each unionFind instance X maintains a family of disjoint sets of\n",
    "    hashable objects, supporting the following two methods:\n",
    "\n",
    "    - X[item] returns a name for the set containing the given item.\n",
    "    Each set is named by an arbitrarily-chosen one of its members; as\n",
    "    long as the set remains unchanged it will keep the same name. If\n",
    "    the item is not yet part of a set in X, a new singleton set is\n",
    "    created for it.\n",
    "\n",
    "    - X.union(item1, item2, ...) merges the sets containing each item\n",
    "    into a single larger set. If any item is not yet part of a set\n",
    "    in X, it is added to X as one of the members of the merged set.\n",
    "\n",
    "    Source: http://www.ics.uci.edu/~eppstein/PADS/UnionFind.py\n",
    "\n",
    "    Union-find data structure. Based on Josiah Carlson's code,\n",
    "    http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/215912\n",
    "    with significant additional changes by D. Eppstein.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Create a new empty union-find structure.\"\"\"\n",
    "        self.weights = {}\n",
    "        self.parents = {}\n",
    "\n",
    "    def __getitem__(self, object):\n",
    "        \"\"\"Find and return the name of the set containing the object.\"\"\"\n",
    "        # check for previously unknown object\n",
    "        if object not in self.parents:\n",
    "            self.parents[object] = object\n",
    "            self.weights[object] = 1\n",
    "            return object\n",
    "\n",
    "        # find path of objects leading to the root\n",
    "        path = [object]\n",
    "        root = self.parents[object]\n",
    "\n",
    "        while root != path[-1]:\n",
    "            path.append(root)\n",
    "            root = self.parents[root]\n",
    "\n",
    "        # compress the path and return\n",
    "        for ancestor in path:\n",
    "            self.parents[ancestor] = root\n",
    "\n",
    "        return root\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Iterate through all items ever found or unioned by this structure.\"\"\"\n",
    "        return iter(self.parents)\n",
    "\n",
    "    def union(self, *objects):\n",
    "        \"\"\"Find the sets containing the objects and merge them all.\"\"\"\n",
    "        roots = [self[x] for x in objects]\n",
    "        heaviest = max([(self.weights[r],r) for r in roots])[1]\n",
    "        for r in roots:\n",
    "            if r != heaviest:\n",
    "                self.weights[heaviest] += self.weights[r]\n",
    "                self.parents[r] = heaviest\n",
    "\n",
    "    def sets(self):\n",
    "        \"\"\"Return a list of each disjoint set\"\"\"\n",
    "        ret = defaultdict(list)\n",
    "        for k, _ in self.parents.items():\n",
    "            ret[self[k]].append(k)\n",
    "        return list(ret.values())\n",
    "\n",
    "\n",
    "class Cluster:\n",
    "    \"\"\"\n",
    "    Cluster items with a Jaccard similarity above\n",
    "    some `threshold` with a high probability.\n",
    "\n",
    "    Based on Rajaraman, \"Mining of Massive Datasets\":\n",
    "\n",
    "    1. Generate items hash signatures\n",
    "    2. Use LSH to map similar signatures to same buckets\n",
    "    3. Use UnionFind to merge buckets containing same values\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, threshold=0.5, size=20):\n",
    "        \"\"\"\n",
    "        The `size` parameter controls the number of hash\n",
    "        functions (\"signature size\") to create.\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.unions = UnionFind()\n",
    "        self.signer = MinHashSignature(size)\n",
    "        self.hasher = LSH(size, threshold)\n",
    "        self.hashmaps = [\n",
    "            defaultdict(list) for _ in range(self.hasher.get_n_bands())\n",
    "        ]\n",
    "\n",
    "    def add(self, item, label=None):\n",
    "        \"\"\"\n",
    "        Add an `item` to the cluster.\n",
    "\n",
    "        Optionally, define a `label` to reference this `item`.\n",
    "        Otherwise, the `item` itself is used as label.\n",
    "        \"\"\"\n",
    "        # A label for this item\n",
    "        if label is None:\n",
    "            label = item\n",
    "\n",
    "        # Add to unionfind structure\n",
    "        self.unions[label]\n",
    "\n",
    "        # Get signature\n",
    "        sig = self.signer.sign(item)\n",
    "\n",
    "        # Union labels with same LSH key in same band\n",
    "        for band_idx, hashval in enumerate(self.hasher.hash(sig)):\n",
    "            self.hashmaps[band_idx][hashval].append(label)\n",
    "            self.unions.union(label, self.hashmaps[band_idx][hashval][0])\n",
    "\n",
    "    def groups(self):\n",
    "        \"\"\"\n",
    "        Get the clustering result.\n",
    "\n",
    "        Returns sets of labels.\n",
    "        \"\"\"\n",
    "        return self.unions.sets()\n",
    "\n",
    "    def match(self, item):\n",
    "        \"\"\"\n",
    "        Get the matching set of labels for `item`.\n",
    "\n",
    "        Returns a (possibly empty) set of items.\n",
    "        \"\"\"\n",
    "        # Get item signature\n",
    "        sig = self.signer.sign(item)\n",
    "\n",
    "        # Find matches\n",
    "        matches = set()\n",
    "\n",
    "        for band_idx, hashval in enumerate(self.hasher.hash(sig)):\n",
    "            if hashval in self.hashmaps[band_idx]:\n",
    "                matches.update(self.hashmaps[band_idx][hashval])\n",
    "\n",
    "        return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the evaluation function from Peter Norvig and as used yesterday, with some reporting tweeks for our particular problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Development Tests\n",
    "TESTS_1 = { 'access': 'acess', 'accessing': 'accesing', 'accommodation':\n",
    "'accomodation acommodation acomodation', 'account': 'acount', 'address':\n",
    "'adress adres', 'addressable': 'addresable', 'arranged': 'aranged arrainged',\n",
    "'arrangeing': 'aranging', 'arrangement': 'arragment', 'articles': 'articals',\n",
    "'aunt': 'annt anut arnt', 'auxiliary': 'auxillary', 'available': 'avaible',\n",
    "'awful': 'awfall afful', 'basically': 'basicaly', 'beginning': 'begining',\n",
    "'benefit': 'benifit', 'benefits': 'benifits', 'between': 'beetween', 'bicycle':\n",
    "'bicycal bycicle bycycle', 'biscuits': \n",
    "'biscits biscutes biscuts bisquits buiscits buiscuts', 'built': 'biult', \n",
    "'cake': 'cak', 'career': 'carrer',\n",
    "'cemetery': 'cemetary semetary', 'centrally': 'centraly', 'certain': 'cirtain',\n",
    "'challenges': 'chalenges chalenges', 'chapter': 'chaper chaphter chaptur',\n",
    "'choice': 'choise', 'choosing': 'chosing', 'clerical': 'clearical',\n",
    "'committee': 'comittee', 'compare': 'compair', 'completely': 'completly',\n",
    "'consider': 'concider', 'considerable': 'conciderable', 'contented':\n",
    "'contenpted contende contended contentid', 'curtains': \n",
    "'cartains certans courtens cuaritains curtans curtians curtions', 'decide': 'descide', 'decided':\n",
    "'descided', 'definitely': 'definately difinately', 'definition': 'defenition',\n",
    "'definitions': 'defenitions', 'description': 'discription', 'desiccate':\n",
    "'desicate dessicate dessiccate', 'diagrammatically': 'diagrammaticaally',\n",
    "'different': 'diffrent', 'driven': 'dirven', 'ecstasy': 'exstacy ecstacy',\n",
    "'embarrass': 'embaras embarass', 'establishing': 'astablishing establising',\n",
    "'experience': 'experance experiance', 'experiences': 'experances', 'extended':\n",
    "'extented', 'extremely': 'extreamly', 'fails': 'failes', 'families': 'familes',\n",
    "'february': 'febuary', 'further': 'futher', 'gallery': 'galery gallary gallerry gallrey', \n",
    "'hierarchal': 'hierachial', 'hierarchy': 'hierchy', 'inconvenient':\n",
    "'inconvienient inconvient inconvinient', 'independent': 'independant independant',\n",
    "'initial': 'intial', 'initials': 'inetials inistals initails initals intials',\n",
    "'juice': 'guic juce jucie juise juse', 'latest': 'lates latets latiest latist', \n",
    "'laugh': 'lagh lauf laught lugh', 'level': 'leval',\n",
    "'levels': 'levals', 'liaison': 'liaision liason', 'lieu': 'liew', 'literature':\n",
    "'litriture', 'loans': 'lones', 'locally': 'localy', 'magnificent': \n",
    "'magnificnet magificent magnifcent magnifecent magnifiscant magnifisent magnificant',\n",
    "'management': 'managment', 'meant': 'ment', 'minuscule': 'miniscule',\n",
    "'minutes': 'muinets', 'monitoring': 'monitering', 'necessary': \n",
    "'neccesary necesary neccesary necassary necassery neccasary', 'occurrence':\n",
    "'occurence occurence', 'often': 'ofen offen offten ofton', 'opposite': \n",
    "'opisite oppasite oppesite oppisit oppisite opposit oppossite oppossitte', 'parallel': \n",
    "'paralel paralell parrallel parralell parrallell', 'particular': 'particulaur',\n",
    "'perhaps': 'perhapse', 'personnel': 'personnell', 'planned': 'planed', 'poem':\n",
    "'poame', 'poems': 'poims pomes', 'poetry': 'poartry poertry poetre poety powetry', \n",
    "'position': 'possition', 'possible': 'possable', 'pretend': \n",
    "'pertend protend prtend pritend', 'problem': 'problam proble promblem proplen',\n",
    "'pronunciation': 'pronounciation', 'purple': 'perple perpul poarple',\n",
    "'questionnaire': 'questionaire', 'really': 'realy relley relly', 'receipt':\n",
    "'receit receite reciet recipt', 'receive': 'recieve', 'refreshment':\n",
    "'reafreshment refreshmant refresment refressmunt', 'remember': 'rember remeber rememmer rermember',\n",
    "'remind': 'remine remined', 'scarcely': 'scarcly scarecly scarely scarsely', \n",
    "'scissors': 'scisors sissors', 'separate': 'seperate',\n",
    "'singular': 'singulaur', 'someone': 'somone', 'sources': 'sorces', 'southern':\n",
    "'southen', 'special': 'speaical specail specal speical', 'splendid': \n",
    "'spledid splended splened splended', 'standardizing': 'stanerdizing', 'stomach': \n",
    "'stomac stomache stomec stumache', 'supersede': 'supercede superceed', 'there': 'ther',\n",
    "'totally': 'totaly', 'transferred': 'transfred', 'transportability':\n",
    "'transportibility', 'triangular': 'triangulaur', 'understand': 'undersand undistand', \n",
    "'unexpected': 'unexpcted unexpeted unexspected', 'unfortunately':\n",
    "'unfortunatly', 'unique': 'uneque', 'useful': 'usefull', 'valuable': 'valubale valuble', \n",
    "'variable': 'varable', 'variant': 'vairiant', 'various': 'vairious',\n",
    "'visited': 'fisited viseted vistid vistied', 'visitors': 'vistors',\n",
    "'voluntary': 'volantry', 'voting': 'voteing', 'wanted': 'wantid wonted',\n",
    "'whether': 'wether', 'wrote': 'rote wote'}\n",
    "\n",
    "# Final Tests\n",
    "TESTS_2 = {'forbidden': 'forbiden', 'decisions': 'deciscions descisions',\n",
    "'supposedly': 'supposidly', 'embellishing': 'embelishing', 'technique':\n",
    "'tecnique', 'permanently': 'perminantly', 'confirmation': 'confermation',\n",
    "'appointment': 'appoitment', 'progression': 'progresion', 'accompanying':\n",
    "'acompaning', 'applicable': 'aplicable', 'regained': 'regined', 'guidelines':\n",
    "'guidlines', 'surrounding': 'serounding', 'titles': 'tittles', 'unavailable':\n",
    "'unavailble', 'advantageous': 'advantageos', 'brief': 'brif', 'appeal':\n",
    "'apeal', 'consisting': 'consisiting', 'clerk': 'cleark clerck', 'component':\n",
    "'componant', 'favourable': 'faverable', 'separation': 'seperation', 'search':\n",
    "'serch', 'receive': 'recieve', 'employees': 'emploies', 'prior': 'piror',\n",
    "'resulting': 'reulting', 'suggestion': 'sugestion', 'opinion': 'oppinion',\n",
    "'cancellation': 'cancelation', 'criticism': 'citisum', 'useful': 'usful',\n",
    "'humour': 'humor', 'anomalies': 'anomolies', 'would': 'whould', 'doubt':\n",
    "'doupt', 'examination': 'eximination', 'therefore': 'therefoe', 'recommend':\n",
    "'recomend', 'separated': 'seperated', 'successful': 'sucssuful succesful',\n",
    "'apparent': 'apparant', 'occurred': 'occureed', 'particular': 'paerticulaur',\n",
    "'pivoting': 'pivting', 'announcing': 'anouncing', 'challenge': 'chalange',\n",
    "'arrangements': 'araingements', 'proportions': 'proprtions', 'organized':\n",
    "'oranised', 'accept': 'acept', 'dependence': 'dependance', 'unequalled':\n",
    "'unequaled', 'numbers': 'numbuers', 'sense': 'sence', 'conversely':\n",
    "'conversly', 'provide': 'provid', 'arrangement': 'arrangment',\n",
    "'responsibilities': 'responsiblities', 'fourth': 'forth', 'ordinary':\n",
    "'ordenary', 'description': 'desription descvription desacription',\n",
    "'inconceivable': 'inconcievable', 'data': 'dsata', 'register': 'rgister',\n",
    "'supervision': 'supervison', 'encompassing': 'encompasing', 'negligible':\n",
    "'negligable', 'allow': 'alow', 'operations': 'operatins', 'executed':\n",
    "'executted', 'interpretation': 'interpritation', 'hierarchy': 'heiarky',\n",
    "'indeed': 'indead', 'years': 'yesars', 'through': 'throut', 'committee':\n",
    "'committe', 'inquiries': 'equiries', 'before': 'befor', 'continued':\n",
    "'contuned', 'permanent': 'perminant', 'choose': 'chose', 'virtually':\n",
    "'vertually', 'correspondence': 'correspondance', 'eventually': 'eventully',\n",
    "'lonely': 'lonley', 'profession': 'preffeson', 'they': 'thay', 'now': 'noe',\n",
    "'desperately': 'despratly', 'university': 'unversity', 'adjournment':\n",
    "'adjurnment', 'possibilities': 'possablities', 'stopped': 'stoped', 'mean':\n",
    "'meen', 'weighted': 'wagted', 'adequately': 'adequattly', 'shown': 'hown',\n",
    "'matrix': 'matriiix', 'profit': 'proffit', 'encourage': 'encorage', 'collate':\n",
    "'colate', 'disaggregate': 'disaggreagte disaggreaget', 'receiving':\n",
    "'recieving reciving', 'proviso': 'provisoe', 'umbrella': 'umberalla', 'approached':\n",
    "'aproached', 'pleasant': 'plesent', 'difficulty': 'dificulty', 'appointments':\n",
    "'apointments', 'base': 'basse', 'conditioning': 'conditining', 'earliest':\n",
    "'earlyest', 'beginning': 'begining', 'universally': 'universaly',\n",
    "'unresolved': 'unresloved', 'length': 'lengh', 'exponentially':\n",
    "'exponentualy', 'utilized': 'utalised', 'set': 'et', 'surveys': 'servays',\n",
    "'families': 'familys', 'system': 'sysem', 'approximately': 'aproximatly',\n",
    "'their': 'ther', 'scheme': 'scheem', 'speaking': 'speeking', 'repetitive':\n",
    "'repetative', 'inefficient': 'ineffiect', 'geneva': 'geniva', 'exactly':\n",
    "'exsactly', 'immediate': 'imediate', 'appreciation': 'apreciation', 'luckily':\n",
    "'luckeley', 'eliminated': 'elimiated', 'believe': 'belive', 'appreciated':\n",
    "'apreciated', 'readjusted': 'reajusted', 'were': 'wer where', 'feeling':\n",
    "'fealing', 'and': 'anf', 'false': 'faulse', 'seen': 'seeen', 'interrogating':\n",
    "'interogationg', 'academically': 'academicly', 'relatively': 'relativly relitivly',\n",
    "'traditionally': 'traditionaly', 'studying': 'studing',\n",
    "'majority': 'majorty', 'build': 'biuld', 'aggravating': 'agravating',\n",
    "'transactions': 'trasactions', 'arguing': 'aurguing', 'sheets': 'sheertes',\n",
    "'successive': 'sucsesive sucessive', 'segment': 'segemnt', 'especially':\n",
    "'especaily', 'later': 'latter', 'senior': 'sienior', 'dragged': 'draged',\n",
    "'atmosphere': 'atmospher', 'drastically': 'drasticaly', 'particularly':\n",
    "'particulary', 'visitor': 'vistor', 'session': 'sesion', 'continually':\n",
    "'contually', 'availability': 'avaiblity', 'busy': 'buisy', 'parameters':\n",
    "'perametres', 'surroundings': 'suroundings seroundings', 'employed':\n",
    "'emploied', 'adequate': 'adiquate', 'handle': 'handel', 'means': 'meens',\n",
    "'familiar': 'familer', 'between': 'beeteen', 'overall': 'overal', 'timing':\n",
    "'timeing', 'committees': 'comittees commitees', 'queries': 'quies',\n",
    "'econometric': 'economtric', 'erroneous': 'errounous', 'decides': 'descides',\n",
    "'reference': 'refereence refference', 'intelligence': 'inteligence',\n",
    "'edition': 'ediion ediition', 'are': 'arte', 'apologies': 'appologies',\n",
    "'thermawear': 'thermawere thermawhere', 'techniques': 'tecniques',\n",
    "'voluntary': 'volantary', 'subsequent': 'subsequant subsiquent', 'currently':\n",
    "'curruntly', 'forecast': 'forcast', 'weapons': 'wepons', 'routine': 'rouint',\n",
    "'neither': 'niether', 'approach': 'aproach', 'available': 'availble',\n",
    "'recently': 'reciently', 'ability': 'ablity', 'nature': 'natior',\n",
    "'commercial': 'comersial', 'agencies': 'agences', 'however': 'howeverr',\n",
    "'suggested': 'sugested', 'career': 'carear', 'many': 'mony', 'annual':\n",
    "'anual', 'according': 'acording', 'receives': 'recives recieves',\n",
    "'interesting': 'intresting', 'expense': 'expence', 'relevant':\n",
    "'relavent relevaant', 'table': 'tasble', 'throughout': 'throuout', 'conference':\n",
    "'conferance', 'sensible': 'sensable', 'described': 'discribed describd',\n",
    "'union': 'unioun', 'interest': 'intrest', 'flexible': 'flexable', 'refered':\n",
    "'reffered', 'controlled': 'controled', 'sufficient': 'suficient',\n",
    "'dissension': 'desention', 'adaptable': 'adabtable', 'representative':\n",
    "'representitive', 'irrelevant': 'irrelavent', 'unnecessarily': 'unessasarily',\n",
    "'applied': 'upplied', 'apologised': 'appologised', 'these': 'thees thess',\n",
    "'choices': 'choises', 'will': 'wil', 'procedure': 'proceduer', 'shortened':\n",
    "'shortend', 'manually': 'manualy', 'disappointing': 'dissapoiting',\n",
    "'excessively': 'exessively', 'comments': 'coments', 'containing': 'containg',\n",
    "'develop': 'develope', 'credit': 'creadit', 'government': 'goverment',\n",
    "'acquaintances': 'aquantences', 'orientated': 'orentated', 'widely': 'widly',\n",
    "'advise': 'advice', 'difficult': 'dificult', 'investigated': 'investegated',\n",
    "'bonus': 'bonas', 'conceived': 'concieved', 'nationally': 'nationaly',\n",
    "'compared': 'comppared compased', 'moving': 'moveing', 'necessity':\n",
    "'nessesity', 'opportunity': 'oppertunity oppotunity opperttunity', 'thoughts':\n",
    "'thorts', 'equalled': 'equaled', 'variety': 'variatry', 'analysis':\n",
    "'analiss analsis analisis', 'patterns': 'pattarns', 'qualities': 'quaties', 'easily':\n",
    "'easyly', 'organization': 'oranisation oragnisation', 'the': 'thw hte thi',\n",
    "'corporate': 'corparate', 'composed': 'compossed', 'enormously': 'enomosly',\n",
    "'financially': 'financialy', 'functionally': 'functionaly', 'discipline':\n",
    "'disiplin', 'announcement': 'anouncement', 'progresses': 'progressess',\n",
    "'except': 'excxept', 'recommending': 'recomending', 'mathematically':\n",
    "'mathematicaly', 'source': 'sorce', 'combine': 'comibine', 'input': 'inut',\n",
    "'careers': 'currers carrers', 'resolved': 'resoved', 'demands': 'diemands',\n",
    "'unequivocally': 'unequivocaly', 'suffering': 'suufering', 'immediately':\n",
    "'imidatly imediatly', 'accepted': 'acepted', 'projects': 'projeccts',\n",
    "'necessary': 'necasery nessasary nessisary neccassary', 'journalism':\n",
    "'journaism', 'unnecessary': 'unessessay', 'night': 'nite', 'output':\n",
    "'oputput', 'security': 'seurity', 'essential': 'esential', 'beneficial':\n",
    "'benificial benficial', 'explaining': 'explaning', 'supplementary':\n",
    "'suplementary', 'questionnaire': 'questionare', 'employment': 'empolyment',\n",
    "'proceeding': 'proceding', 'decision': 'descisions descision', 'per': 'pere',\n",
    "'discretion': 'discresion', 'reaching': 'reching', 'analysed': 'analised',\n",
    "'expansion': 'expanion', 'although': 'athough', 'subtract': 'subtrcat',\n",
    "'analysing': 'aalysing', 'comparison': 'comparrison', 'months': 'monthes',\n",
    "'hierarchal': 'hierachial', 'misleading': 'missleading', 'commit': 'comit',\n",
    "'auguments': 'aurgument', 'within': 'withing', 'obtaining': 'optaning',\n",
    "'accounts': 'acounts', 'primarily': 'pimarily', 'operator': 'opertor',\n",
    "'accumulated': 'acumulated', 'extremely': 'extreemly', 'there': 'thear',\n",
    "'summarys': 'sumarys', 'analyse': 'analiss', 'understandable':\n",
    "'understadable', 'safeguard': 'safegaurd', 'consist': 'consisit',\n",
    "'declarations': 'declaratrions', 'minutes': 'muinutes muiuets', 'associated':\n",
    "'assosiated', 'accessibility': 'accessability', 'examine': 'examin',\n",
    "'surveying': 'servaying', 'politics': 'polatics', 'annoying': 'anoying',\n",
    "'again': 'agiin', 'assessing': 'accesing', 'ideally': 'idealy', 'scrutinized':\n",
    "'scrutiniesed', 'simular': 'similar', 'personnel': 'personel', 'whereas':\n",
    "'wheras', 'when': 'whn', 'geographically': 'goegraphicaly', 'gaining':\n",
    "'ganing', 'requested': 'rquested', 'separate': 'seporate', 'students':\n",
    "'studens', 'prepared': 'prepaired', 'generated': 'generataed', 'graphically':\n",
    "'graphicaly', 'suited': 'suted', 'variable': 'varible vaiable', 'building':\n",
    "'biulding', 'required': 'reequired', 'necessitates': 'nessisitates',\n",
    "'together': 'togehter', 'profits': 'proffits'}\n",
    "\n",
    "\n",
    "def spelltest(tests, verbose=False):\n",
    "    \"\"\"Use one of the two provided tests.\"\"\"\n",
    "    import time\n",
    "    n, bad, missed, junk, unknown, start = 0, 0, 0, 0, 0, time.clock()\n",
    "    # missed: how often the right word wasn't in the cluster\n",
    "    \n",
    "    for target, wrongs in tests.items():\n",
    "        for wrong in wrongs.split():\n",
    "            n += 1\n",
    "            w = correct(wrong) # our \"API\" definition\n",
    "            \n",
    "            if w != target:\n",
    "                bad += 1\n",
    "                \n",
    "                if target not in VOCAB:\n",
    "                    unknown += 1\n",
    "                elif target not in match(wrong):\n",
    "                    missed += 1\n",
    "                elif wrong in VOCAB:\n",
    "                    junk += 1\n",
    "                elif verbose:\n",
    "                    print('correct(%r) => %r (%d); expected %r (%d)' % (\n",
    "                        wrong, w, VOCAB[w], target, VOCAB[target]\n",
    "                    ))\n",
    "    \n",
    "    print('%d%% correct' % (100. - 100. * bad / n))\n",
    "    print('tested', n, 'words ->')\n",
    "    print('wrong on', bad, 'of those words:')\n",
    "    print(unknown, 'targets are not known to system (%d%%)' % (100.*unknown / n))\n",
    "    print(junk, 'wrong words in vocabulary (%d%%)' % (100.*junk / n))\n",
    "    print('possible leads:')\n",
    "    print(missed, 'words have target not in matched group (%d%%)' % (100.*missed / n))\n",
    "    print(bad - missed - unknown - junk, 'other types of mistakes (%d%%)' % (100.*(bad - missed - unknown - junk)/n))\n",
    "    print('performance:')\n",
    "    print(time.clock() - start, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the known vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jaccard(X, Y):\n",
    "    \"\"\"The Jaccard similarity between two sets.\"\"\"\n",
    "    x = set(X)\n",
    "    y = set(Y)\n",
    "    return float(len(x & y)) / len(x | y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1161192, 40234)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "from collections import Counter\n",
    "\n",
    "CORPUS = brown.words()\n",
    "VOCAB = Counter(w.lower() for w in CORPUS if w.isalpha())\n",
    "len (CORPUS), len(VOCAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the k-shingles for this vocabulary by setting the parameter K:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 2\n",
    "    \n",
    "def shingle(s, k):\n",
    "    \"\"\"Generate k-length shingles of string s.\"\"\"\n",
    "    k = min(len(s), k)\n",
    "    for i in range(len(s) - k + 1):\n",
    "        yield s[i:i+k]\n",
    "\n",
    "NGRAMS = [(w, frozenset(shingle(w, K))) for w in VOCAB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose to evaluate one of the next two cells:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Either** evaluate this for k-shingling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold = 0.834956560905386\n",
      "bandwidth = 11\n"
     ]
    }
   ],
   "source": [
    "DICTIONARY = Cluster(.84, 80)\n",
    "print(\"threshold =\", DICTIONARY.hasher.exact_threshold)\n",
    "print(\"bandwidth =\", DICTIONARY.hasher.bandwidth)\n",
    "\n",
    "for w, n in NGRAMS:\n",
    "    DICTIONARY.add(n, w)\n",
    "\n",
    "def match(word):\n",
    "    return DICTIONARY.match(set(shingle(word, K)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Or** evaluate this cell to use unigram-based matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold = 0.834956560905386\n",
      "bandwidth = 11\n"
     ]
    }
   ],
   "source": [
    "DICTIONARY = Cluster(.84, 80)\n",
    "print(\"threshold =\", DICTIONARY.hasher.exact_threshold)\n",
    "print(\"bandwidth =\", DICTIONARY.hasher.bandwidth)\n",
    "\n",
    "for w in VOCAB:\n",
    "    DICTIONARY.add(w, w)\n",
    "\n",
    "def match(word):\n",
    "    return DICTIONARY.match(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1010.000, 39.836)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_groups = len(DICTIONARY.groups()) * 1.\n",
    "num_groups, sum(len(g) for g in DICTIONARY.groups()) / num_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding the best LSH cluster, we will calculate the edit distance of each candidate to the word being corrected, because LSH provided a significantly smaller search space. Note that on average we need to do the above number of comparisons:\n",
    "\n",
    "```python\n",
    "sum(len(g) for g in DICTIONARY.groups()) / num_groups\n",
    "```\n",
    "\n",
    "In this scenario, to still have reasonable speed, it seems finding an average group size of about 100 candidates is ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def edit_distance(s1, s2):\n",
    "    len1 = len(s1)\n",
    "    len2 = len(s2)\n",
    "    lev = _edit_dist_init(len1 + 1, len2 + 1)\n",
    "\n",
    "    # iterate over the array\n",
    "    for i in range(len1):\n",
    "        for j in range(len2):\n",
    "            _edit_dist_step(lev, i + 1, j + 1, s1, s2)\n",
    "    return lev[len1][len2]\n",
    "\n",
    "def _edit_dist_init(len1, len2):\n",
    "    lev = []\n",
    "    for i in range(len1):\n",
    "        lev.append([0] * len2)  # initialize 2-D array to zero\n",
    "    for i in range(len1):\n",
    "        lev[i][0] = i           # column 0: 0,1,2,3,4,...\n",
    "    for j in range(len2):\n",
    "        lev[0][j] = j           # row 0: 0,1,2,3,4,...\n",
    "    return lev\n",
    "\n",
    "def _edit_dist_step(lev, i, j, s1, s2):\n",
    "    c1 = s1[i - 1]\n",
    "    c2 = s2[j - 1]\n",
    "\n",
    "    # skipping a character in s1\n",
    "    a = lev[i - 1][j] + 1\n",
    "    # skipping a character in s2\n",
    "    b = lev[i][j - 1] + 1\n",
    "    # substitution\n",
    "    c = lev[i - 1][j - 1] + (c1 != c2)\n",
    "\n",
    "    # transposition\n",
    "    d = c + 1  # never picked by default\n",
    "    if i > 1 and j > 1:\n",
    "        if s1[i - 2] == c2 and s2[j - 2] == c1:\n",
    "            d = lev[i - 2][j - 2] + 1\n",
    "\n",
    "    # pick the cheapest\n",
    "    lev[i][j] = min(a, b, c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def correct(word):\n",
    "    \"Find the best spelling correction for this word.\"\n",
    "    # do not correct known words:\n",
    "    if word in VOCAB:\n",
    "        return word\n",
    "    \n",
    "    candidates = match(word)\n",
    "    \n",
    "    if candidates:\n",
    "        # measure distance to word and sort by increasing distance\n",
    "        d_c_pairs = sorted((edit_distance(word, c), c) for c in candidates)\n",
    "        # select the candidates with the shortest distance min_d\n",
    "        min_d = d_c_pairs[0][0]\n",
    "        candidates = [d_c[1] for d_c in filter(lambda i: i[0] == min_d, d_c_pairs)]\n",
    "        # return the most frequent of the selected candidates\n",
    "        return max(candidates, key=VOCAB.get)\n",
    "    else:\n",
    "        # no candidates fallback: do not correct\n",
    "        return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the new evaluation method, we immediately can se if we need to change the cluster similarty parameter of (not in matched group) or the distance settings (shingle-size, edit distance calcluation, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64% correct\n",
      "tested 270 words ->\n",
      "wrong on 95 of those words:\n",
      "15 targets are not known to system (5%)\n",
      "5 wrong words in vocabulary (1%)\n",
      "possible leads:\n",
      "59 words have target not in matched group (21%)\n",
      "16 other types of mistakes (5%)\n",
      "performance:\n",
      "1.9214910000000032 seconds\n"
     ]
    }
   ],
   "source": [
    "spelltest(TESTS_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67% correct\n",
      "tested 400 words ->\n",
      "wrong on 132 of those words:\n",
      "22 targets are not known to system (5%)\n",
      "10 wrong words in vocabulary (2%)\n",
      "possible leads:\n",
      "74 words have target not in matched group (18%)\n",
      "26 other types of mistakes (6%)\n",
      "performance:\n",
      "3.8779480000000035 seconds\n"
     ]
    }
   ],
   "source": [
    "spelltest(TESTS_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that with using simple unigrams, we get roughly a similar accuracy as Peter Norvig's method, but have more than tripled the correction speed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
