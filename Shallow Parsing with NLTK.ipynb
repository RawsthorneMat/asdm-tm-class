{
 "metadata": {
  "name": "",
  "signature": "sha256:a10e558acc78700caf2435016191745ee1f2751dfced5fbc6ca76f6d865b9438"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Shallow Parsing with NLTK"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "On PoS Tagging, Named Entity Recognition, and Chunking"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To be able to move into relationship extraction (or any other \"advanced text mining\" task), it is crucial to do reliable PoS tagging, NER and phrase chunking - in summary often referred to as \"shallow parsing\". That implies that we are leaving aside natural language [dependency] parsing in favour of \"high-throughput\" text mining and hope to figure out the \"dependencies\" we would get from the former using patterns and/or machine learning techniques instead. This could be done, for example, by using the resulting chunks, entities, tokens & tags, n-grams, and any other content as features for a classifier that then should detect the presence of a (binary, semantic) relationship. We can even add in a few manual feature rules to improve performance, which is easy, for example, with MaxEnt, as we have seen yesterday.\n",
      "\n",
      "Most of what we are doing here - training chunkers, what Trees are, NER with NLTK - is well documented in the NLTK Book, [Chapter 7](http://www.nltk.org/book/ch07.html), which you should have a look at if you want to get serious with NLTK."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Quick-and-dirty Tagging, Chunking, and NER with NLTK"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import ne_chunk, pos_tag, word_tokenize as tokenize\n",
      "from nltk.stem.snowball import EnglishStemmer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def analyze(sentence):\n",
      "    return ne_chunk(pos_tag(tokenize(sentence)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print analyze(\n",
      "    u'The economist Jeffrey Sachs coined the term \"shock therapy\".'\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  The/DT\n",
        "  economist/NN\n",
        "  (PERSON Jeffrey/NNP Sachs/NNP)\n",
        "  coined/VBD\n",
        "  the/DT\n",
        "  term/NN\n",
        "  ``/``\n",
        "  shock/NN\n",
        "  therapy/NN\n",
        "  ''/''\n",
        "  ./.)\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print analyze(u'The Google Nexus challenges the iPhone\u2018s dominance.').pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  The/DT\n",
        "  (ORGANIZATION Google/NNP Nexus/NNP)\n",
        "  challenges/NNS\n",
        "  the/DT\n",
        "  iPhone\u2018s/NNS\n",
        "  dominance/NN\n",
        "  ./.)\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print analyze(u'The quick brown fox jumped over the lazy dog.').pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  The/DT\n",
        "  quick/NN\n",
        "  brown/NN\n",
        "  fox/NN\n",
        "  jumped/VBD\n",
        "  over/IN\n",
        "  the/DT\n",
        "  lazy/NN\n",
        "  dog/NN\n",
        "  ./.)\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print analyze(u'The brown fox quickly jumped over the lazy dog.').pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  The/DT\n",
        "  brown/NN\n",
        "  fox/NN\n",
        "  quickly/RB\n",
        "  jumped/VBD\n",
        "  over/IN\n",
        "  the/DT\n",
        "  lazy/NN\n",
        "  dog/NN\n",
        "  ./.)\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print analyze(u'A summer school in Madrid is sunnier than one in the UK.').pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  A/DT\n",
        "  summer/NN\n",
        "  school/NN\n",
        "  in/IN\n",
        "  (GPE Madrid/NNP)\n",
        "  is/VBZ\n",
        "  sunnier/JJR\n",
        "  than/IN\n",
        "  one/CD\n",
        "  in/IN\n",
        "  the/DT\n",
        "  (ORGANIZATION UK/NNP)\n",
        "  ./.)\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`GPE` = geo-political entity\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "NLTK's \"default\" NER (`ne_chunk( pos_tag( word_tokenize( sentence )))`) provides good-enough out-of the box NLP to experiment with, but you'll have to do more if you need to get those \"high-quality\" results. For that, NLTK provides wrappers to the most powerfull CRF taggers around, like [MALLET](http://mallet.cs.umass.edu). Furthermore, due the nature of Python, it is very easy to write wrappers for C(++)-based NLP tools; e.g., [CRFsuite](http://www.chokkan.org/software/crfsuite/), a high-speed CRF in \"pure\" C, comes with a Python wrapper. As long as they have a I/O (stdin/stdout) interface to send and receive text, for any others as well, for examle the all-in-one [GENIA Tagger](http://www.nactem.ac.uk/tsujii/GENIA/tagger/) for the biomedical domain.\n",
      "\n",
      "For the remainder of this tutorial, we will [download](http://nlp.stanford.edu/software/CRF-NER.shtml) and install the **Stanford NER Tagger**, and their [PoS Tagger](http://nlp.stanford.edu/software/tagger.shtml) with an already trained model and see how the above examples can be improved.\n",
      "\n",
      "To train a NER model for newswire, you could use [CoNLL2003](http://www.cnts.ua.ac.be/conll2003/ner/) - anoyingly enough, however, that is not part of NLTK, and requires making a written request for the (Reuters) text corpus. Gabrilovich maintains [a list](http://www.cs.technion.ac.il/~gabr/resources/data/ne_datasets.html) of \"standard\" NER-tagger corpora (PER/ORG/GPE...). If you were interested in detecting gene and protein names, you might have a look at our [BioCreative](http://www.biocreative.org/) Gene-Entity corpora (note that you have to register on the website before you can download any corpus on the site, thereby confirming that you will not use the data for commerical purposes)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Stanford NER Setup"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tag.stanford import NERTagger\n",
      "from nltk.tag.stanford import POSTagger\n",
      "import nltk.data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that for the below commands, we assume the Stanford NER and PoS Taggers are in subdirectories located in the same directory as this notebook:\n",
      "\n",
      "- The NER Tagger files should be in a subdirectory with the path `stanford/ner/`.\n",
      "- The PoS Tagger files should be in a subdriectory with the path `stanford/pos/`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ner = NERTagger('stanford/ner/english.all.3class.distsim.crf.ser.gz',\n",
      "                'stanford/ner/stanford-ner-3.4.jar', 'UTF-8') # encoding!\n",
      "pos = POSTagger('stanford/pos/english-bidirectional-distsim.tagger',\n",
      "                'stanford/pos/stanford-postagger-3.4.jar', 'UTF-8') # encoding!\n",
      "st = EnglishStemmer()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def analyze(sentence):\n",
      "    tokens = tokenize(sentence)\n",
      "    stems = [st.stem(t) for t in tokens] #map(st.stem, tokens)\n",
      "    _, tags = zip(*pos.tag(tokens))\n",
      "    _, entities = zip(*ner.tag(tokens))\n",
      "    return zip(tokens, stems, tags, entities)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "analyze(u'The quick brown fox jumped over the lazy dog.')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "[(u'The', u'the', u'DT', u'O'),\n",
        " (u'quick', u'quick', u'JJ', u'O'),\n",
        " (u'brown', u'brown', u'JJ', u'O'),\n",
        " (u'fox', u'fox', u'NN', u'O'),\n",
        " (u'jumped', u'jump', u'VBD', u'O'),\n",
        " (u'over', u'over', u'IN', u'O'),\n",
        " (u'the', u'the', u'DT', u'O'),\n",
        " (u'lazy', u'lazi', u'JJ', u'O'),\n",
        " (u'dog', u'dog', u'NN', u'O'),\n",
        " (u'.', u'.', u'.', u'O')]"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "analyze(u'A summer school in Madrid is sunnier than one in the UK.')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "[(u'A', u'a', u'DT', u'O'),\n",
        " (u'summer', u'summer', u'NN', u'O'),\n",
        " (u'school', u'school', u'NN', u'O'),\n",
        " (u'in', u'in', u'IN', u'O'),\n",
        " (u'Madrid', u'madrid', u'NNP', u'LOCATION'),\n",
        " (u'is', u'is', u'VBZ', u'O'),\n",
        " (u'sunnier', u'sunnier', u'JJR', u'O'),\n",
        " (u'than', u'than', u'IN', u'O'),\n",
        " (u'one', u'one', u'CD', u'O'),\n",
        " (u'in', u'in', u'IN', u'O'),\n",
        " (u'the', u'the', u'DT', u'O'),\n",
        " (u'UK', u'uk', u'NNP', u'LOCATION'),\n",
        " (u'.', u'.', u'.', u'O')]"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Much better, indeed!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "analyze(u'The Google Nexus challanges the iPhone\u2018s dominance.')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "[(u'The', u'the', u'DT', u'O'),\n",
        " (u'Google', u'googl', u'NNP', u'ORGANIZATION'),\n",
        " (u'Nexus', u'nexus', u'NNP', u'O'),\n",
        " (u'challanges', u'challang', u'VBZ', u'O'),\n",
        " (u'the', u'the', u'DT', u'O'),\n",
        " (u'iPhone\\u2018s', u'iphon', u'NN', u'O'),\n",
        " (u'dominance', u'domin', u'NN', u'O'),\n",
        " (u'.', u'.', u'.', u'O')]"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Good: notice that the Stanford NER now correctly *misses* the \"Nexus\" and \"iPhone\" entities."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "MegaM Chunking and PoS Tagging"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After this, we only need one final step: chunking. As this is \"trivial\" once we have good PoS tags, we will train our own NLTK chunker. But, being lazy as always, there is a neat collection of scripts to train NLTK text, PoS and chunk classifiers: [NLTK-Trainer](http://nltk-trainer.readthedocs.org/en/latest/index.html), however, it requires you to use Python 2.x. You could either [download](https://github.com/japerk/nltk-trainer/archive/master.zip) it or fetch it using Git: `git clone https://github.com/japerk/nltk-trainer.git`. We will train a MEMM chunker using MegaM, or, if you don't have MegaM, will use NaiveBayes.\n",
      "\n",
      "This also requires you run `python setup.py install` in the `python-trainer` directory."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run nltk-trainer/train_chunker.py conll2000 --classifier MEGAM"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading conll2000\n",
        "10948 chunks, training on 10948"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training ClassifierChunker with ['MEGAM'] classifier\n",
        "[Found megam: /usr/local/bin/megam.opt]\n",
        "Constructing training corpus for classifier."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training classifier (259104 instances)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training MEGAM classifier\n",
        "evaluating ClassifierChunker"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ChunkParse score:\n",
        "    IOB Accuracy:  95.0%\n",
        "    Precision:     92.5%\n",
        "    Recall:        93.0%\n",
        "    F-Measure:     92.8%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "dumping ClassifierChunker to /Users/fleitner/nltk_data/chunkers/conll2000_MEGAM.pickle\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This should produce an F-Measure of about 93%.\n",
      "\n",
      "If you do not have MegaM installed, you could at least try the Na\u00efve Bayes approach."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run nltk-trainer/train_chunker.py conll2000 --classifier NaiveBayes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading conll2000\n",
        "10948 chunks, training on 10948"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training ClassifierChunker with ['NaiveBayes'] classifier\n",
        "Constructing training corpus for classifier."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training classifier (259104 instances)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training NaiveBayes classifier\n",
        "evaluating ClassifierChunker"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ChunkParse score:\n",
        "    IOB Accuracy:  93.5%\n",
        "    Precision:     88.8%\n",
        "    Recall:        91.1%\n",
        "    F-Measure:     89.9%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "dumping ClassifierChunker to /Users/fleitner/nltk_data/chunkers/conll2000_NaiveBayes.pickle\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This should produce an F-Measure of about 90%."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%run nltk-trainer/train_chunker.py conll2000 --classifier sklearn.*"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will also take our chances and see how a MegaM-based MEMM PoS tagger performs and if it would be possible to therefore skip the Stanford PoS tagger (alternative: Naive Bayes PoS tagging). **Warning**: This step takes a long time (minutes, at leat)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run nltk-trainer/train_tagger.py conll2000 --classifier MEGAM"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run nltk-trainer/train_tagger.py conll2000 --classifier NaiveBayes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you completed the above training steps, you can now import the models:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chunker = nltk.data.load('chunkers/conll2000_MEGAM.pickle')\n",
      "pos = nltk.data.load('taggers/conll2000_MEGAM_aubt.pickle')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chunker = nltk.data.load('chunkers/conll2000_NaiveBayes.pickle')\n",
      "pos = nltk.data.load('taggers/conll2000_NaiveBayes_aubt.pickle')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's see how good this works, using whatever classifier pair you trained. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print chunker.parse(pos.tag(tokenize('The economist Jeffrey Sachs coined the term \"shock therapy\".'))).pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP The/DT economist/NN Jeffrey/NNP Sachs/NNP)\n",
        "  (VP coined/VBD)\n",
        "  (NP the/DT term/NN)\n",
        "  ``/``\n",
        "  (NP shock/NN therapy/NN)\n",
        "  ''/''\n",
        "  ./.)\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chunker.parse(pos.tag(tokenize('The Google Nexus challanges the iPhone\u2018s dominance.')))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "Tree('S', [Tree('NP', [('The', 'DT'), ('Google', 'JJ'), ('Nexus', 'NNPS')]), Tree('VP', [('challanges', 'VBZ')]), Tree('NP', [('the', 'DT'), ('iPhone\\xe2\\x80\\x98s', 'NNPS'), ('dominance', 'NN')]), ('.', '.')])"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print chunker.parse(pos.tag(tokenize('The Google Nexus challanges the iPhone\u2018s dominance.'))).pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP The/DT Google/JJ Nexus/NNPS)\n",
        "  (VP challanges/VBZ)\n",
        "  (NP the/DT iPhone\u2018s/NNPS dominance/NN)\n",
        "  ./.)\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's create a new function and combine this with the entity tags:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def analyze(sentence):\n",
      "    \n",
      "    def combine(tree, *iters):\n",
      "        for idx in range(len(tree)):\n",
      "            n = tree[idx]\n",
      "\n",
      "            if isinstance(n, nltk.tree.Tree):\n",
      "                combine(n, *iters)\n",
      "            elif isinstance(n, tuple):\n",
      "                tmp = [n[0], n[1]] + [next(i) for i in iters]\n",
      "                tree[idx] = tuple(tmp)\n",
      "            else:\n",
      "                raise RuntimeError('unexpected ' + repr(n))\n",
      "            \n",
      "    tokens = tokenize(sentence)\n",
      "    _, entities = zip(*ner.tag(tokens))\n",
      "    stems = map(st.stem, tokens)\n",
      "    tree = chunker.parse(pos.tag(tokens))\n",
      "    combine(tree, iter(stems), iter(entities))\n",
      "    return tree "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print analyze(u'The Google Nexus challanges the iPhone\u2018s dominance.').pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP The/DT/the/O Google/JJ/googl/ORGANIZATION Nexus/NNPS/nexus/O)\n",
        "  (VP challanges/VBZ/challang/O)\n",
        "  (NP the/DT/the/O iPhone\u2018s/NNPS/iphon/O dominance/NN/domin/O)\n",
        "  ./././O)\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print analyze(u'The brown fox quickly jumps over the lazy dog.').pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP The/DT/the/O brown/JJ/brown/O fox/NN/fox/O)\n",
        "  quickly/RB/quick/O\n",
        "  (VP jumps/VBZ/jump/O)\n",
        "  (PP over/IN/over/O)\n",
        "  (NP the/DT/the/O lazy/JJ/lazi/O dog/NN/dog/O)\n",
        "  ./././O)\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print analyze(u'The quick brown fox jumped over the lazy dog.').pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP The/DT/the/O quick/JJ/quick/O brown/NN/brown/O)\n",
        "  (NP fox/WP$/fox/O)\n",
        "  (VP jumped/VBD/jump/O)\n",
        "  (PP over/IN/over/O)\n",
        "  (NP the/DT/the/O lazy/JJ/lazi/O dog/NN/dog/O)\n",
        "  ./././O)\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pos = POSTagger('stanford/pos/english-bidirectional-distsim.tagger',\n",
      "                'stanford/pos/stanford-postagger-3.4.jar', 'UTF-8') # encoding!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print analyze(u'The quick brown fox jumped over the lazy dog.').pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP The/DT/the/O quick/JJ/quick/O brown/NN/brown/O)\n",
        "  (NP fox/WP$/fox/O)\n",
        "  (VP jumped/VBD/jump/O)\n",
        "  (PP over/IN/over/O)\n",
        "  (NP the/DT/the/O lazy/JJ/lazi/O dog/NN/dog/O)\n",
        "  ./././O)\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we wrap all up and create a class to do language processing in the future (on the way fixing a ton of bugs to actually run it on any English text...):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "from nltk import data, word_tokenize as tokenize\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "from nltk.tag.stanford import NERTagger\n",
      "from nltk.tree import Tree\n",
      "\n",
      "\n",
      "class ShallowParser(object):\n",
      "    \n",
      "    def __init__(self,\n",
      "                 chunker_data='chunkers/conll2000_MEGAM.pickle',\n",
      "                 sf_ner_model_path='stanford/ner/english.all.3class.distsim.crf.ser.gz',\n",
      "                 sf_ner_jar_path='stanford/ner/stanford-ner-3.4.jar',\n",
      "                 sf_pos_model_path='stanford/pos/english-bidirectional-distsim.tagger',\n",
      "                 sf_pos_jar_path='stanford/pos/stanford-postagger-3.4.jar',\n",
      "                 encoding='UTF-8'):\n",
      "        self.chunker = data.load(chunker_data)\n",
      "        self.ner = NERTagger(sf_ner_model_path,\n",
      "                             sf_ner_jar_path,\n",
      "                             encoding)\n",
      "        self.pos = POSTagger(sf_pos_model_path,\n",
      "                             sf_pos_jar_path,\n",
      "                             encoding)\n",
      "        self.stem = PorterStemmer().stem\n",
      "\n",
      "    def parse(self, tokens):\n",
      "        return self.batch_parse([tokens])[0]\n",
      "\n",
      "    def batch_parse(self, sentences):\n",
      "        entities = self.ner.batch_tag(sentences)\n",
      "        entities = NEChunker.fix_stanford(entities, sentences)\n",
      "        entities = NEChunker.extract_tags(entities)\n",
      "        stems = [[self.stem(t) for t in s] for s in sentences]\n",
      "        pos_tags = self.pos.batch_tag(sentences)\n",
      "        # apparently, the PoSTagger does not interfere\n",
      "        # with its input\n",
      "        # NEChunker.fix_stanford(pos_tags, sentences)\n",
      "        trees = self.chunker.batch_parse(pos_tags)\n",
      "        \n",
      "        for t, s, e in zip(trees, stems, entities):\n",
      "            try:\n",
      "                NEChunker.combine(t, iter(s), iter(e))\n",
      "            except RuntimeError:\n",
      "                print \"Tree:\", t.pprint()\n",
      "                print \"Stems:\", len(s), s\n",
      "                print \"Entities:\", len(e), e\n",
      "                raise\n",
      "        \n",
      "        return trees\n",
      "\n",
      "    ########################\n",
      "    # fix_stanford methods #\n",
      "    ########################\n",
      "    \n",
      "    @staticmethod\n",
      "    def fix_stanford(tags, sentences):\n",
      "        \"\"\"\n",
      "        The Standford NER Tagger splits tokens and sentences\n",
      "        without us \"asking\" for it.\n",
      "        \n",
      "        This fixes the (wrong) splits introduced and removes\n",
      "        tokens the tagger sometimes introduces \"de novo\".\n",
      "        \"\"\"\n",
      "        fixed_tags = []\n",
      "        s = 0\n",
      "        tag_iter = NEChunker._iter_stanford(tags)\n",
      "        \n",
      "        for tok in NEChunker._iter_sentences(sentences):\n",
      "            if tok is None:\n",
      "                assert len(fixed_tags) == len(sentences[s])\n",
      "                yield fixed_tags\n",
      "                fixed_tags = []\n",
      "                s += 1\n",
      "            else:\n",
      "                NEChunker._fix_tag(tok, sentences[s],\n",
      "                                   tag_iter, fixed_tags)\n",
      "\n",
      "    @staticmethod\n",
      "    def _iter_sentences(sentences):\n",
      "        # helper method for fix_stanford\n",
      "        for s in sentences:\n",
      "            for t in s:\n",
      "                yield t\n",
      "                \n",
      "            yield None\n",
      "    \n",
      "    @staticmethod\n",
      "    def _iter_stanford(tags):\n",
      "        # helper method for fix_stanford\n",
      "        for s in tags:\n",
      "            for t in s:\n",
      "                if re.search(r\"\\-..B\\-\", t[0]):\n",
      "                    # reconstruct \"escaped\"\n",
      "                    # brackets in tokens\n",
      "                    tmp = t[0]\n",
      "                    tmp = tmp.replace(\"-LRB-\", \"(\")\n",
      "                    tmp = tmp.replace(\"-RRB-\", \")\")\n",
      "                    tmp = tmp.replace(\"-LSB-\", \"[\")\n",
      "                    tmp = tmp.replace(\"-RSB-\", \"]\")\n",
      "                    tmp = tmp.replace(\"-LCB-\", \"{\")\n",
      "                    tmp = tmp.replace(\"-RCB-\", \"}\")\n",
      "                    t = (tmp, t[1])\n",
      "                yield t\n",
      "    \n",
      "    @staticmethod\n",
      "    def _fix_tag(tok, sentence, tag_iter, fixed_tags):\n",
      "        \"\"\"\n",
      "        Fixes the unwanted changes introduced by the\n",
      "        Stanford NER Tagger.\n",
      "        \"\"\"\n",
      "        # helper method for fix_stanford\n",
      "        tag = NEChunker._scan_tag(next(tag_iter), tag_iter, tok)\n",
      "\n",
      "        if tag[0] == tok:\n",
      "            # a match; we're done!\n",
      "            fixed_tags.append(tag)\n",
      "        elif tag[0] == u'.':\n",
      "            # non-existant sentence terminal introduced\n",
      "            peek = next(tag_iter)\n",
      "\n",
      "            if peek[0] == tok:\n",
      "                fixed_tags.append(peek)\n",
      "            else:\n",
      "               NEChunker._failed_alignment(tag, fixed_tags, tok, sentence)\n",
      "        elif tok.replace('/', '') == tag[0]:\n",
      "            # slashes in the token were removed\n",
      "            fixed_tags.append(tag)\n",
      "        elif tok.replace('/', '').startswith(tag[0]):\n",
      "            # merge splitted tokens with slashes back together\n",
      "            tmp = tok.replace('/', '')\n",
      "            tag = NEChunker._scan_tag(tag, tag_iter, tmp)\n",
      "            \n",
      "            if tag[0] == tmp:\n",
      "                fixed_tags.append(tag)\n",
      "            else:\n",
      "                 NEChunker._failed_alignment(tag, fixed_tags, tok, sentence)\n",
      "        else:\n",
      "            NEChunker._failed_alignment(tag, fixed_tags, tok, sentence)\n",
      "                \n",
      "    @staticmethod\n",
      "    def _scan_tag(tag, tag_iter, tok):\n",
      "        # recursive helper method for fix_stanford\n",
      "        if len(tok)           >= len(tag[0]) and \\\n",
      "           len(tag[0])        >  0           and \\\n",
      "           tag[0][-1]         == u\"`\"        and \\\n",
      "           tok[len(tag[0])-1] == \"'\"         :\n",
      "            # SNER splits tokens like \"ab'cd\" and replaces\n",
      "            # the quote with a backtick...\n",
      "            tag = (tag[0].replace(u\"`\", u\"'\"), tag[1])\n",
      "        \n",
      "        if len(tok) - 1 == len(tag[0]) and \\\n",
      "           tok[-1]      == \"/\"         and \\\n",
      "           tok[:-1]     == tag[0]:\n",
      "            return tag\n",
      "        elif tag[0] != tok and tok.startswith(tag[0]):\n",
      "            # merge splitted tokens back together\n",
      "            merge = next(tag_iter)\n",
      "            return NEChunker._append_tag((tag[0] + merge[0], tag[1]), tag_iter, tok)\n",
      "        else:  \n",
      "            return tag\n",
      "        \n",
      "    @staticmethod\n",
      "    def _failed_alignment(tag, tags, tok, tokens):\n",
      "        # helper method for fix_stanford\n",
      "        raise RuntimeError(\n",
      "            u'failed aligning tag \"%s\":\\n%s\\nto sentence token \"%s\":\\n%s' % (\n",
      "                tag,\n",
      "                u\" \".join(t[0] for t in tags),\n",
      "                tok,\n",
      "                u\" \".join(tokens)\n",
      "            )\n",
      "        )\n",
      "\n",
      "    ##########################\n",
      "    # tag generation methods #\n",
      "    ##########################\n",
      "    \n",
      "    @staticmethod\n",
      "    def extract_tags(sentences, idx=-1):\n",
      "        \"\"\"\n",
      "        Drop all but one tag per token in sentences.\n",
      "        \n",
      "        I.e., drops all but one element in a list of\n",
      "        lists of lists.\n",
      "        \"\"\"\n",
      "        return [[t[idx] for t in s] for s in sentences]\n",
      "    \n",
      "    @staticmethod\n",
      "    def combine(tree, *iters):\n",
      "        \"\"\"\n",
      "        Merge one ore more tags into the tree.\n",
      "        \n",
      "        The tags have to be provided as iterators,\n",
      "        not lists.\n",
      "        \"\"\"\n",
      "        for idx in range(len(tree)):\n",
      "            n = tree[idx]\n",
      "\n",
      "            if isinstance(n, Tree):\n",
      "                NEChunker.combine(n, *iters)\n",
      "            elif isinstance(n, tuple):\n",
      "                tree[idx] = NEChunker._merge_tags(n, iters)\n",
      "            else:\n",
      "                raise RuntimeError('unexpected ' + repr(n) +\n",
      "                                   ' in ' + repr(tree))\n",
      "    \n",
      "    @staticmethod\n",
      "    def _merge_tags(node, tag_iters):\n",
      "        # helper method for combine\n",
      "        tmp = [node[0], node[1]]\n",
      "\n",
      "        try:\n",
      "            for i in tag_iters:\n",
      "                tmp.append(next(i))                    \n",
      "        except StopIteration:\n",
      "            raise RuntimeError('items to short in ' +\n",
      "                               repr(node))\n",
      "\n",
      "        return tuple(tmp)\n",
      "\n",
      "\n",
      "parser = ShallowParser()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 133
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that we no longer tokenize inside the function to work better with the NLTK API."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "print parser.parse(tokenize(u'The quick brown fox jumped over the lazy dog.')).pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP The/DT/The/O quick/JJ/quick/O brown/JJ/brown/O fox/NN/fox/O)\n",
        "  (VP jumped/VBD/jump/O)\n",
        "  (PP over/IN/over/O)\n",
        "  (NP the/DT/the/O lazy/JJ/lazi/O dog/NN/dog/O)\n",
        "  ./././O)\n"
       ]
      }
     ],
     "prompt_number": 134
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "example = u'However, [the jury] said it believes \"these two offices should be combined to achieve greater efficiency and reduce the cost of administration\".'\n",
      "print parser.parse(tokenize(example)).pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  However/RB/Howev/O\n",
        "  ,/,/,/O\n",
        "  (NP [/CD/[/O)\n",
        "  (NP the/DT/the/O jury/NN/juri/O ]/NN/]/O)\n",
        "  (VP said/VBD/said/O)\n",
        "  (NP it/PRP/it/O)\n",
        "  (VP believes/VBZ/believ/O)\n",
        "  ``/``/``/O\n",
        "  (NP these/DT/these/O two/CD/two/O offices/NNS/offic/O)\n",
        "  (VP\n",
        "    should/MD/should/O\n",
        "    be/VB/be/O\n",
        "    combined/VBN/combin/O\n",
        "    to/TO/to/O\n",
        "    achieve/VB/achiev/O)\n",
        "  (NP greater/JJR/greater/O efficiency/NN/effici/O)\n",
        "  and/CC/and/O\n",
        "  (VP reduce/VB/reduc/O)\n",
        "  (NP the/DT/the/O cost/NN/cost/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP administration/NN/administr/O)\n",
        "  ''/''/''/O\n",
        "  ./././O)\n"
       ]
      }
     ],
     "prompt_number": 135
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "example = u'The Hartsfield home is at 637 E. Pelham Rd. Aj.'\n",
      "print parser.parse(tokenize(example)).pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP\n",
        "    The/DT/The/O\n",
        "    Hartsfield/NNP/Hartsfield/ORGANIZATION\n",
        "    home/NN/home/O)\n",
        "  (VP is/VBZ/is/O)\n",
        "  (PP at/IN/at/O)\n",
        "  (NP\n",
        "    637/CD/637/O\n",
        "    E./NNP/E./LOCATION\n",
        "    Pelham/NNP/Pelham/LOCATION\n",
        "    Rd./NNP/Rd./LOCATION\n",
        "    Aj/NNP/Aj/PERSON)\n",
        "  ./././O)\n"
       ]
      }
     ],
     "prompt_number": 136
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "example = u'Talking of the rapid population growth (upwards of 12,000 babies born daily) with an immigrant entering the United States every 1-1/2 minutes, he said \"our organization has not been keeping pace with this challenge\".'\n",
      "print parser.parse(tokenize(example)).pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (PP Talking/VBG/Talk/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP\n",
        "    the/DT/the/O\n",
        "    rapid/JJ/rapid/O\n",
        "    population/NN/popul/O\n",
        "    growth/NN/growth/O\n",
        "    (/CD/(/O\n",
        "    upwards/NNS/upward/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP 12,000/CD/12,000/O babies/NNS/babi/O)\n",
        "  (VP born/VBN/born/O)\n",
        "  (NP daily/JJ/daili/O )/NN/)/O)\n",
        "  (PP with/IN/with/O)\n",
        "  (NP an/DT/an/O immigrant/JJ/immigr/O)\n",
        "  (VP entering/VBG/enter/O)\n",
        "  (NP\n",
        "    the/DT/the/O\n",
        "    United/NNP/Unit/LOCATION\n",
        "    States/NNPS/State/LOCATION)\n",
        "  (NP every/DT/everi/O 1-1/2/CD/1-1/2/O minutes/NNS/minut/O)\n",
        "  ,/,/,/O\n",
        "  (NP he/PRP/he/O)\n",
        "  (VP said/VBD/said/O)\n",
        "  ``/``/``/O\n",
        "  (NP our/PRP$/our/O organization/NN/organ/O)\n",
        "  (VP has/VBZ/ha/O not/RB/not/O been/VBN/been/O keeping/VBG/keep/O)\n",
        "  (NP pace/NN/pace/O)\n",
        "  (PP with/IN/with/O)\n",
        "  (NP this/DT/thi/O challenge/NN/challeng/O)\n",
        "  ''/''/''/O\n",
        "  ./././O)\n"
       ]
      }
     ],
     "prompt_number": 137
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "example = u'the sweet and funny little dance about potato planting called \"Bul\\'ba\";;'\n",
      "print parser.parse(tokenize(example)).pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP\n",
        "    the/DT/the/O\n",
        "    sweet/JJ/sweet/O\n",
        "    and/CC/and/O\n",
        "    funny/JJ/funni/O\n",
        "    little/JJ/littl/O\n",
        "    dance/NN/danc/O)\n",
        "  (PP about/IN/about/O)\n",
        "  (NP potato/NN/potato/O)\n",
        "  (VP planting/VBG/plant/O called/VBN/call/O)\n",
        "  ``/``/``/O\n",
        "  (NP Bul'ba/NN/Bul'ba/O)\n",
        "  ''/''/''/O\n",
        "  ;/:/;/O\n",
        "  ;/:/;/O)\n"
       ]
      }
     ],
     "prompt_number": 138
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "example = u'The fraction of exchange was determined as the ratio of the counts / minute observed in the carbon tetrachloride to the counts / minute calculated for the carbon tetrachloride fractions.'\n",
      "print parser.parse(tokenize(example)).pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP The/DT/The/O fraction/NN/fraction/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP exchange/NN/exchang/O)\n",
        "  (VP was/VBD/wa/O determined/VBN/determin/O)\n",
        "  (PP as/IN/as/O)\n",
        "  (NP the/DT/the/O ratio/NN/ratio/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP the/DT/the/O counts/NNS/count/O)\n",
        "  //:///O\n",
        "  (NP minute/NN/minut/O)\n",
        "  (VP observed/VBN/observ/O)\n",
        "  (PP in/IN/in/O)\n",
        "  (NP\n",
        "    the/DT/the/O\n",
        "    carbon/NN/carbon/O\n",
        "    tetrachloride/NN/tetrachlorid/O)\n",
        "  (PP to/TO/to/O)\n",
        "  (NP the/DT/the/O counts/NNS/count/O)\n",
        "  //:///O\n",
        "  (NP minute/NN/minut/O)\n",
        "  (VP calculated/VBN/calcul/O)\n",
        "  (PP for/IN/for/O)\n",
        "  (NP\n",
        "    the/DT/the/O\n",
        "    carbon/NN/carbon/O\n",
        "    tetrachloride/NN/tetrachlorid/O\n",
        "    fractions/NNS/fraction/O)\n",
        "  ./././O)\n"
       ]
      }
     ],
     "prompt_number": 139
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "for t in parser.batch_parse(nltk.corpus.brown.sents()[200:220]):\n",
      "    print t.pprint()\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP\n",
        "    Fears/NNS/Fear/O\n",
        "    prejudicial/JJ/prejudici/O\n",
        "    aspects/NNS/aspect/O))\n",
        "\n",
        "(S\n",
        "  ``/``/``/O\n",
        "  (NP The/DT/The/O statements/NNS/statement/O)\n",
        "  (VP may/MD/may/O be/VB/be/O)\n",
        "  highly/RB/highli/O\n",
        "  prejudicial/JJ/prejudici/O\n",
        "  (PP to/TO/to/O)\n",
        "  (NP my/PRP$/my/O client/NN/client/O)\n",
        "  ''/''/''/O\n",
        "  ,/,/,/O\n",
        "  (NP Bellows/NNP/Bellow/PERSON)\n",
        "  (VP told/VBD/told/O)\n",
        "  (NP the/DT/the/O court/NN/court/O)\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  ``/``/``/O\n",
        "  (NP Some/DT/Some/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP the/DT/the/O defendants/NNS/defend/O)\n",
        "  strongly/RB/strongli/O\n",
        "  (VP indicated/VBD/indic/O)\n",
        "  (NP they/PRP/they/O)\n",
        "  (VP knew/VBD/knew/O)\n",
        "  (NP they/PRP/they/O)\n",
        "  (VP were/VBD/were/O receiving/VBG/receiv/O stolen/VBN/stolen/O)\n",
        "  (NP property/NN/properti/O)\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  (NP It/PRP/It/O)\n",
        "  (VP is/VBZ/is/O)\n",
        "  impossible/JJ/imposs/O\n",
        "  (VP to/TO/to/O get/VB/get/O)\n",
        "  (NP a/DT/a/O fair/JJ/fair/O trial/NN/trial/O)\n",
        "  when/WRB/when/O\n",
        "  (NP some/DT/some/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP the/DT/the/O defendants/NNS/defend/O)\n",
        "  (VP made/VBD/made/O)\n",
        "  (NP statements/NNS/statement/O)\n",
        "  (VP involving/VBG/involv/O)\n",
        "  (NP themselves/PRP/themselv/O)\n",
        "  and/CC/and/O\n",
        "  (NP others/NNS/other/O)\n",
        "  ''/''/''/O\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  (NP Judge/NNP/Judg/O Parsons/NNP/Parson/PERSON)\n",
        "  (VP leaned/VBD/lean/O)\n",
        "  (PP over/IN/over/O)\n",
        "  (NP the/DT/the/O bench/NN/bench/O)\n",
        "  and/CC/and/O\n",
        "  (VP inquired/VBD/inquir/O)\n",
        "  ,/,/,/O\n",
        "  ``/``/``/O\n",
        "  (NP You/PRP/You/O)\n",
        "  (VP mean/VBP/mean/O)\n",
        "  (NP some/DT/some/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP the/DT/the/O defendants/NNS/defend/O)\n",
        "  (VP made/VBD/made/O)\n",
        "  (NP statements/NNS/statement/O)\n",
        "  (VP admitting/VBG/admit/O)\n",
        "  (NP this/DT/thi/O)\n",
        "  ''/''/''/O\n",
        "  ?/./?/O\n",
        "  ?/./?/O)\n",
        "\n",
        "(S\n",
        "  ``/``/``/O\n",
        "  Yes/UH/Ye/O\n",
        "  ,/,/,/O\n",
        "  (NP your/PRP$/your/O honor/NN/honor/O)\n",
        "  ''/''/''/O\n",
        "  ,/,/,/O\n",
        "  (VP replied/VBD/repli/O)\n",
        "  (NP Bellows/NNP/Bellow/PERSON)\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  ``/``/``/O\n",
        "  (NP What/WP/What/O)\n",
        "  (NP this/DT/thi/O)\n",
        "  (VP amounts/VBZ/amount/O)\n",
        "  (PP to/TO/to/O)\n",
        "  ,/,/,/O\n",
        "  if/IN/if/O\n",
        "  true/JJ/true/O\n",
        "  ,/,/,/O\n",
        "  (VP is/VBZ/is/O)\n",
        "  that/IN/that/O\n",
        "  (NP there/EX/there/O)\n",
        "  (VP will/MD/will/O be/VB/be/O)\n",
        "  (NP a/DT/a/O free-for-all/NN/free-for-al/O fight/NN/fight/O)\n",
        "  (PP in/IN/in/O)\n",
        "  (NP this/DT/thi/O case/NN/case/O)\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  (NP There/EX/There/O)\n",
        "  (VP is/VBZ/is/O)\n",
        "  (NP a/DT/a/O conflict/NN/conflict/O)\n",
        "  (PP among/IN/among/O)\n",
        "  (NP the/DT/the/O defendants/NNS/defend/O)\n",
        "  ''/''/''/O\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  (NP Washington/NNP/Washington/LOCATION)\n",
        "  ,/,/,/O\n",
        "  (NP July/NNP/Juli/O 24/CD/24/O))\n",
        "\n",
        "(S\n",
        "  --/:/--/O\n",
        "  (NP\n",
        "    President/NNP/Presid/O\n",
        "    Kennedy/NNP/Kennedi/PERSON\n",
        "    today/NN/today/O)\n",
        "  (VP pushed/VBD/push/O)\n",
        "  aside/RB/asid/O\n",
        "  (NP\n",
        "    other/JJ/other/O\n",
        "    White/NNP/White/ORGANIZATION\n",
        "    House/NNP/Hous/ORGANIZATION\n",
        "    business/NN/busi/O)\n",
        "  (VP to/TO/to/O devote/VB/devot/O)\n",
        "  (NP\n",
        "    all/PDT/all/O\n",
        "    his/PRP$/hi/O\n",
        "    time/NN/time/O\n",
        "    and/CC/and/O\n",
        "    attention/NN/attent/O)\n",
        "  (PP to/TO/to/O)\n",
        "  (VP working/VBG/work/O)\n",
        "  (PP on/IN/on/O)\n",
        "  (NP\n",
        "    the/DT/the/O\n",
        "    Berlin/NNP/Berlin/LOCATION\n",
        "    crisis/NN/crisi/O\n",
        "    address/NN/address/O)\n",
        "  (NP he/PRP/he/O)\n",
        "  (VP will/MD/will/O deliver/VB/deliv/O)\n",
        "  (NP tomorrow/NN/tomorrow/O night/NN/night/O)\n",
        "  (PP to/TO/to/O)\n",
        "  (NP the/DT/the/O American/JJ/American/O people/NNS/peopl/O)\n",
        "  (PP over/IN/over/O)\n",
        "  (NP\n",
        "    nationwide/JJ/nationwid/O\n",
        "    television/NN/televis/O\n",
        "    and/CC/and/O\n",
        "    radio/NN/radio/O)\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  (NP The/DT/The/O President/NNP/Presid/O)\n",
        "  (VP spent/VBD/spent/O)\n",
        "  (NP much/JJ/much/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP the/DT/the/O week-end/NN/week-end/O)\n",
        "  (PP at/IN/at/O)\n",
        "  (NP his/PRP$/hi/O summer/NN/summer/O home/NN/home/O)\n",
        "  (PP on/IN/on/O)\n",
        "  (NP Cape/NNP/Cape/LOCATION Cod/NNP/Cod/LOCATION)\n",
        "  (VP writing/VBG/write/O)\n",
        "  (NP the/DT/the/O first/JJ/first/O drafts/NNS/draft/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP portions/NNS/portion/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP the/DT/the/O address/NN/address/O)\n",
        "  (PP with/IN/with/O)\n",
        "  (NP the/DT/the/O help/NN/help/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP\n",
        "    White/NNP/White/ORGANIZATION\n",
        "    House/NNP/Hous/ORGANIZATION\n",
        "    aids/NNS/aid/O)\n",
        "  (PP in/IN/in/O)\n",
        "  (NP Washington/NNP/Washington/LOCATION)\n",
        "  (PP with/IN/with/O)\n",
        "  (NP whom/WP/whom/O)\n",
        "  (NP he/PRP/he/O)\n",
        "  (VP talked/VBD/talk/O)\n",
        "  (PP by/IN/by/O)\n",
        "  (NP telephone/NN/telephon/O)\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  Shortly/RB/Shortli/O\n",
        "  (PP after/IN/after/O)\n",
        "  (NP the/DT/the/O Chief/NNP/Chief/O Executive/NNP/Execut/O)\n",
        "  (VP returned/VBD/return/O)\n",
        "  (PP to/TO/to/O)\n",
        "  (NP Washington/NNP/Washington/LOCATION)\n",
        "  (PP in/IN/in/O)\n",
        "  (NP midmorning/NN/midmorn/O)\n",
        "  (PP from/IN/from/O)\n",
        "  (NP Hyannis/NNP/Hyanni/LOCATION Port/NNP/Port/LOCATION)\n",
        "  ,/,/,/O\n",
        "  (NP Mass./NNP/Mass./LOCATION)\n",
        "  ,/,/,/O\n",
        "  (NP\n",
        "    a/DT/a/O\n",
        "    White/NNP/White/O\n",
        "    House/NNP/Hous/O\n",
        "    spokesman/NN/spokesman/O)\n",
        "  (VP said/VBD/said/O)\n",
        "  (NP the/DT/the/O address/NN/address/O text/NN/text/O)\n",
        "  still/RB/still/O\n",
        "  (VP had/VBD/had/O)\n",
        "  ``/``/``/O\n",
        "  quite/RB/quit/O\n",
        "  (NP a/DT/a/O way/NN/way/O)\n",
        "  (VP to/TO/to/O go/VB/go/O)\n",
        "  ''/''/''/O\n",
        "  (PP toward/IN/toward/O)\n",
        "  (NP completion/NN/complet/O)\n",
        "  ./././O)\n",
        "\n",
        "(S (NP Decisions/NNS/Decis/O) (VP are/VBP/are/O made/VBN/made/O))\n",
        "\n",
        "(S\n",
        "  (VP Asked/VBN/Ask/O to/TO/to/O elaborate/VB/elabor/O)\n",
        "  ,/,/,/O\n",
        "  (NP Pierre/NNP/Pierr/PERSON Salinger/NNP/Saling/PERSON)\n",
        "  ,/,/,/O\n",
        "  (NP\n",
        "    White/NNP/White/O\n",
        "    House/NNP/Hous/O\n",
        "    press/NN/press/O\n",
        "    secretary/NN/secretari/O)\n",
        "  ,/,/,/O\n",
        "  (VP replied/VBD/repli/O)\n",
        "  ,/,/,/O\n",
        "  ``/``/``/O\n",
        "  (NP I/PRP/I/O)\n",
        "  (VP would/MD/would/O say/VB/say/O)\n",
        "  (NP it's/NNS/it'/O)\n",
        "  (VP got/VBD/got/O to/TO/to/O go/VB/go/O)\n",
        "  (PP thru/IN/thru/O)\n",
        "  (NP several/JJ/sever/O more/JJR/more/O drafts/NNS/draft/O)\n",
        "  ''/''/''/O\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  (NP Salinger/NNP/Saling/PERSON)\n",
        "  (VP said/VBD/said/O)\n",
        "  (NP\n",
        "    the/DT/the/O\n",
        "    work/NN/work/O\n",
        "    President/NNP/Presid/O\n",
        "    Kennedy/NNP/Kennedi/PERSON)\n",
        "  ,/,/,/O\n",
        "  (NP advisers/NNS/advis/O)\n",
        "  ,/,/,/O\n",
        "  and/CC/and/O\n",
        "  (NP members/NNS/member/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP his/PRP$/hi/O staff/NN/staff/O)\n",
        "  (VP were/VBD/were/O doing/VBG/do/O)\n",
        "  (PP on/IN/on/O)\n",
        "  (NP the/DT/the/O address/NN/address/O)\n",
        "  (VP involved/VBD/involv/O)\n",
        "  (NP composition/NN/composit/O and/CC/and/O wording/NN/word/O)\n",
        "  ,/,/,/O\n",
        "  rather/RB/rather/O\n",
        "  (PP than/IN/than/O)\n",
        "  (NP last/JJ/last/O minute/NN/minut/O decisions/NNS/decis/O)\n",
        "  (PP on/IN/on/O)\n",
        "  (NP administration/NN/administr/O plans/NNS/plan/O)\n",
        "  (VP to/TO/to/O meet/VB/meet/O)\n",
        "  (NP\n",
        "    the/DT/the/O\n",
        "    latest/JJS/latest/O\n",
        "    Berlin/NNP/Berlin/LOCATION\n",
        "    crisis/NN/crisi/O)\n",
        "  (VP precipitated/VBN/precipit/O)\n",
        "  (PP by/IN/by/O)\n",
        "  (NP\n",
        "    Russia's/NN/Russia'/LOCATION\n",
        "    demands/NNS/demand/O\n",
        "    and/CC/and/O\n",
        "    proposals/NNS/propos/O)\n",
        "  (PP for/IN/for/O)\n",
        "  (NP the/DT/the/O city/NN/citi/O)\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  (NP The/DT/The/O last/JJ/last/O 10/CD/10/O cases/NNS/case/O)\n",
        "  (PP in/IN/in/O)\n",
        "  (NP the/DT/the/O investigation/NN/investig/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP the/DT/the/O Nov./NNP/Nov./O 8/CD/8/O election/NN/elect/O)\n",
        "  (VP were/VBD/were/O dismissed/VBN/dismiss/O)\n",
        "  (NP yesterday/NN/yesterday/O)\n",
        "  (PP by/IN/by/O)\n",
        "  (NP\n",
        "    Acting/NNP/Act/O\n",
        "    Judge/NNP/Judg/O\n",
        "    John/NNP/John/PERSON\n",
        "    M./NNP/M./PERSON\n",
        "    Karns/NNP/Karn/PERSON)\n",
        "  ,/,/,/O\n",
        "  (NP who/WP/who/O)\n",
        "  (VP charged/VBD/charg/O)\n",
        "  that/IN/that/O\n",
        "  (NP the/DT/the/O prosecution/NN/prosecut/O)\n",
        "  (VP obtained/VBD/obtain/O)\n",
        "  (NP evidence/NN/evid/O)\n",
        "  ``/``/``/O\n",
        "  (PP by/IN/by/O)\n",
        "  (NP\n",
        "    unfair/JJ/unfair/O\n",
        "    and/CC/and/O\n",
        "    fundamentally/RB/fundament/O\n",
        "    illegal/JJ/illeg/O\n",
        "    means/NNS/mean/O)\n",
        "  ''/''/''/O\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  (NP Karns/NNP/Karn/PERSON)\n",
        "  (VP said/VBD/said/O)\n",
        "  that/IN/that/O\n",
        "  (NP the/DT/the/O cases/NNS/case/O)\n",
        "  (VP involved/VBD/involv/O)\n",
        "  (NP a/DT/a/O matter/NN/matter/O)\n",
        "  ``/``/``/O\n",
        "  (PP of/IN/of/O)\n",
        "  (NP\n",
        "    even/RB/even/O\n",
        "    greater/JJR/greater/O\n",
        "    significance/NN/signific/O)\n",
        "  (PP than/IN/than/O)\n",
        "  (NP the/DT/the/O guilt/NN/guilt/O or/CC/or/O innocence/NN/innoc/O)\n",
        "  ''/''/''/O\n",
        "  (PP of/IN/of/O)\n",
        "  (NP the/DT/the/O 50/CD/50/O persons/NNS/person/O)\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  (NP He/PRP/He/O)\n",
        "  (VP said/VBD/said/O)\n",
        "  (NP evidence/NN/evid/O)\n",
        "  (VP was/VBD/wa/O obtained/VBN/obtain/O)\n",
        "  ``/``/``/O\n",
        "  (PP in/IN/in/O)\n",
        "  (NP violation/NN/violat/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP the/DT/the/O legal/JJ/legal/O rights/NNS/right/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP citizens/NNS/citizen/O)\n",
        "  ''/''/''/O\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  (NP Karns'/NN/Karns'/PERSON ruling/NN/rule/O)\n",
        "  (VP pertained/VBD/pertain/O)\n",
        "  (PP to/TO/to/O)\n",
        "  (NP eight/CD/eight/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP the/DT/the/O 10/CD/10/O cases/NNS/case/O)\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  (PP In/IN/In/O)\n",
        "  (NP the/DT/the/O two/CD/two/O other/JJ/other/O cases/NNS/case/O)\n",
        "  (NP he/PRP/he/O)\n",
        "  (VP ruled/VBD/rule/O)\n",
        "  that/IN/that/O\n",
        "  (NP the/DT/the/O state/NN/state/O)\n",
        "  (VP had/VBD/had/O been/VBN/been/O)\n",
        "  ``/``/``/O\n",
        "  unable/JJ/unabl/O\n",
        "  (VP to/TO/to/O make/VB/make/O)\n",
        "  (NP a/DT/a/O case/NN/case/O)\n",
        "  ''/''/''/O\n",
        "  ./././O)\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 140
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "timeit parser.batch_parse(nltk.corpus.brown.sents())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 20min 59s per loop\n"
       ]
      }
     ],
     "prompt_number": 141
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "On a Mac with 1.3 GHz Intel Core i5, *shallow parsing* the 57,340 Brown corpus sentences takes 21 min, that is **45 sentences/sec**."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}