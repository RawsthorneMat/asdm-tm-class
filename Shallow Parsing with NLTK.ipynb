{
 "metadata": {
  "name": "",
  "signature": "sha256:5a8b5e454d54710be4e6d88c83bc54f98e4f2f2725a4c4a97eb050164e0469d0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Shallow Parsing with NLTK"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "On PoS Tagging, Named Entity Recognition, and Chunking"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To be able to move into relationship extraction (or any other \"advanced text mining\" task), it is crucial to do reliable PoS tagging, NER and phrase chunking - in summary often referred to as \"shallow parsing\". That implies that we are leaving aside natural language [dependency] parsing in favour of \"high-throughput\" text mining and hope to figure out the \"dependencies\" we would get from the former using patterns and/or machine learning techniques instead. This could be done, for example, by using the resulting chunks, entities, tokens & tags, n-grams, and any other content as features for a classifier that then should detect the presence of a (binary, semantic) relationship. We can even add in a few manual feature rules to improve performance, which is easy, for example, with MaxEnt, as we have seen yesterday.\n",
      "\n",
      "Most of what we are doing here - training chunkers, what Trees are, NER with NLTK - is well documented in the NLTK Book, [Chapter 7](http://www.nltk.org/book/ch07.html), which you should have a look at if you want to get serious with NLTK."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Quick-and-dirty Tagging, Chunking, and NER with NLTK"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import ne_chunk, pos_tag, word_tokenize as tokenize\n",
      "from nltk.stem.snowball import EnglishStemmer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def analyze(sentence):\n",
      "    return ne_chunk(pos_tag(tokenize(sentence)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print analyze(\n",
      "    u'The economist Jeffrey Sachs coined the term \"shock therapy\".'\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  The/DT\n",
        "  economist/NN\n",
        "  (PERSON Jeffrey/NNP Sachs/NNP)\n",
        "  coined/VBD\n",
        "  the/DT\n",
        "  term/NN\n",
        "  ``/``\n",
        "  shock/NN\n",
        "  therapy/NN\n",
        "  ''/''\n",
        "  ./.)\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print analyze(u'The Google Nexus challenges the iPhone\u2018s dominance.').pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  The/DT\n",
        "  (ORGANIZATION Google/NNP Nexus/NNP)\n",
        "  challenges/NNS\n",
        "  the/DT\n",
        "  iPhone\u2018s/NNS\n",
        "  dominance/NN\n",
        "  ./.)\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print analyze(u'The quick brown fox jumped over the lazy dog.').pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  The/DT\n",
        "  quick/NN\n",
        "  brown/NN\n",
        "  fox/NN\n",
        "  jumped/VBD\n",
        "  over/IN\n",
        "  the/DT\n",
        "  lazy/NN\n",
        "  dog/NN\n",
        "  ./.)\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print analyze(u'The brown fox quickly jumped over the lazy dog.').pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  The/DT\n",
        "  brown/NN\n",
        "  fox/NN\n",
        "  quickly/RB\n",
        "  jumped/VBD\n",
        "  over/IN\n",
        "  the/DT\n",
        "  lazy/NN\n",
        "  dog/NN\n",
        "  ./.)\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print analyze(u'A summer school in Madrid is sunnier than one in the UK.').pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  A/DT\n",
        "  summer/NN\n",
        "  school/NN\n",
        "  in/IN\n",
        "  (GPE Madrid/NNP)\n",
        "  is/VBZ\n",
        "  sunnier/JJR\n",
        "  than/IN\n",
        "  one/CD\n",
        "  in/IN\n",
        "  the/DT\n",
        "  (ORGANIZATION UK/NNP)\n",
        "  ./.)\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`GPE` = geo-political entity\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "NLTK's \"default\" NER (`ne_chunk( pos_tag( word_tokenize( sentence )))`) provides good-enough out-of the box NLP to experiment with, but you'll have to do more if you need to get those \"high-quality\" results. For that, NLTK provides wrappers to the most powerfull CRF taggers around, like [MALLET](http://mallet.cs.umass.edu). Furthermore, due the nature of Python, it is very easy to write wrappers for C(++)-based NLP tools; e.g., [CRFsuite](http://www.chokkan.org/software/crfsuite/), a high-speed CRF in \"pure\" C, comes with a Python wrapper. As long as they have a I/O (stdin/stdout) interface to send and receive text, for any others as well, for examle the all-in-one [GENIA Tagger](http://www.nactem.ac.uk/tsujii/GENIA/tagger/) for the biomedical domain.\n",
      "\n",
      "For the remainder of this tutorial, we will [download](http://nlp.stanford.edu/software/CRF-NER.shtml) and install the **Stanford NER Tagger**, and their [PoS Tagger](http://nlp.stanford.edu/software/tagger.shtml) with an already trained model and see how the above examples can be improved.\n",
      "\n",
      "To train a NER model for newswire, you could use [CoNLL2003](http://www.cnts.ua.ac.be/conll2003/ner/) - anoyingly enough, however, that is not part of NLTK, and requires making a written request for the (Reuters) text corpus. Gabrilovich maintains [a list](http://www.cs.technion.ac.il/~gabr/resources/data/ne_datasets.html) of \"standard\" NER-tagger corpora (PER/ORG/GPE...). If you were interested in detecting gene and protein names, you might have a look at our [BioCreative](http://www.biocreative.org/) Gene-Entity corpora (note that you have to register on the website before you can download any corpus on the site, thereby confirming that you will not use the data for commerical purposes)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Stanford NER Setup"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tag.stanford import NERTagger\n",
      "from nltk.tag.stanford import POSTagger\n",
      "import nltk.data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that for the below commands, we assume the Stanford NER and PoS Taggers are in subdirectories located in the same directory as this notebook:\n",
      "\n",
      "- The NER Tagger files should be in a subdirectory with the path `stanford/ner/`.\n",
      "- The PoS Tagger files should be in a subdriectory with the path `stanford/pos/`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ner = NERTagger('stanford/ner/english.all.3class.distsim.crf.ser.gz',\n",
      "                'stanford/ner/stanford-ner-3.4.jar', 'UTF-8') # encoding!\n",
      "pos = POSTagger('stanford/pos/english-bidirectional-distsim.tagger',\n",
      "                'stanford/pos/stanford-postagger-3.4.jar', 'UTF-8') # encoding!\n",
      "st = EnglishStemmer()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def analyze(sentence):\n",
      "    tokens = tokenize(sentence)\n",
      "    stems = [st.stem(t) for t in tokens] #map(st.stem, tokens)\n",
      "    _, tags = zip(*pos.tag(tokens))\n",
      "    _, entities = zip(*ner.tag(tokens))\n",
      "    return zip(tokens, stems, tags, entities)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "analyze(u'The quick brown fox jumped over the lazy dog.')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "[(u'The', u'the', u'DT', u'O'),\n",
        " (u'quick', u'quick', u'JJ', u'O'),\n",
        " (u'brown', u'brown', u'JJ', u'O'),\n",
        " (u'fox', u'fox', u'NN', u'O'),\n",
        " (u'jumped', u'jump', u'VBD', u'O'),\n",
        " (u'over', u'over', u'IN', u'O'),\n",
        " (u'the', u'the', u'DT', u'O'),\n",
        " (u'lazy', u'lazi', u'JJ', u'O'),\n",
        " (u'dog', u'dog', u'NN', u'O'),\n",
        " (u'.', u'.', u'.', u'O')]"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "analyze(u'A summer school in Madrid is sunnier than one in the UK.')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "[(u'A', u'a', u'DT', u'O'),\n",
        " (u'summer', u'summer', u'NN', u'O'),\n",
        " (u'school', u'school', u'NN', u'O'),\n",
        " (u'in', u'in', u'IN', u'O'),\n",
        " (u'Madrid', u'madrid', u'NNP', u'LOCATION'),\n",
        " (u'is', u'is', u'VBZ', u'O'),\n",
        " (u'sunnier', u'sunnier', u'JJR', u'O'),\n",
        " (u'than', u'than', u'IN', u'O'),\n",
        " (u'one', u'one', u'CD', u'O'),\n",
        " (u'in', u'in', u'IN', u'O'),\n",
        " (u'the', u'the', u'DT', u'O'),\n",
        " (u'UK', u'uk', u'NNP', u'LOCATION'),\n",
        " (u'.', u'.', u'.', u'O')]"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Much better, indeed!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "analyze(u'The Google Nexus challanges the iPhone\u2018s dominance.')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "[(u'The', u'the', u'DT', u'O'),\n",
        " (u'Google', u'googl', u'NNP', u'ORGANIZATION'),\n",
        " (u'Nexus', u'nexus', u'NNP', u'O'),\n",
        " (u'challanges', u'challang', u'VBZ', u'O'),\n",
        " (u'the', u'the', u'DT', u'O'),\n",
        " (u'iPhone\\u2018s', u'iphon', u'NN', u'O'),\n",
        " (u'dominance', u'domin', u'NN', u'O'),\n",
        " (u'.', u'.', u'.', u'O')]"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Good: notice that the Stanford NER now correctly *misses* the \"Nexus\" and \"iPhone\" entities."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "MegaM Chunking and PoS Tagging"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After this, we only need one final step: chunking. As this is \"trivial\" once we have good PoS tags, we will train our own NLTK chunker. But, being lazy as always, there is a neat collection of scripts to train NLTK text, PoS and chunk classifiers: [NLTK-Trainer](http://nltk-trainer.readthedocs.org/en/latest/index.html), however, it requires you to use Python 2.x. You could either [download](https://github.com/japerk/nltk-trainer/archive/master.zip) it or fetch it using Git: `git clone https://github.com/japerk/nltk-trainer.git`. We will train a MEMM chunker using MegaM, or, if you don't have MegaM, will use NaiveBayes.\n",
      "\n",
      "This also requires you run `python setup.py install` in the `python-trainer` directory."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run nltk-trainer/train_chunker.py conll2000 --classifier MEGAM"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading conll2000\n",
        "10948 chunks, training on 10948"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training ClassifierChunker with ['MEGAM'] classifier\n",
        "[Found megam: /usr/local/bin/megam.opt]\n",
        "Constructing training corpus for classifier."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training classifier (259104 instances)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training MEGAM classifier\n",
        "evaluating ClassifierChunker"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ChunkParse score:\n",
        "    IOB Accuracy:  95.0%\n",
        "    Precision:     92.5%\n",
        "    Recall:        93.0%\n",
        "    F-Measure:     92.8%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "dumping ClassifierChunker to /Users/fleitner/nltk_data/chunkers/conll2000_MEGAM.pickle\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This should produce an F-Measure of about 93%.\n",
      "\n",
      "If you do not have MegaM installed, you could at least try the Na\u00efve Bayes approach."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run nltk-trainer/train_chunker.py conll2000 --classifier NaiveBayes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading conll2000\n",
        "10948 chunks, training on 10948"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training ClassifierChunker with ['NaiveBayes'] classifier\n",
        "Constructing training corpus for classifier."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training classifier (259104 instances)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training NaiveBayes classifier\n",
        "evaluating ClassifierChunker"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ChunkParse score:\n",
        "    IOB Accuracy:  93.5%\n",
        "    Precision:     88.8%\n",
        "    Recall:        91.1%\n",
        "    F-Measure:     89.9%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "dumping ClassifierChunker to /Users/fleitner/nltk_data/chunkers/conll2000_NaiveBayes.pickle\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This should produce an F-Measure of about 90%."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%run nltk-trainer/train_chunker.py conll2000 --classifier sklearn.*"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will also take our chances and see how a MegaM-based MEMM PoS tagger performs and if it would be possible to therefore skip the Stanford PoS tagger (alternative: Naive Bayes PoS tagging). **Warning**: This step takes a long time (minutes, at leat)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run nltk-trainer/train_tagger.py conll2000 --classifier MEGAM"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run nltk-trainer/train_tagger.py conll2000 --classifier NaiveBayes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you completed the above training steps, you can now import the models:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chunker = nltk.data.load('chunkers/conll2000_MEGAM.pickle')\n",
      "pos = nltk.data.load('taggers/conll2000_MEGAM_aubt.pickle')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chunker = nltk.data.load('chunkers/conll2000_NaiveBayes.pickle')\n",
      "pos = nltk.data.load('taggers/conll2000_NaiveBayes_aubt.pickle')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's see how good this works, using whatever classifier pair you trained. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print chunker.parse(pos.tag(tokenize('The economist Jeffrey Sachs coined the term \"shock therapy\".'))).pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP The/DT economist/NN Jeffrey/NNP Sachs/NNP)\n",
        "  (VP coined/VBD)\n",
        "  (NP the/DT term/NN)\n",
        "  ``/``\n",
        "  (NP shock/NN therapy/NN)\n",
        "  ''/''\n",
        "  ./.)\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chunker.parse(pos.tag(tokenize('The Google Nexus challanges the iPhone\u2018s dominance.')))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "Tree('S', [Tree('NP', [('The', 'DT'), ('Google', 'JJ'), ('Nexus', 'NNPS')]), Tree('VP', [('challanges', 'VBZ')]), Tree('NP', [('the', 'DT'), ('iPhone\\xe2\\x80\\x98s', 'NNPS'), ('dominance', 'NN')]), ('.', '.')])"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print chunker.parse(pos.tag(tokenize('The Google Nexus challanges the iPhone\u2018s dominance.'))).pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP The/DT Google/JJ Nexus/NNPS)\n",
        "  (VP challanges/VBZ)\n",
        "  (NP the/DT iPhone\u2018s/NNPS dominance/NN)\n",
        "  ./.)\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's create a new function and combine this with the entity tags:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def analyze(sentence):\n",
      "    \n",
      "    def combine(tree, *iters):\n",
      "        for idx in range(len(tree)):\n",
      "            n = tree[idx]\n",
      "\n",
      "            if isinstance(n, nltk.tree.Tree):\n",
      "                combine(n, *iters)\n",
      "            elif isinstance(n, tuple):\n",
      "                tmp = [n[0], n[1]] + [next(i) for i in iters]\n",
      "                tree[idx] = tuple(tmp)\n",
      "            else:\n",
      "                raise RuntimeError('unexpected ' + repr(n))\n",
      "            \n",
      "    tokens = tokenize(sentence)\n",
      "    _, entities = zip(*ner.tag(tokens))\n",
      "    stems = map(st.stem, tokens)\n",
      "    tree = chunker.parse(pos.tag(tokens))\n",
      "    combine(tree, iter(stems), iter(entities))\n",
      "    return tree "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print analyze(u'The Google Nexus challanges the iPhone\u2018s dominance.').pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP The/DT/the/O Google/JJ/googl/ORGANIZATION Nexus/NNPS/nexus/O)\n",
        "  (VP challanges/VBZ/challang/O)\n",
        "  (NP the/DT/the/O iPhone\u2018s/NNPS/iphon/O dominance/NN/domin/O)\n",
        "  ./././O)\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print analyze(u'The brown fox quickly jumps over the lazy dog.').pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP The/DT/the/O brown/JJ/brown/O fox/NN/fox/O)\n",
        "  quickly/RB/quick/O\n",
        "  (VP jumps/VBZ/jump/O)\n",
        "  (PP over/IN/over/O)\n",
        "  (NP the/DT/the/O lazy/JJ/lazi/O dog/NN/dog/O)\n",
        "  ./././O)\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print analyze(u'The quick brown fox jumped over the lazy dog.').pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP The/DT/the/O quick/JJ/quick/O brown/NN/brown/O)\n",
        "  (NP fox/WP$/fox/O)\n",
        "  (VP jumped/VBD/jump/O)\n",
        "  (PP over/IN/over/O)\n",
        "  (NP the/DT/the/O lazy/JJ/lazi/O dog/NN/dog/O)\n",
        "  ./././O)\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pos = POSTagger('stanford/pos/english-bidirectional-distsim.tagger',\n",
      "                'stanford/pos/stanford-postagger-3.4.jar', 'UTF-8') # encoding!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print analyze(u'The quick brown fox jumped over the lazy dog.').pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP The/DT/the/O quick/JJ/quick/O brown/NN/brown/O)\n",
        "  (NP fox/WP$/fox/O)\n",
        "  (VP jumped/VBD/jump/O)\n",
        "  (PP over/IN/over/O)\n",
        "  (NP the/DT/the/O lazy/JJ/lazi/O dog/NN/dog/O)\n",
        "  ./././O)\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we wrap all up and create a class to do language processing in the future (on the way fixing a ton of bugs to actually run it on any English text...):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "from nltk import data, word_tokenize as tokenize\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "from nltk.tag.stanford import NERTagger\n",
      "from nltk.tree import Tree\n",
      "\n",
      "\n",
      "class NEChunker(object):\n",
      "    \n",
      "    def __init__(self,\n",
      "                 chunker_data='chunkers/conll2000_MEGAM.pickle',\n",
      "                 sf_ner_model_path='stanford/ner/english.all.3class.distsim.crf.ser.gz',\n",
      "                 sf_ner_jar_path='stanford/ner/stanford-ner-3.4.jar',\n",
      "                 sf_pos_model_path='stanford/pos/english-bidirectional-distsim.tagger',\n",
      "                 sf_pos_jar_path='stanford/pos/stanford-postagger-3.4.jar',\n",
      "                 encoding='UTF-8'):\n",
      "        self.chunker = data.load(chunker_data)\n",
      "        self.ner = NERTagger(sf_ner_model_path,\n",
      "                             sf_ner_jar_path,\n",
      "                             encoding)\n",
      "        self.pos = POSTagger(sf_pos_model_path,\n",
      "                             sf_pos_jar_path,\n",
      "                             encoding)\n",
      "        self.stem = PorterStemmer().stem\n",
      "\n",
      "    def parse(self, tokens):\n",
      "        return self.batch_parse([tokens])[0]\n",
      "\n",
      "    def batch_parse(self, sentences):\n",
      "        entities = self.ner.batch_tag(sentences)\n",
      "        NEChunker.fix_stanford(entities, sentences)\n",
      "        entities = NEChunker.extract_tags(entities)\n",
      "        stems = [[self.stem(t) for t in s] for s in sentences]\n",
      "        pos_tags = self.pos.batch_tag(sentences)\n",
      "        # apparently, the PoSTagger does not interfere\n",
      "        # with its input\n",
      "        # NEChunker.fix_stanford(pos_tags, sentences)\n",
      "        trees = self.chunker.batch_parse(pos_tags)\n",
      "        \n",
      "        for t, s, e in zip(trees, stems, entities):\n",
      "            try:\n",
      "                NEChunker.combine(t, iter(s), iter(e))\n",
      "            except RuntimeError:\n",
      "                print \"Tree:\", t.pprint()\n",
      "                print \"Stems:\", len(s), s\n",
      "                print \"Entities:\", len(e), e\n",
      "                raise\n",
      "        \n",
      "        return trees\n",
      "\n",
      "    @staticmethod\n",
      "    def fix_stanford(tags, sentences):\n",
      "        \"\"\"\n",
      "        StandfordTaggers splits tokens and sentences unwarrented.\n",
      "        \n",
      "        This fixes the (wrong!) splits they introduce and removes\n",
      "        the sentence terminal dots it introduces \"de novo\".\n",
      "        \"\"\"\n",
      "        for i, s in enumerate(sentences):\n",
      "            if len(tags[i]) < len(s):\n",
      "                NEChunker._merge_next_tag(tags, i)\n",
      "                \n",
      "            if len(tags[i]) > len(s):\n",
      "                NEChunker._merge_tokens(tags[i], s)\n",
      "                \n",
      "            if len(tags[i]) > len(s):\n",
      "                NEChunker._split_tag(tags, i, len(s))\n",
      "                \n",
      "            if len(tags[i]) != len(s):\n",
      "                NEChunker._fail_alignment(tags[i], s)\n",
      "        \n",
      "        assert len(tags) == len(sentences), \\\n",
      "            'could not align the Stanford tags: ' + \\\n",
      "            '%d != %d' % (len(tags), len(sentences))\n",
      "    \n",
      "    @staticmethod\n",
      "    def _fail_alignment(tags, sentence):\n",
      "        raise RuntimeError(\n",
      "            'could not align the tags:\\n' + \\\n",
      "            '%s\\n%s' % (\" \".join(t[0] for t in tags),\n",
      "                        \" \".join(sentence))\n",
      "        )\n",
      "    \n",
      "    @staticmethod\n",
      "    def _merge_next_tag(tags, i):\n",
      "        tags[i] += tags[i+1]\n",
      "        del tags[i+1]\n",
      "\n",
      "    @staticmethod\n",
      "    def _merge_tokens(tags, tokens):\n",
      "        for i, tok in enumerate(tokens):\n",
      "            # if SNER removed replicated terminals\n",
      "            # (like \"??\"), add them back in\n",
      "            if i >= len(tags) and tok in (u\"?\", u\".\", u\"!\"):\n",
      "                tags.append((tok, \".\"))\n",
      "                continue\n",
      "                \n",
      "            assert len(tags) > i, \\\n",
      "                u'aligning \"%s\" in\\n%s\\nat offset=%d to\\n%s\\n(len=%d) will fail' % (\n",
      "                    tok, u' '.join(tokens), i,\n",
      "                    u' '.join([u for u, v in tags]), len(tags)\n",
      "                )\n",
      "            \n",
      "            if tags[i][0] != tok:\n",
      "                t = tags[i][0]\n",
      "                \n",
      "                # if SNER introduced an extra sentence\n",
      "                # terminal token, remove it again\n",
      "                if t == '.' and len(tags) > len(tokens):\n",
      "                    del tags[i]\n",
      "                    continue\n",
      "                \n",
      "                # naively assume SNER's replacements of brackets\n",
      "                # with bracket-escape sequences are all correct\n",
      "                if re.match(r\"\\-..B\\-\", t):\n",
      "                    continue\n",
      "                \n",
      "                # otherwise, the SNER token must at least\n",
      "                # match the beginning of the \"real\" token\n",
      "                assert tok.startswith(t), \\\n",
      "                    u'expected \"%s\" in\\n%s\\nto start with \"%s\" from\\n%s' % (\n",
      "                        tok, u\" \".join(tokens), t,\n",
      "                        u\" \".join([u for u, v in tags])\n",
      "                    )\n",
      "                # join the current SNER token with the next\n",
      "                tags[i] = (t + tags[i+1][0], tags[i][1])\n",
      "                # now it should be matching\n",
      "                assert tags[i][0] == tok, \\\n",
      "                    u'joining \"%s\" to match \"%s\" failed\\n%s' % (\n",
      "                        tags[i][0], tok, u\" \".join(tokens)\n",
      "                    )\n",
      "                # remove the splitted terminal\n",
      "                del tags[i+1]\n",
      "        \n",
      "    @staticmethod\n",
      "    def _split_tag(tags, i, l):\n",
      "        tail = tags[i][l:]\n",
      "        tags[i] = tags[i][:l]\n",
      "        tags.insert(i + 1, tail)\n",
      "\n",
      "    @staticmethod\n",
      "    def extract_tags(sentences, idx=-1):\n",
      "        \"\"\"\n",
      "        Drop all but one tag per token in sentences.\n",
      "        \n",
      "        I.e., drops all but one element in a list of\n",
      "        lists of lists.\n",
      "        \"\"\"\n",
      "        return [[t[idx] for t in s] for s in sentences]\n",
      "    \n",
      "    @staticmethod\n",
      "    def combine(tree, *iters):\n",
      "        \"\"\"\n",
      "        Merge one ore more tags into the tree.\n",
      "        \n",
      "        The tags have to be provided as iterators,\n",
      "        not lists.\n",
      "        \"\"\"\n",
      "        for idx in range(len(tree)):\n",
      "            n = tree[idx]\n",
      "\n",
      "            if isinstance(n, Tree):\n",
      "                NEChunker.combine(n, *iters)\n",
      "            elif isinstance(n, tuple):\n",
      "                tree[idx] = NEChunker._merge_tags(n, iters)\n",
      "            else:\n",
      "                raise RuntimeError('unexpected ' + repr(n) +\n",
      "                                   ' in ' + repr(tree))\n",
      "    \n",
      "    @staticmethod\n",
      "    def _merge_tags(node, tag_iters):\n",
      "        tmp = [node[0], node[1]]\n",
      "\n",
      "        try:\n",
      "            for i in tag_iters:\n",
      "                tmp.append(next(i))                    \n",
      "        except StopIteration:\n",
      "            raise RuntimeError('items to short in ' +\n",
      "                               repr(tree))\n",
      "\n",
      "        return tuple(tmp)\n",
      "\n",
      "\n",
      "nec = NEChunker()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that we no longer tokenize inside the function to work better with the NLTK API."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print nec.parse(tokenize(u'The quick brown fox jumped over the lazy dog.')).pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP The/DT/The/O quick/JJ/quick/O brown/JJ/brown/O fox/NN/fox/O)\n",
        "  (VP jumped/VBD/jump/O)\n",
        "  (PP over/IN/over/O)\n",
        "  (NP the/DT/the/O lazy/JJ/lazi/O dog/NN/dog/O)\n",
        "  ./././O)\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "example = u'However, the jury said it believes \"these two offices should be combined to achieve greater efficiency and reduce the cost of administration\".'\n",
      "print nec.parse(tokenize(example)).pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  However/RB/Howev/O\n",
        "  ,/,/,/O\n",
        "  (NP the/DT/the/O jury/NN/juri/O)\n",
        "  (VP said/VBD/said/O)\n",
        "  (NP it/PRP/it/O)\n",
        "  (VP believes/VBZ/believ/O)\n",
        "  ``/``/``/O\n",
        "  (NP these/DT/these/O two/CD/two/O offices/NNS/offic/O)\n",
        "  (VP\n",
        "    should/MD/should/O\n",
        "    be/VB/be/O\n",
        "    combined/VBN/combin/O\n",
        "    to/TO/to/O\n",
        "    achieve/VB/achiev/O)\n",
        "  (NP greater/JJR/greater/O efficiency/NN/effici/O)\n",
        "  and/CC/and/O\n",
        "  (VP reduce/VB/reduc/O)\n",
        "  (NP the/DT/the/O cost/NN/cost/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP administration/NN/administr/O)\n",
        "  ''/''/''/O\n",
        "  ./././O)\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "example = u'The Hartsfield home is at 637 E. Pelham Rd. Aj.'\n",
      "print nec.parse(tokenize(example)).pprint()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP\n",
        "    The/DT/The/O\n",
        "    Hartsfield/NNP/Hartsfield/ORGANIZATION\n",
        "    home/NN/home/O)\n",
        "  (VP is/VBZ/is/O)\n",
        "  (PP at/IN/at/O)\n",
        "  (NP\n",
        "    637/CD/637/O\n",
        "    E./NNP/E./LOCATION\n",
        "    Pelham/NNP/Pelham/LOCATION\n",
        "    Rd./NNP/Rd./LOCATION\n",
        "    Aj/NNP/Aj/PERSON)\n",
        "  ./././O)\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for t in nec.batch_parse(nltk.corpus.brown.sents()[200:220]):\n",
      "    print t.pprint()\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP\n",
        "    Fears/NNS/Fear/O\n",
        "    prejudicial/JJ/prejudici/O\n",
        "    aspects/NNS/aspect/O))\n",
        "\n",
        "(S\n",
        "  ``/``/``/O\n",
        "  (NP The/DT/The/O statements/NNS/statement/O)\n",
        "  (VP may/MD/may/O be/VB/be/O)\n",
        "  highly/RB/highli/O\n",
        "  prejudicial/JJ/prejudici/O\n",
        "  (PP to/TO/to/O)\n",
        "  (NP my/PRP$/my/O client/NN/client/O)\n",
        "  ''/''/''/O\n",
        "  ,/,/,/O\n",
        "  (NP Bellows/NNP/Bellow/PERSON)\n",
        "  (VP told/VBD/told/O)\n",
        "  (NP the/DT/the/O court/NN/court/O)\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  ``/``/``/O\n",
        "  (NP Some/DT/Some/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP the/DT/the/O defendants/NNS/defend/O)\n",
        "  strongly/RB/strongli/O\n",
        "  (VP indicated/VBD/indic/O)\n",
        "  (NP they/PRP/they/O)\n",
        "  (VP knew/VBD/knew/O)\n",
        "  (NP they/PRP/they/O)\n",
        "  (VP were/VBD/were/O receiving/VBG/receiv/O stolen/VBN/stolen/O)\n",
        "  (NP property/NN/properti/O)\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  (NP It/PRP/It/O)\n",
        "  (VP is/VBZ/is/O)\n",
        "  impossible/JJ/imposs/O\n",
        "  (VP to/TO/to/O get/VB/get/O)\n",
        "  (NP a/DT/a/O fair/JJ/fair/O trial/NN/trial/O)\n",
        "  when/WRB/when/O\n",
        "  (NP some/DT/some/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP the/DT/the/O defendants/NNS/defend/O)\n",
        "  (VP made/VBD/made/O)\n",
        "  (NP statements/NNS/statement/O)\n",
        "  (VP involving/VBG/involv/O)\n",
        "  (NP themselves/PRP/themselv/O)\n",
        "  and/CC/and/O\n",
        "  (NP others/NNS/other/O)\n",
        "  ''/''/''/O\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  (NP Judge/NNP/Judg/O Parsons/NNP/Parson/PERSON)\n",
        "  (VP leaned/VBD/lean/O)\n",
        "  (PP over/IN/over/O)\n",
        "  (NP the/DT/the/O bench/NN/bench/O)\n",
        "  and/CC/and/O\n",
        "  (VP inquired/VBD/inquir/O)\n",
        "  ,/,/,/O\n",
        "  ``/``/``/O\n",
        "  (NP You/PRP/You/O)\n",
        "  (VP mean/VBP/mean/O)\n",
        "  (NP some/DT/some/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP the/DT/the/O defendants/NNS/defend/O)\n",
        "  (VP made/VBD/made/O)\n",
        "  (NP statements/NNS/statement/O)\n",
        "  (VP admitting/VBG/admit/O)\n",
        "  (NP this/DT/thi/O)\n",
        "  ''/''/''/O\n",
        "  ?/./?/O\n",
        "  ?/./?/O)\n",
        "\n",
        "(S\n",
        "  ``/``/``/O\n",
        "  Yes/UH/Ye/O\n",
        "  ,/,/,/O\n",
        "  (NP your/PRP$/your/O honor/NN/honor/O)\n",
        "  ''/''/''/O\n",
        "  ,/,/,/O\n",
        "  (VP replied/VBD/repli/O)\n",
        "  (NP Bellows/NNP/Bellow/PERSON)\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  ``/``/``/O\n",
        "  (NP What/WP/What/O)\n",
        "  (NP this/DT/thi/O)\n",
        "  (VP amounts/VBZ/amount/O)\n",
        "  (PP to/TO/to/O)\n",
        "  ,/,/,/O\n",
        "  if/IN/if/O\n",
        "  true/JJ/true/O\n",
        "  ,/,/,/O\n",
        "  (VP is/VBZ/is/O)\n",
        "  that/IN/that/O\n",
        "  (NP there/EX/there/O)\n",
        "  (VP will/MD/will/O be/VB/be/O)\n",
        "  (NP a/DT/a/O free-for-all/NN/free-for-al/O fight/NN/fight/O)\n",
        "  (PP in/IN/in/O)\n",
        "  (NP this/DT/thi/O case/NN/case/O)\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  (NP There/EX/There/O)\n",
        "  (VP is/VBZ/is/O)\n",
        "  (NP a/DT/a/O conflict/NN/conflict/O)\n",
        "  (PP among/IN/among/O)\n",
        "  (NP the/DT/the/O defendants/NNS/defend/O)\n",
        "  ''/''/''/O\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  (NP Washington/NNP/Washington/LOCATION)\n",
        "  ,/,/,/O\n",
        "  (NP July/NNP/Juli/O 24/CD/24/O))\n",
        "\n",
        "(S\n",
        "  --/:/--/O\n",
        "  (NP\n",
        "    President/NNP/Presid/O\n",
        "    Kennedy/NNP/Kennedi/PERSON\n",
        "    today/NN/today/O)\n",
        "  (VP pushed/VBD/push/O)\n",
        "  aside/RB/asid/O\n",
        "  (NP\n",
        "    other/JJ/other/O\n",
        "    White/NNP/White/ORGANIZATION\n",
        "    House/NNP/Hous/ORGANIZATION\n",
        "    business/NN/busi/O)\n",
        "  (VP to/TO/to/O devote/VB/devot/O)\n",
        "  (NP\n",
        "    all/PDT/all/O\n",
        "    his/PRP$/hi/O\n",
        "    time/NN/time/O\n",
        "    and/CC/and/O\n",
        "    attention/NN/attent/O)\n",
        "  (PP to/TO/to/O)\n",
        "  (VP working/VBG/work/O)\n",
        "  (PP on/IN/on/O)\n",
        "  (NP\n",
        "    the/DT/the/O\n",
        "    Berlin/NNP/Berlin/LOCATION\n",
        "    crisis/NN/crisi/O\n",
        "    address/NN/address/O)\n",
        "  (NP he/PRP/he/O)\n",
        "  (VP will/MD/will/O deliver/VB/deliv/O)\n",
        "  (NP tomorrow/NN/tomorrow/O night/NN/night/O)\n",
        "  (PP to/TO/to/O)\n",
        "  (NP the/DT/the/O American/JJ/American/O people/NNS/peopl/O)\n",
        "  (PP over/IN/over/O)\n",
        "  (NP\n",
        "    nationwide/JJ/nationwid/O\n",
        "    television/NN/televis/O\n",
        "    and/CC/and/O\n",
        "    radio/NN/radio/O)\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  (NP The/DT/The/O President/NNP/Presid/O)\n",
        "  (VP spent/VBD/spent/O)\n",
        "  (NP much/JJ/much/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP the/DT/the/O week-end/NN/week-end/O)\n",
        "  (PP at/IN/at/O)\n",
        "  (NP his/PRP$/hi/O summer/NN/summer/O home/NN/home/O)\n",
        "  (PP on/IN/on/O)\n",
        "  (NP Cape/NNP/Cape/LOCATION Cod/NNP/Cod/LOCATION)\n",
        "  (VP writing/VBG/write/O)\n",
        "  (NP the/DT/the/O first/JJ/first/O drafts/NNS/draft/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP portions/NNS/portion/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP the/DT/the/O address/NN/address/O)\n",
        "  (PP with/IN/with/O)\n",
        "  (NP the/DT/the/O help/NN/help/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP\n",
        "    White/NNP/White/ORGANIZATION\n",
        "    House/NNP/Hous/ORGANIZATION\n",
        "    aids/NNS/aid/O)\n",
        "  (PP in/IN/in/O)\n",
        "  (NP Washington/NNP/Washington/LOCATION)\n",
        "  (PP with/IN/with/O)\n",
        "  (NP whom/WP/whom/O)\n",
        "  (NP he/PRP/he/O)\n",
        "  (VP talked/VBD/talk/O)\n",
        "  (PP by/IN/by/O)\n",
        "  (NP telephone/NN/telephon/O)\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  Shortly/RB/Shortli/O\n",
        "  (PP after/IN/after/O)\n",
        "  (NP the/DT/the/O Chief/NNP/Chief/O Executive/NNP/Execut/O)\n",
        "  (VP returned/VBD/return/O)\n",
        "  (PP to/TO/to/O)\n",
        "  (NP Washington/NNP/Washington/LOCATION)\n",
        "  (PP in/IN/in/O)\n",
        "  (NP midmorning/NN/midmorn/O)\n",
        "  (PP from/IN/from/O)\n",
        "  (NP Hyannis/NNP/Hyanni/LOCATION Port/NNP/Port/LOCATION)\n",
        "  ,/,/,/O\n",
        "  (NP Mass./NNP/Mass./LOCATION)\n",
        "  ,/,/,/O\n",
        "  (NP\n",
        "    a/DT/a/O\n",
        "    White/NNP/White/O\n",
        "    House/NNP/Hous/O\n",
        "    spokesman/NN/spokesman/O)\n",
        "  (VP said/VBD/said/O)\n",
        "  (NP the/DT/the/O address/NN/address/O text/NN/text/O)\n",
        "  still/RB/still/O\n",
        "  (VP had/VBD/had/O)\n",
        "  ``/``/``/O\n",
        "  quite/RB/quit/O\n",
        "  (NP a/DT/a/O way/NN/way/O)\n",
        "  (VP to/TO/to/O go/VB/go/O)\n",
        "  ''/''/''/O\n",
        "  (PP toward/IN/toward/O)\n",
        "  (NP completion/NN/complet/O)\n",
        "  ./././O)\n",
        "\n",
        "(S (NP Decisions/NNS/Decis/O) (VP are/VBP/are/O made/VBN/made/O))\n",
        "\n",
        "(S\n",
        "  (VP Asked/VBN/Ask/O to/TO/to/O elaborate/VB/elabor/O)\n",
        "  ,/,/,/O\n",
        "  (NP Pierre/NNP/Pierr/PERSON Salinger/NNP/Saling/PERSON)\n",
        "  ,/,/,/O\n",
        "  (NP\n",
        "    White/NNP/White/O\n",
        "    House/NNP/Hous/O\n",
        "    press/NN/press/O\n",
        "    secretary/NN/secretari/O)\n",
        "  ,/,/,/O\n",
        "  (VP replied/VBD/repli/O)\n",
        "  ,/,/,/O\n",
        "  ``/``/``/O\n",
        "  (NP I/PRP/I/O)\n",
        "  (VP would/MD/would/O say/VB/say/O)\n",
        "  (NP it's/NNS/it'/O)\n",
        "  (VP got/VBD/got/O to/TO/to/O go/VB/go/O)\n",
        "  (PP thru/IN/thru/O)\n",
        "  (NP several/JJ/sever/O more/JJR/more/O drafts/NNS/draft/O)\n",
        "  ''/''/''/O\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  (NP Salinger/NNP/Saling/PERSON)\n",
        "  (VP said/VBD/said/O)\n",
        "  (NP\n",
        "    the/DT/the/O\n",
        "    work/NN/work/O\n",
        "    President/NNP/Presid/O\n",
        "    Kennedy/NNP/Kennedi/PERSON)\n",
        "  ,/,/,/O\n",
        "  (NP advisers/NNS/advis/O)\n",
        "  ,/,/,/O\n",
        "  and/CC/and/O\n",
        "  (NP members/NNS/member/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP his/PRP$/hi/O staff/NN/staff/O)\n",
        "  (VP were/VBD/were/O doing/VBG/do/O)\n",
        "  (PP on/IN/on/O)\n",
        "  (NP the/DT/the/O address/NN/address/O)\n",
        "  (VP involved/VBD/involv/O)\n",
        "  (NP composition/NN/composit/O and/CC/and/O wording/NN/word/O)\n",
        "  ,/,/,/O\n",
        "  rather/RB/rather/O\n",
        "  (PP than/IN/than/O)\n",
        "  (NP last/JJ/last/O minute/NN/minut/O decisions/NNS/decis/O)\n",
        "  (PP on/IN/on/O)\n",
        "  (NP administration/NN/administr/O plans/NNS/plan/O)\n",
        "  (VP to/TO/to/O meet/VB/meet/O)\n",
        "  (NP\n",
        "    the/DT/the/O\n",
        "    latest/JJS/latest/O\n",
        "    Berlin/NNP/Berlin/LOCATION\n",
        "    crisis/NN/crisi/O)\n",
        "  (VP precipitated/VBN/precipit/O)\n",
        "  (PP by/IN/by/O)\n",
        "  (NP\n",
        "    Russia's/NN/Russia'/LOCATION\n",
        "    demands/NNS/demand/O\n",
        "    and/CC/and/O\n",
        "    proposals/NNS/propos/O)\n",
        "  (PP for/IN/for/O)\n",
        "  (NP the/DT/the/O city/NN/citi/O)\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  (NP The/DT/The/O last/JJ/last/O 10/CD/10/O cases/NNS/case/O)\n",
        "  (PP in/IN/in/O)\n",
        "  (NP the/DT/the/O investigation/NN/investig/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP the/DT/the/O Nov./NNP/Nov./O 8/CD/8/O election/NN/elect/O)\n",
        "  (VP were/VBD/were/O dismissed/VBN/dismiss/O)\n",
        "  (NP yesterday/NN/yesterday/O)\n",
        "  (PP by/IN/by/O)\n",
        "  (NP\n",
        "    Acting/NNP/Act/O\n",
        "    Judge/NNP/Judg/O\n",
        "    John/NNP/John/PERSON\n",
        "    M./NNP/M./PERSON\n",
        "    Karns/NNP/Karn/PERSON)\n",
        "  ,/,/,/O\n",
        "  (NP who/WP/who/O)\n",
        "  (VP charged/VBD/charg/O)\n",
        "  that/IN/that/O\n",
        "  (NP the/DT/the/O prosecution/NN/prosecut/O)\n",
        "  (VP obtained/VBD/obtain/O)\n",
        "  (NP evidence/NN/evid/O)\n",
        "  ``/``/``/O\n",
        "  (PP by/IN/by/O)\n",
        "  (NP\n",
        "    unfair/JJ/unfair/O\n",
        "    and/CC/and/O\n",
        "    fundamentally/RB/fundament/O\n",
        "    illegal/JJ/illeg/O\n",
        "    means/NNS/mean/O)\n",
        "  ''/''/''/O\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  (NP Karns/NNP/Karn/PERSON)\n",
        "  (VP said/VBD/said/O)\n",
        "  that/IN/that/O\n",
        "  (NP the/DT/the/O cases/NNS/case/O)\n",
        "  (VP involved/VBD/involv/O)\n",
        "  (NP a/DT/a/O matter/NN/matter/O)\n",
        "  ``/``/``/O\n",
        "  (PP of/IN/of/O)\n",
        "  (NP\n",
        "    even/RB/even/O\n",
        "    greater/JJR/greater/O\n",
        "    significance/NN/signific/O)\n",
        "  (PP than/IN/than/O)\n",
        "  (NP the/DT/the/O guilt/NN/guilt/O or/CC/or/O innocence/NN/innoc/O)\n",
        "  ''/''/''/O\n",
        "  (PP of/IN/of/O)\n",
        "  (NP the/DT/the/O 50/CD/50/O persons/NNS/person/O)\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  (NP He/PRP/He/O)\n",
        "  (VP said/VBD/said/O)\n",
        "  (NP evidence/NN/evid/O)\n",
        "  (VP was/VBD/wa/O obtained/VBN/obtain/O)\n",
        "  ``/``/``/O\n",
        "  (PP in/IN/in/O)\n",
        "  (NP violation/NN/violat/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP the/DT/the/O legal/JJ/legal/O rights/NNS/right/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP citizens/NNS/citizen/O)\n",
        "  ''/''/''/O\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  (NP Karns'/NN/Karns'/PERSON ruling/NN/rule/O)\n",
        "  (VP pertained/VBD/pertain/O)\n",
        "  (PP to/TO/to/O)\n",
        "  (NP eight/CD/eight/O)\n",
        "  (PP of/IN/of/O)\n",
        "  (NP the/DT/the/O 10/CD/10/O cases/NNS/case/O)\n",
        "  ./././O)\n",
        "\n",
        "(S\n",
        "  (PP In/IN/In/O)\n",
        "  (NP the/DT/the/O two/CD/two/O other/JJ/other/O cases/NNS/case/O)\n",
        "  (NP he/PRP/he/O)\n",
        "  (VP ruled/VBD/rule/O)\n",
        "  that/IN/that/O\n",
        "  (NP the/DT/the/O state/NN/state/O)\n",
        "  (VP had/VBD/had/O been/VBN/been/O)\n",
        "  ``/``/``/O\n",
        "  unable/JJ/unabl/O\n",
        "  (VP to/TO/to/O make/VB/make/O)\n",
        "  (NP a/DT/a/O case/NN/case/O)\n",
        "  ''/''/''/O\n",
        "  ./././O)\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "timeit nec.batch_parse(nltk.corpus.brown.sents())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AssertionError",
       "evalue": "expected \"'\" in\n' know enough to sue '\nto start with \"?\" from\n? ' know enough to sue ' Berger insisted that `` we know enough to sue for the full amount '' .",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-55-aea2a6efa864>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'timeit nec.batch_parse(nltk.corpus.brown.sents())'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/Library/Python/2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2203\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2204\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2207\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2124\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2125\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2126\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2127\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell)\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/timeit.pyc\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
        "\u001b[0;32m<ipython-input-51-e65dfebf45b7>\u001b[0m in \u001b[0;36mbatch_parse\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbatch_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mNEChunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix_stanford\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNEChunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mstems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-51-e65dfebf45b7>\u001b[0m in \u001b[0;36mfix_stanford\u001b[0;34m(tags, sentences)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0mNEChunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-51-e65dfebf45b7>\u001b[0m in \u001b[0;36m_merge_tokens\u001b[0;34m(tags, tokens)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 assert tok.startswith(t),                     u'expected \"%s\" in\\n%s\\nto start with \"%s\" from\\n%s' % (\n\u001b[1;32m    119\u001b[0m                         \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                         \u001b[0;34mu\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m                     )\n\u001b[1;32m    122\u001b[0m                 \u001b[0;31m# join the current SNER token with the next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAssertionError\u001b[0m: expected \"'\" in\n' know enough to sue '\nto start with \"?\" from\n? ' know enough to sue ' Berger insisted that `` we know enough to sue for the full amount '' ."
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "On a Mac with 1.3 GHz Intel Core i5, parsing these 57,340 sentences took X min, that is X sec per sentence. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}