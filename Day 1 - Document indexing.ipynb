{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document indexing with LSH Forest\n",
    "\n",
    "Practical course material for the ASDM Class 09 (Text Mining) by Florian Leitner.\n",
    "\n",
    "© 2016 Florian Leitner. All rights reserved.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Today's lab will cover the two most fundamental techniques of Text Mining: Document classification and indexing; This notebook covers the latter (\"building the world's fastest search engine\").\n",
    "\n",
    "For background, please check the \"Pitfalls for budding data scientists\" and the \"Shingling and Jaccard's similarity\" notebooks.\n",
    "\n",
    "An excellent resource for basic LSH is the free book (PDF) by Rajaraman, [\"Mining of Massive Datasets\"](http://infolab.stanford.edu/~ullman/mmds/book.pdf). We will specifically be looking at a \"self-tuning\" LSH design, the [LSH Forest by Bawa et al.](http://infolab.stanford.edu/~bawa/Pub/similarity.pdf).\n",
    "\n",
    "Ensure you have **NLTK** installed or install it with:\n",
    "\n",
    "- Anaconda Python: `conda install nltk`\n",
    "- Stock Python: `pip3 install nltk`\n",
    "\n",
    "Also install the `segtok` tokenization and segmentation library created by yours truly (the instructor, me):\n",
    "\n",
    "- Anaconda Python: `conda install segtok`\n",
    "- Stock Python: `pip3 install segtok`\n",
    "\n",
    "Once done, the following imports should work; If not, you can always skip the practical document indexing examples below.\n",
    "\n",
    "Once done, the following imports should work; If not, you can always skip the practical example at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import segtok\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you have at least NLTK book (and/or popular) downloaded and installed from the dialog window that shows up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline --no-import-all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard's set similarity\n",
    "\n",
    "(see the relevant Notebook for a detailed discussion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jaccard(seq1, seq2):\n",
    "    \"\"\"\n",
    "    The Jaccard similarity between two sequences:\n",
    "    |∩(X,Y)| / |∪(X,Y)|\n",
    "    \"\"\"\n",
    "    \n",
    "    x = set(seq1)\n",
    "    y = set(seq2)\n",
    "    return len(x & y) / len(x | y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-hash signatures\n",
    "\n",
    "(see the \"Pitfalls...\" Notebook for a discussion of `hash` vs. `id`)\n",
    "\n",
    "We will look into an implementation of LSH by [Christian Jauvin](http://cjauvin.github.io/); see [github.com/go2starr/lshhdc](https://github.com/go2starr/lshhdc) for educational puposes (but note that SciKit-Learn will provide us with the implementation we will be using)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MinHashSignature:\n",
    "    \"\"\"Hash signatures for sets/tuples using minhash.\"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        \"\"\"\n",
    "        Define the dimension of the hash pool\n",
    "        (number of hash functions).\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.hashes = self.hash_functions()\n",
    "\n",
    "    def hash_functions(self):\n",
    "        \"\"\"\n",
    "        Return a list of `dim` different hash functions h_n.\n",
    "        \"\"\"\n",
    "        \n",
    "        # In essence, this implements \n",
    "        # the hash permutations suggested\n",
    "        # in slide \"Minash signatures (1/2)\"\n",
    "        def hash_factory(mutator):\n",
    "            # note that each element in \n",
    "            # an item being min-hashed\n",
    "            # will be pushed in here, as \"x\"\n",
    "            # I.e., x typically is a token,\n",
    "            # character, or n-gram, etc.\n",
    "            return lambda x: hash(\n",
    "                \"salt\" + str(mutator) + str(x) + \"salt\"\n",
    "            )\n",
    "        \n",
    "        return [ hash_factory(_) for _ in range(self.dim) ]\n",
    "\n",
    "    def sign(self, item):\n",
    "        \"\"\"\n",
    "        Return the minhash signatures\n",
    "        for any iterable input `item` D_i.\n",
    "        \"\"\"\n",
    "        \n",
    "        sig = [ float(\"inf\") ] * self.dim\n",
    "        \n",
    "        for hash_ix, hash_fn in enumerate(self.hashes):\n",
    "            # minhashing (requires item is iterable):\n",
    "            # c.f., slide \"Minhash signatures (2/2)\"\n",
    "            #\n",
    "            # as per the algorithm shown: for each hash\n",
    "            # function, choose the hash value from that\n",
    "            # input item (token, character, n-gram, etc.)\n",
    "            # that has the smallest (min) hashed value\n",
    "            # as the value to use in the final signature\n",
    "            sig[hash_ix] = min(hash_fn(i) for i in item)\n",
    "        \n",
    "        return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MinHash signature that generates four different hash values (\"dimensions\") for the same string; This signature can be understood as generating four different, random orders of the input item that gets is signed (aka., \"hash-permuted row IDs\" in the slides, \"Minhash signatures (1/2)\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-7299365442126544006,\n",
       " -6389310932557286797,\n",
       " -6959240971047010868,\n",
       " -8605836430286245850]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig4 = MinHashSignature(4)\n",
    "sig4.sign(\"example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Banded Locality Sensitive Hashing\n",
    "\n",
    "(see the \"Pitfalls...\" Notebook for a discussion of floating point issues with probabity values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BandedLSH:\n",
    "    # n. bands:\n",
    "    # b = dictionary.hasher.size /\n",
    "    #     dictionary.hasher.bandwidth\n",
    "    #    \n",
    "    # rows per band:\n",
    "    # r = dictionary.hasher.bandwidth \n",
    "    \n",
    "    def __init__(self, size, threshold):\n",
    "        \"\"\"\n",
    "        Set up for a given similarity `threshold`\n",
    "        and a given total BandedLSH `size`.\n",
    "        \"\"\"\n",
    "        \n",
    "        # total size (n. rows) of the matrix\n",
    "        # (slide \"Banded locality sensitve hasing\")\n",
    "        self.size = size\n",
    "        # *desired* Jaccard similarity threshold\n",
    "        # of the items belonging together\n",
    "        self.threshold = threshold\n",
    "        # number hashes to use per band\n",
    "        # to get to that threshold\n",
    "        # \"rows per band\"\n",
    "        self.bandwidth = self.get_bandwidth(\n",
    "            size,\n",
    "            threshold)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_bandwidth(n, t):\n",
    "        \"\"\"\n",
    "        Approximate the bandwidth\n",
    "        (number of rows in each band)\n",
    "        for a fixed total number of\n",
    "        rows `n` (bandwith * n. bands)\n",
    "        needed to achieve the desired\n",
    "        similarity threshold `t`.\n",
    "\n",
    "        Similarity threshold t = (1. / b) ** (1. / r)\n",
    "        with size (total rows in LSH) n = b * r\n",
    "        where:\n",
    "        b = number of bands\n",
    "        r = number of rows per band\n",
    "        therefore:\n",
    "        b = 1 / (t ** r)\n",
    "        and:\n",
    "        r = n /b\n",
    "        \"\"\"\n",
    "        \n",
    "        best = n # default: 1 band with `n` rows\n",
    "        minerr = float(\"inf\") # default: \"infinite\" error\n",
    "        \n",
    "        # Try finding the smallest possible bandwith r\n",
    "        # by starting with one row per band, etc:\n",
    "        for r in range(1, n + 1):\n",
    "            try:\n",
    "                # calculate the number of bands b needed to\n",
    "                # achieve the desired threshold t with the\n",
    "                # current number of rows r\n",
    "                b = 1. / (t ** r)\n",
    "            except: # Divide by zero; size is too huge to manage\n",
    "                return best\n",
    "            \n",
    "            # how close would the calculated number bands b\n",
    "            # with the current bandwith r, \n",
    "            # required to achieve the desired threshold t,\n",
    "            # be to the desired total LSH size n?\n",
    "            err = abs(n - b * r)\n",
    "            \n",
    "            if err < minerr:\n",
    "                best = r\n",
    "                minerr = err\n",
    "                \n",
    "        return best\n",
    "\n",
    "    def hash(self, sig):\n",
    "        \"\"\"\n",
    "        Generate one locality-sensitive hash for each band for an\n",
    "        input item D with the (min-)hashed signature `sig`.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Note that this implementation requires a min-hash signature of\n",
    "        # the same size as the total BandedLSH size, which is technically\n",
    "        # not required (as discussed in class); We could assign the same\n",
    "        # signatures to multiple bands, as long as there are sufficient\n",
    "        # signature hash values to generate a unique selection of values\n",
    "        # for each band.\n",
    "        # But as hashing is cheap, we don't go to such lengths here...\n",
    "        # Note that in the slides you see several hashes per band; here,\n",
    "        # instead, we just combine those hash values (the \"band\") into one\n",
    "        # single hash value.\n",
    "        for band in zip(*(iter(sig),) * self.bandwidth):\n",
    "            # the \"salt\" is to make this approach more robust\n",
    "            # and is specific to how hashing works (out of scope here)\n",
    "            yield hash(\"salt\" + str(band) + \"tlas\")\n",
    "\n",
    "    @property\n",
    "    def exact_threshold(self):\n",
    "        \"\"\"\n",
    "        The actual threshold,\n",
    "        as defined by the calculated bandwith.\n",
    "        \"\"\"\n",
    "        \n",
    "        r = self.bandwidth\n",
    "        b = self.size / r\n",
    "        return (1. / b) ** (1. / r)\n",
    "\n",
    "    def get_n_bands(self):\n",
    "        \"\"\"\n",
    "        Calculate the number of bands.\n",
    "        \"\"\"\n",
    "        \n",
    "        return int(self.size / self.bandwidth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zip-list iterated over in `BandedLSH.hash` is simply the min-hash signature values subset for each band. E.g., assuming `size = 6` for the BandedLSH and, therefore, also for the min-hash `signature` (see inline comments, we just use as many min-hashes as we have total rows), and assuming we calculated a `bandwidth` of `2` with `BandedLSH.get_bandwidth`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 4), (5, 6)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature = [1,2,3,4,5,6]\n",
    "bandwidth = 2\n",
    "list(zip(*(iter(signature),) * bandwidth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, what we get out of `BandedLSH.hash` is one **hash value** for each **band** given an item's (min-hash) `signature` and the (calculated) LSH `bandwidth`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3713081631934410656, 3713083796997400956, 3713085962043070856]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\n",
    "    map(\n",
    "        hash,\n",
    "        zip(*(iter(signature),) * bandwidth)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the bandwith had been `3`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3), (4, 5, 6)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(signature)\n",
    "list(zip(*(iter(signature),) * 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention to number of bands. This indicates how \"hard\" the threshold can be cut off. If this number is small, you might very fuzzy clusters (i.e., a very streched sigmoid in the plot below). The bandwidth (rows/band) in turn is proportional to the (desired) Jaccard similiarty (the horizontal position of the sigmoid curve).\n",
    "\n",
    "To visualize the \"cutoff hardness\" of your setup, you can visualize the probability of two documents being added to the same cluster (i.e., set of buckets) as a function of their Jaccard similarity. (see slide \"Banded Locaility Sensitive Minhashing\", $p_{agreement}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niFRTUhARkWpKCiIiUk1JQUREqv1/IEDWKAk7EEkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1083f2c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bandwidth = 30\n",
    "rows_per_band = 5\n",
    "p_agreement = lambda sim: 1 - (1 - sim**rows_per_band)**bandwidth\n",
    "x = np.arange(0.0, 1.0, 0.01)\n",
    "\n",
    "plt.plot(x, p_agreement(x))\n",
    "plt.title('Clustering probability')\n",
    "# (as a function of bandwidth and n. bands)\n",
    "plt.xlabel('Jaccard similarity')\n",
    "plt.ylabel('Clustering P')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to play around with the above cell, chaning `bandwidth` and `rows per band` to understand how those two parameters influnce the likelihood of two documents sharing *at least one bucket* given their Jaccard similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document indexing\n",
    "\n",
    "While the toy implementation above was practical to discuss the algorithm, we will be using a special [LSH **Forest**](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LSHForest.html#sklearn-neighbors-lshforest) implementation to retrieve the closest matching document given a query (document).\n",
    "\n",
    "Another attractive LSH implemention is avaliable from the [`datasketch`](https://ekzhu.github.io/datasketch/lsh.html) Python library.\n",
    "\n",
    "### Corpus preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus sizes: train = 11314 ; test = 7532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from segtok.segmenter import split_multi\n",
    "from segtok.tokenizer import word_tokenizer, split_contractions\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "train, test = fetch_20newsgroups(), fetch_20newsgroups(subset='test')\n",
    "\n",
    "stopwords_en = frozenset(stopwords.words('english'))\n",
    "stopwords_none = frozenset([])\n",
    "\n",
    "\n",
    "def tokenize(raw_text):\n",
    "    \"\"\"\n",
    "    Convert text to lower-case tokens of length > 2.\n",
    "    \"\"\"\n",
    "    \n",
    "    for sentence in split_multi(raw_text):\n",
    "        for raw in split_contractions(word_tokenizer(sentence)):\n",
    "            token = raw.lower()\n",
    "            \n",
    "            if len(token) > 2 and any(c.isalnum() for c in token):\n",
    "                yield token\n",
    "\n",
    "                \n",
    "def filter_and_stem(raw_text, stopwords):\n",
    "    \n",
    "    for token in tokenize(raw_text):\n",
    "        yield token\n",
    "        if token not in stopwords:\n",
    "            yield stemmer.stem(token)\n",
    "        else:\n",
    "            yield None\n",
    "                \n",
    "                \n",
    "def shingle(\n",
    "    raw_text,\n",
    "    stopwords=stopwords_none,\n",
    "    bigrams=False):\n",
    "    \"\"\"\n",
    "    Shingle text to stop-word-free,\n",
    "    stemmed uni- and bi-grams.\n",
    "    \"\"\"\n",
    "    \n",
    "    last = None\n",
    "    \n",
    "    for t in filter_and_stem(raw_text, stopwords):\n",
    "        if t is not None:\n",
    "            yield t\n",
    "            \n",
    "            if bigrams and last is not None:\n",
    "                yield \"{}_{}\".format(last, t)\n",
    "            \n",
    "        last = t\n",
    "\n",
    "print(\"Corpus sizes: train =\",\n",
    "      len(train.data),\n",
    "      \"; test =\", len(test.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at how that tokenization/shingling works on a document in our collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n"
     ]
    }
   ],
   "source": [
    "example = train['data'][0].strip()\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a real-world scenario, you'd probably start cleaning up the above \"mess\", removing the header and possibly the footers from the messages.\n",
    "\n",
    "You can either use `tokenize` to work with single tokens, or `shingle` to work with [higher] n-grams and stopword-filtering, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "front tellm addition there years out lerxst other university sports thing this info car thi wa engin body maryland earli all early whatever park can name thanks e-mail anyone 70s saw bumper were univers rest organ nntp-posting-host know called pleas doors production realli tellme line 60s model really separate what bricklin engine from looking neighborhood enlighten was histori thank product separ sport wonder lines 60 your please wam.umd.edu subject day made looked call brought look year you colleg funky addit could door have 2-door where college 70 history the wondering whatev rac3.wam.umd.edu specs late anyon funki small spec organization bodi\n"
     ]
    }
   ],
   "source": [
    "tokens = frozenset(shingle(example))\n",
    "print(\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert the whole 20News *train* corpus into tokens - this can take a while (possibly 5 minutes!) on most machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 13s, sys: 4.02 s, total: 4min 17s\n",
      "Wall time: 4min 20s\n"
     ]
    }
   ],
   "source": [
    "%time corpus = [frozenset(shingle(doc)) for doc in train['data']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity threshold estimation\n",
    "\n",
    "First, we'd like to find out how our document similarity behaves across our corpus.\n",
    "To do that we select a random \"target\" document and compute it's Jaccard similiary against all other documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other doc with max. Jaccard similarity = 0.15346534653465346\n",
      "Mean Jaccard similarity = 0.0763576100964\n",
      "Median Jaccard similarity = 0.07659574468085106\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11430fa20>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZc29nIonNEkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11430f208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "target = random.choice(range(len(corpus)))\n",
    "others = [i for i in range(len(corpus)) if i != target]\n",
    "jsim = lambda i: jaccard(corpus[target], corpus[i])\n",
    "max_sim = max((jsim(i) for i in others))\n",
    "\n",
    "print(\"other doc with max. Jaccard similarity =\",\n",
    "      max_sim)\n",
    "\n",
    "sims = pd.Series(jsim(i) for i in others)\n",
    "print(\"Mean Jaccard similarity =\", sims.mean())\n",
    "print(\"Median Jaccard similarity =\", sims.median())\n",
    "sims.plot.box()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun the above cell a few times to get a feeling for the similarty.\n",
    "\n",
    "What you can see is that \"plain\" LSH will *not* be very useful for **clustering** our corpus (represented as _bags-of-words_), because most documents have a very low similiarty to each other.\n",
    "Rather, where LSH shines is detecting duplicates or even plagiarism, because then the Jaccard simliarity is (very) high.\n",
    "\n",
    "### LSH Forest setup\n",
    "\n",
    "A special variant of LSH, LSH *Forest*, is able to detect the best match for a query (document) if there is any reasonable Jaccard similarity between that query and some document in your collection.\n",
    "In addition, and as before for the document classification, we set up a TF-IDF document vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.23 s, sys: 387 ms, total: 9.62 s\n",
      "Wall time: 9.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.neighbors import LSHForest\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    min_df=3,\n",
    "    max_df=0.5,\n",
    "    #ngram_range=(1,2),\n",
    "    token_pattern='(?u)(?:\\\\b|(?<=_))[^\\W_][^\\W_]+(?:\\\\b|(?=_))',\n",
    ")\n",
    "# train (aka, fit) the vectorizer to the train data:\n",
    "X_train = vectorizer.fit_transform(train.data)\n",
    "\n",
    "classifier = LSHForest(n_estimators=25, n_neighbors=1)\n",
    "classifier.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we've indexed all our documents in virtually no time! Please let this sink in for a moment - most search engines spend ages indexing content, while this solution hardly spends any noticable time at all (compared to the collection size, over 10,000 documents).\n",
    "\n",
    "The parameter that has the strongest impact on indexing and search times is `n_estimators`, but increasing it improves the retrieval quality. Depending on your needs, you will almost certainly want to adjust that parameter to your use-case. The `n_neighbors` parameter only sets the default number of document returned by a query; We are here only evaluating how LSH Forest performs \"on its own\", so setting this to one is great. Under normal circumstances, you might want to use a higher value and among the returned values check for the best value to further remove any error, avoiding false positives, and/or to present the user with a result *list* (instead of the best result only). For the reamining parameters, please refer to the [SciKit-Learn documentation](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LSHForest.html#sklearn.neighbors.LSHForest).\n",
    "\n",
    "## Document search\n",
    "\n",
    "Now, lets use this classifier to predict the nearest neighbor for each of the \"test\" documents that we did not yet index (so this isn't just trivially returning exact matches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 32s, sys: 2.64 s, total: 2min 35s\n",
      "Wall time: 2min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# with the fitted vectorizer, transform the test data:\n",
    "X_test = vectorizer.transform(test.data)\n",
    "\n",
    "# now run the queries:\n",
    "indices = classifier.kneighbors(\n",
    "    X_test,\n",
    "    n_neighbors=1,\n",
    "    return_distance=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is quite noteworthy here is the time it took to search our documents. We searched for around 7,000 documents in a pool of almost 12,000; So each query just took a small fraction of a second!\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "Now lets try to randomly query for 10 documents using exact maching to give you an idea just how blazingly fast the above was generated. Also, lets evaluate how good our index is; That is, the mean \"true\" rank of the document reported as the best match for those ten cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: rank =   610 @ Jaccard sim. = 0.120 ; true doc: sim. = 0.329\n",
      "prediction: rank =  1449 @ Jaccard sim. = 0.092 ; true doc: sim. = 0.195\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "trr = 0.0 # total reciprocal rank\n",
    "n_queries = len(test['data']) // 100\n",
    "samples = random.sample(list(enumerate(indices)), n_queries)\n",
    "\n",
    "for i, (prediction,) in samples:\n",
    "    # 'encode' the query\n",
    "    # ~ vectorizer.transform\n",
    "    query = frozenset(shingle(test['data'][i]))\n",
    "    \n",
    "    # compare query each doc in corpus - this is the most costly part\n",
    "    # ~ classifier.kneighbors 1/2\n",
    "    pairs = [(jaccard(corpus[idx], query), idx)\n",
    "             for idx in range(len(corpus))]\n",
    "    \n",
    "    # sort and reverse the results (also costly, but much less)\n",
    "    # ~ classifier.kneighbors 2/2\n",
    "    pairs = list(reversed(sorted(pairs)))\n",
    "    \n",
    "    # finally, generate and print some statistics for evaluation\n",
    "    max_val = pairs[0]\n",
    "    _, correct_ranking = zip(*pairs)\n",
    "    rank = correct_ranking.index(prediction) + 1\n",
    "    sim = 0.0\n",
    "    \n",
    "    if rank == 0:\n",
    "        rank = len(corpus)\n",
    "    else:\n",
    "        sim = pairs[rank-1][0]\n",
    "\n",
    "    trr += 1 / rank\n",
    "        \n",
    "    # report a few cases\n",
    "    if random.random() < 0.2:\n",
    "        print(\"prediction: rank = %5d\" % rank,\n",
    "              \"@ Jaccard sim. = %.3f\" % sim,\n",
    "              \"; true doc: sim. = %.3f\" % max_val[0])\n",
    "\n",
    "mrr = trr / n_queries # mean reciprocal rank\n",
    "print(\"\\nmean rank = %.0f\\n\" % (1 / mrr)) # estimated mean rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this process is around one to two orders of magnitude slower (notice that we queried 100 times less documents than before against LSH Forest, but that fraction about the same time to run).\n",
    "\n",
    "Another useful insight is that when the predicted document isn't the first, the overall best document (\"`true doc: sim.`\") tends to be of relatively low similarity, too - so we might argue that those are cases where there's no good match to start with.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Admittedly, document clustering with LSH is a bit of a black art. But then, about any data sketching is mostly black art... However, getting this right is extremely rewarding, because indexing documents using only hashes is probably the most performant and scalable path towards building a super-fast query engine - even if it is anything but \"exact science\", in a quite literal sense."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
