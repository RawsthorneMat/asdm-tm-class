{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality Sensitive Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical course material for the ASDM Class 09 (Text Mining) by Florian Leitner.\n",
    "\n",
    "Â© 2016 Florian Leitner. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%e'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "# lower the number of digits shown for floating-point numbers:\n",
    "%precision %e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Banker's rounding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casting to an `int` chops off the decimals: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1, 2, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(1.01), int(1.5), int(1.99), int(2.5), int(3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if using `round(float)` the you get Banker's rounding (on even final digit it rounds down, on uneven digits up):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 2, 2, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(1.01), round(1.5), round(1.99), round(2.5), round(3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why should this bother you? For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 2]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[round(i/2) for i in range(1,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 2]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i//2 for i in range(1,5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you are surprised by this behaviour; It is the standard [IEEE 754](https://en.wikipedia.org/wiki/Floating_point#Rounding_modes) behaviour for floating points that should be followed by all programming languages; It's languages that *don't* exhibit this bevahiour that are \"wrong\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `defaultdict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick review of `deafultdict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = defaultdict(int)\n",
    "demo['defined'] = demo['defined'] + 1 # NB: we did not define a value for 'defined', it magically initialized itself!\n",
    "demo['missing'], demo['defined']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All practicality aside, this can become a trap, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'missing' in demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'really missing' in demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard-based word similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shingle(s, k):\n",
    "    \"\"\"Generate k-length shingles of string s.\"\"\"\n",
    "    k = min(len(s), k)\n",
    "    for i in range(len(s) - k + 1):\n",
    "        yield s[i:i+k]\n",
    "\n",
    "def hshingle(s, k):\n",
    "    \"\"\"Generate k-length shingles then hash.\"\"\"\n",
    "    for s in shingle(s, k):\n",
    "        yield hash(s)\n",
    "\n",
    "def jaccard(X, Y):\n",
    "    \"\"\"The Jaccard similarity between two sets.\"\"\"\n",
    "    x = set(X)\n",
    "    y = set(Y)\n",
    "    return float(len(x & y)) / len(x | y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create all character trigrams of a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['str', 'tri', 'rin', 'ing']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(shingle('string', 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the integer hash of a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7191288385591411446"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-7191288385591411446,\n",
       " -675603213479884509,\n",
       " -7932968880153686062,\n",
       " -4052543708567640597]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hshingle('string', 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character unigram similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.000, 0.714, 0.222)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard('string', 'string'), jaccard('string', 'strang'), jaccard('string', 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.333"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard('word', 'wirt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard('alabama', 'malba') # not so good..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, it is better to use higher order; Bigram similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.429"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard(frozenset(shingle('alabama', 2)), frozenset(shingle('malba', 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or even character trigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard(frozenset(shingle('alabama', 3)), frozenset(shingle('malba', 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard(frozenset(shingle('word', 3)), frozenset(shingle('wirt', 3))) # not so good, either..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.571"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard('karlos', 'carol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.125"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard(frozenset(shingle('karlos', 2)), frozenset(shingle('carol', 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, depending on the scenario, a problem-specific decision for using uni-, bi- or, maybe a trigram sets has to be made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing minhash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We will look into an implementation of LSH by [Christian Jauvin](http://cjauvin.github.io/); see [github.com/go2starr/lshhdc](https://github.com/go2starr/lshhdc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MinHashSignature:\n",
    "    \"\"\"Hash signatures for sets/tuples using minhash.\"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        \"\"\"\n",
    "        Define the dimension of the hash pool\n",
    "        (number of hash functions).\n",
    "        \"\"\"\n",
    "        self.dim = dim\n",
    "        self.hashes = self.hash_functions()\n",
    "\n",
    "    def hash_functions(self):\n",
    "        \"\"\"Return dim different hash functions.\"\"\"\n",
    "        def hash_factory(n):\n",
    "            return lambda x: hash(\"salt\" + str(n) + str(x) + \"salt\")\n",
    "        \n",
    "        return [ hash_factory(_) for _ in range(self.dim) ]\n",
    "\n",
    "    def sign(self, item):\n",
    "        \"\"\"Return the minhash signatures for the `item`.\"\"\"\n",
    "        sig = [ float(\"inf\") ] * self.dim\n",
    "        \n",
    "        for hash_ix, hash_fn in enumerate(self.hashes):\n",
    "            # minhashing; requires item is iterable:\n",
    "            sig[hash_ix] = min(hash_fn(i) for i in item)\n",
    "        \n",
    "        return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MinHash signature that generates four different hash values (\"dimensions\") for the same string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-7288557722884770586,\n",
       " -6492668138025979145,\n",
       " -8490456313657407868,\n",
       " -5809476266454031930]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig4 = MinHashSignature(4)\n",
    "sig4.sign(\"example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality Sensitive Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSH:\n",
    "    \"\"\"\n",
    "    Locality sensitive hashing.\n",
    "    \n",
    "    Uses a banding approach to hash\n",
    "    similar signatures to the same buckets.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size, threshold):\n",
    "        \"\"\"\n",
    "        LSH approximating a given similarity `threshold`\n",
    "        with a given hash signature `size`.\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.threshold = threshold\n",
    "        self.bandwidth = self.get_bandwidth(size, threshold)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_bandwidth(n, t):\n",
    "        \"\"\"\n",
    "        Approximate the bandwidth (number of rows in each band)\n",
    "        needed to get threshold.\n",
    "\n",
    "        Threshold t = (1/b) ** (1/r)\n",
    "        where\n",
    "        b = # of bands\n",
    "        r = # of rows per band\n",
    "        n = b * r = size of signature\n",
    "        \"\"\"\n",
    "        best = n # 1\n",
    "        minerr = float(\"inf\")\n",
    "        \n",
    "        for r in range(1, n + 1):\n",
    "            try:\n",
    "                b = 1. / (t ** r)\n",
    "            except: # Divide by zero, your signature is huge\n",
    "                return best\n",
    "            \n",
    "            err = abs(n - b * r)\n",
    "            \n",
    "            if err < minerr:\n",
    "                best = r\n",
    "                minerr = err\n",
    "                \n",
    "        return best\n",
    "\n",
    "    def hash(self, sig):\n",
    "        \"\"\"Generate hash values for this signature.\"\"\"\n",
    "        for band in zip(*(iter(sig),) * self.bandwidth):\n",
    "            yield hash(\"salt\" + str(band) + \"tlas\")\n",
    "\n",
    "    @property\n",
    "    def exact_threshold(self):\n",
    "        \"\"\"The exact threshold defined by the chosen bandwith.\"\"\"\n",
    "        r = self.bandwidth\n",
    "        b = self.size / r\n",
    "        return (1. / b) ** (1. / r)\n",
    "\n",
    "    def get_n_bands(self):\n",
    "        \"\"\"The number of bands.\"\"\"\n",
    "        return int(self.size / self.bandwidth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper class: UnionFind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an additional data structure to unite the equal bins in the disperesd bands into one virtual set. Please refer to the free Book by Rajaraman, \"Mining of Massive Datasets\", for more information why and how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class UnionFind:\n",
    "    \"\"\"\n",
    "    Union-find data structure.\n",
    "\n",
    "    Each unionFind instance X maintains a family of disjoint sets of\n",
    "    hashable objects, supporting the following two methods:\n",
    "\n",
    "    - X[item] returns a name for the set containing the given item.\n",
    "    Each set is named by an arbitrarily-chosen one of its members; as\n",
    "    long as the set remains unchanged it will keep the same name. If\n",
    "    the item is not yet part of a set in X, a new singleton set is\n",
    "    created for it.\n",
    "\n",
    "    - X.union(item1, item2, ...) merges the sets containing each item\n",
    "    into a single larger set. If any item is not yet part of a set\n",
    "    in X, it is added to X as one of the members of the merged set.\n",
    "    \n",
    "    Source: http://www.ics.uci.edu/~eppstein/PADS/UnionFind.py\n",
    "\n",
    "    Union-find data structure. Based on Josiah Carlson's code,\n",
    "    http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/215912\n",
    "    with significant additional changes by D. Eppstein.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Create a new empty union-find structure.\"\"\"\n",
    "        self.weights = {}\n",
    "        self.parents = {}\n",
    "\n",
    "    def __getitem__(self, object):\n",
    "        \"\"\"Find and return the name of the set containing the object.\"\"\"\n",
    "        # check for previously unknown object\n",
    "        if object not in self.parents:\n",
    "            self.parents[object] = object\n",
    "            self.weights[object] = 1\n",
    "            return object\n",
    "\n",
    "        # find path of objects leading to the root\n",
    "        path = [object]\n",
    "        root = self.parents[object]\n",
    "        \n",
    "        while root != path[-1]:\n",
    "            path.append(root)\n",
    "            root = self.parents[root]\n",
    "\n",
    "        # compress the path and return\n",
    "        for ancestor in path:\n",
    "            self.parents[ancestor] = root\n",
    "            \n",
    "        return root\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Iterate through all items ever found or unioned by this structure.\"\"\"\n",
    "        return iter(self.parents)\n",
    "\n",
    "    def union(self, *objects):\n",
    "        \"\"\"Find the sets containing the objects and merge them all.\"\"\"\n",
    "        roots = [self[x] for x in objects]\n",
    "        heaviest = max([(self.weights[r],r) for r in roots])[1]\n",
    "        for r in roots:\n",
    "            if r != heaviest:\n",
    "                self.weights[heaviest] += self.weights[r]\n",
    "                self.parents[r] = heaviest\n",
    "\n",
    "    def sets(self):\n",
    "        \"\"\"Return a list of each disjoint set\"\"\"\n",
    "        ret = defaultdict(list)\n",
    "        for k, _ in self.parents.items():\n",
    "            ret[self[k]].append(k)\n",
    "        return list(ret.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to give you a quick and dirty understanding of what it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sets [[0, 1]]\n",
      "name of containing set: \n",
      "item 0 belongs to set 1\n",
      "item 1 belongs to set 1\n"
     ]
    }
   ],
   "source": [
    "uf = UnionFind()\n",
    "uf.union(0, 1)\n",
    "print('sets', uf.sets())\n",
    "print('name of containing set:', ''.join(\"\\nitem %i belongs to set %i\" % (i, uf[i]) for i in uf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding and uniting two new items to the set produces two independent sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sets [[0, 1], [2, 3]]\n",
      "name of containing set: \n",
      "item 0 belongs to set 1\n",
      "item 1 belongs to set 1\n",
      "item 2 belongs to set 3\n",
      "item 3 belongs to set 3\n"
     ]
    }
   ],
   "source": [
    "uf.union(2, 3)\n",
    "print('sets', uf.sets())\n",
    "print('name of containing set:', ''.join(\"\\nitem %i belongs to set %i\" % (i, uf[i]) for i in uf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we unite two items in two disjoint sets, the sets are merged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sets [[0, 1, 2, 3]]\n",
      "name of containing set: \n",
      "item 0 belongs to set 3\n",
      "item 1 belongs to set 3\n",
      "item 2 belongs to set 3\n",
      "item 3 belongs to set 3\n"
     ]
    }
   ],
   "source": [
    "uf.union(3, 0)\n",
    "print('sets', uf.sets())\n",
    "assert uf.sets() == [[0, 1, 2, 3]], uf.sets()\n",
    "print('name of containing set:', ''.join(\"\\nitem %i belongs to set %i\" % (i, uf[i]) for i in uf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking up an item not yet in a set puts it in its own, new set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sets [[0, 1, 2, 3], [4]]\n",
      "name of containing set: \n",
      "item 0 belongs to set 3\n",
      "item 1 belongs to set 3\n",
      "item 2 belongs to set 3\n",
      "item 3 belongs to set 3\n",
      "item 4 belongs to set 4\n"
     ]
    }
   ],
   "source": [
    "uf[4]\n",
    "print('sets', uf.sets())\n",
    "print('name of containing set:', ''.join(\"\\nitem %i belongs to set %i\" % (i, uf[i]) for i in uf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sets [[0, 1, 2, 3], [4]]\n",
      "name of containing set: \n",
      "item 0 belongs to set 3\n",
      "item 1 belongs to set 3\n",
      "item 2 belongs to set 3\n",
      "item 3 belongs to set 3\n",
      "item 4 belongs to set 4\n"
     ]
    }
   ],
   "source": [
    "uf[3]\n",
    "print('sets', uf.sets())\n",
    "print('name of containing set:', ''.join(\"\\nitem %i belongs to set %i\" % (i, uf[i]) for i in uf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A banded LSH implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Cluster:\n",
    "    \"\"\"\n",
    "    Cluster items with a Jaccard similarity above\n",
    "    some `threshold` with a high probability.\n",
    "\n",
    "    Based on Rajaraman, \"Mining of Massive Datasets\":\n",
    "    \n",
    "    1. Generate items hash signatures\n",
    "    2. Use LSH to map similar signatures to same buckets\n",
    "    3. Use UnionFind to merge buckets containing same values\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=0.5, size=10):\n",
    "        \"\"\"\n",
    "        The `size` parameter controls the number of hash\n",
    "        functions (\"signature size\") to create.\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.unions = UnionFind()\n",
    "        self.signer = MinHashSignature(size)\n",
    "        self.hasher = LSH(size, threshold)\n",
    "        self.hashmaps = [\n",
    "            defaultdict(list) for _ in range(self.hasher.get_n_bands())\n",
    "        ]\n",
    "\n",
    "    def add(self, item, label=None):\n",
    "        \"\"\"\n",
    "        Add an `item` to the cluster.\n",
    "        \n",
    "        Optionally, use a `label` to reference this `item`.\n",
    "        Otherwise, the `item` itself is used as the label.\n",
    "        \"\"\"\n",
    "        # Ensure label for this item\n",
    "        if label is None:\n",
    "            label = item\n",
    "\n",
    "        # Add to unionfind structure\n",
    "        self.unions[label]\n",
    "\n",
    "        # Get item signature\n",
    "        sig = self.signer.sign(item)\n",
    "\n",
    "        # Unite labels with the same LSH keys in the same band\n",
    "        for band_idx, hashval in enumerate(self.hasher.hash(sig)):\n",
    "            self.hashmaps[band_idx][hashval].append(label)\n",
    "            self.unions.union(label, self.hashmaps[band_idx][hashval][0])\n",
    "\n",
    "    def groups(self):\n",
    "        \"\"\"\n",
    "        Get the clustering result.\n",
    "        \n",
    "        Returns sets of labels.\n",
    "        \"\"\"\n",
    "        return self.unions.sets()\n",
    "\n",
    "    def match(self, item):\n",
    "        \"\"\"\n",
    "        Get a set of matching labels for `item`.\n",
    "        \n",
    "        Returns a (possibly empty) set of labels.\n",
    "        \"\"\"\n",
    "        # Get signature\n",
    "        sig = self.signer.sign(item)\n",
    "        \n",
    "        matches = set()\n",
    "        \n",
    "        for band_idx, hashval in enumerate(self.hasher.hash(sig)):\n",
    "            if hashval in self.hashmaps[band_idx]:\n",
    "                matches.update(self.hashmaps[band_idx][hashval])\n",
    "        \n",
    "        return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An unigram example case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = Cluster(.75, 30)\n",
    "dictionary.add('string')\n",
    "dictionary.match('strang')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the hasher has calculated a bandwith to achieve an (exact) threshold as close as possible to the desired (given) threshold and MinHash signature size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.765, 6)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.hasher.exact_threshold, dictionary.hasher.bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.714"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard('strang', 'string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'strang'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.add('strang')\n",
    "dictionary.match('strang')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next example does not match (do you know why?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.add('word')\n",
    "dictionary.match('wird')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, the Jaccard similarity is too low and they end in different buckets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.600"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard('wird', 'word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if you add the word to the dictionary, it will be put into different groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['word'], ['string'], ['wird'], ['strang']]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.add('wird')\n",
    "dictionary.groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wird'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.match('wird')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary.add('strangled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'strangled', 'string'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.match('stringled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['word'], ['string'], ['wird'], ['strang'], ['strangled']]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A bigram case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'string'}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = Cluster(.25, 30)\n",
    "dictionary.add(frozenset(shingle('string', 2)), 'string')\n",
    "dictionary.match(frozenset(shingle('strang', 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.429"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard(frozenset(shingle('string', 2)), frozenset(shingle('strang', 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.258, 2)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.hasher.exact_threshold, dictionary.hasher.bandwidth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so forth... You might want to experiment on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With LSH we can match sets of items by their Jaccard-based set similarity. Note once more that this technique isn't limited to matching similar tokens and is mostly used to identify similar texts, as discussed in the lecture. So instead of character-n-grams, the input might as well be word-n-grams generated from a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilities and underflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we have the following three sets of (joint) probabilities; To make this plausible, lets say, they are for the a hundred tokens in our document that our feature selection procedure has elected. And the different probabilities assigned to each token are due to the three different labels we might assign to the document (I.e., we are trying to label a document with one of three different labels, and will assign it the label that has the highest joint probability over all tokens, much like a multinomial Naive Bayes classifier.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probabilities = [\n",
    "    [1e-4]*100, [1e-5]*100, [1e-8]*100\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you see immediately, the document should be assigned the last label with the highest probabilities. But lets try calculate actual probabilities for each of the three labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow(1e-4, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow(1e-5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow(1e-8, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoops! We quickly reached the limit of our processing capabilites; Two of the three cases give us a wrong answer (`P=0`). This is typically remedied with a simple trick: Work in log-transformed space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log, exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-921.034"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = sum([log(1e-4)]*100); A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1151.293"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = sum([log(1e-5)]*100); B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1842.068"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = sum([log(1e-8)]*100); C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, though, that we cannot move back into normal space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we calculate the normalized probability for each of the three labels? I.e., how to get probability scores for the three labels, that, when summed up, equal 1, that is, `P(a) = a / (a+b+c) = exp(A) / (exp(A) + exp(B) + exp(C))` if `A = log(a)` (a.s.f.)? We cannot do that directly, unless we get zeros, after all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.000, -230.259, -921.034]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logP = [A, B, C]\n",
    "#\n",
    "# Here is the critical trick: subtract the max. log(P_i) value from all values:\n",
    "#\n",
    "normP = [i - max(logP) for i in logP]\n",
    "#\n",
    "# Result:\n",
    "#\n",
    "normP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means, the normalized `log(P_i)` for the label with the max. probablity now is zero (i.e., its `P_i = 1`), and all other results are adjusted accordingly to this norm. Now we can safely transform back into our real space; Even if a number now is zero (as is the case for our thrid label), you can in most cases happily ignore that underflow problem, as that probablity is so infinitesimaly small that for all practical concerns it can be considered zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.000000e+00, 1.000000e-100, 0.000000e+00]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expP = list(map(exp, normP)); expP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.000000e+00, 1.000000e-100, 0.000000e+00]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = [i / sum(expP) for i in expP]; P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is still a tiny margin of floating-point error in this result, becasue the above result is strictly summed up, does not sum to one, but to about `1 + 10e-101`... But that's not even testable (It might become noticable if the three label probabilities had been a closer call, although - so usually, you will need to check floating point numbers according to some precision you wish to ensure to make sure that your probabilities are all correct):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(P) == 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But unless you care about being correct beyond 100 digits in the case above, the outcome is probably acceptable in 99.99% of all practical cases (I.e., all but financial institutions will be happy with this \"imperfect\" solution...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
